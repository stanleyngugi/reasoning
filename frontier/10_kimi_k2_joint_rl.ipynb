{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 10 — Kimi K2: Joint RL with Verifiable + Rubric Rewards\n",
                "\n",
                "> **Purpose:** Understand Moonshot AI's dual-reward RL system: verifiable rewards for objective tasks + self-critique rubrics for open-ended tasks.\n",
                "\n",
                "**Key insight:** Combine hard verification (unit tests, math checks) with soft self-critique (rubric-based evaluation) in a unified RL framework.\n",
                "\n",
                "| Reward Type | For Tasks | Mechanism | Example |\n",
                "|-------------|-----------|-----------|--------|\n",
                "| **Verifiable** | Code, Math | Rule-based pass/fail | Unit tests |\n",
                "| **Rubric-based** | Open-ended | Self-critique scoring | Helpfulness rubric |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from typing import Dict, List, Tuple, Optional, Callable\n",
                "from dataclasses import dataclass\n",
                "import re\n",
                "\n",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Kimi K2 Architecture Overview\n",
                "\n",
                "```\n",
                "┌──────────────────────────────────────────────────────────────┐\n",
                "│                    Kimi K2 (1T params)                       │\n",
                "│                    32B active / token                        │\n",
                "├──────────────────────────────────────────────────────────────┤\n",
                "│  MoE: 384 experts, 8+1 active per token                      │\n",
                "│  61 layers, 64 attention heads                               │\n",
                "│  Context: 128K-256K tokens                                   │\n",
                "├──────────────────────────────────────────────────────────────┤\n",
                "│                   Joint RL Training                          │\n",
                "│  ┌─────────────────┐    ┌─────────────────┐                  │\n",
                "│  │ Verifiable      │    │ Self-Critique   │                  │\n",
                "│  │ Rewards         │    │ Rubric Rewards  │                  │\n",
                "│  │ (Code, Math)    │    │ (Open-ended)    │                  │\n",
                "│  └────────┬────────┘    └────────┬────────┘                  │\n",
                "│           └──────────┬───────────┘                           │\n",
                "│                      ▼                                       │\n",
                "│              Combined Reward → Policy Update                 │\n",
                "└──────────────────────────────────────────────────────────────┘\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verifiable Reward System\n",
                "\n",
                "For tasks with objective ground truth (code, math)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class VerificationResult:\n",
                "    \"\"\"Result of verifiable reward computation.\"\"\"\n",
                "    passed: bool\n",
                "    score: float  # 0.0 or 1.0 for binary\n",
                "    details: str\n",
                "\n",
                "\n",
                "class CodeVerifier:\n",
                "    \"\"\"\n",
                "    Verify code correctness via test execution.\n",
                "    \n",
                "    Kimi K2 uses this for coding tasks where\n",
                "    unit tests provide objective verification.\n",
                "    \"\"\"\n",
                "    \n",
                "    def verify(self, code: str, test_cases: List[Dict]) -> VerificationResult:\n",
                "        \"\"\"\n",
                "        Run code against test cases.\n",
                "        \n",
                "        Args:\n",
                "            code: Generated code solution\n",
                "            test_cases: List of {'input': ..., 'expected': ...}\n",
                "        \n",
                "        Returns:\n",
                "            VerificationResult with pass/fail\n",
                "        \"\"\"\n",
                "        passed_count = 0\n",
                "        total = len(test_cases)\n",
                "        \n",
                "        for test in test_cases:\n",
                "            try:\n",
                "                # In production: sandboxed execution\n",
                "                # Here: simplified simulation\n",
                "                local_vars = {}\n",
                "                exec(code, {}, local_vars)\n",
                "                \n",
                "                # Check if function exists and run it\n",
                "                func_name = self._extract_function_name(code)\n",
                "                if func_name and func_name in local_vars:\n",
                "                    result = local_vars[func_name](test['input'])\n",
                "                    if result == test['expected']:\n",
                "                        passed_count += 1\n",
                "            except Exception as e:\n",
                "                # Test failed\n",
                "                pass\n",
                "        \n",
                "        success = passed_count == total\n",
                "        score = 1.0 if success else 0.0\n",
                "        \n",
                "        return VerificationResult(\n",
                "            passed=success,\n",
                "            score=score,\n",
                "            details=f\"{passed_count}/{total} tests passed\"\n",
                "        )\n",
                "    \n",
                "    def _extract_function_name(self, code: str) -> Optional[str]:\n",
                "        \"\"\"Extract the main function name from code.\"\"\"\n",
                "        match = re.search(r'def (\\w+)\\(', code)\n",
                "        return match.group(1) if match else None\n",
                "\n",
                "\n",
                "class MathVerifier:\n",
                "    \"\"\"\n",
                "    Verify mathematical answer correctness.\n",
                "    \n",
                "    Supports exact matching and numerical tolerance.\n",
                "    \"\"\"\n",
                "    \n",
                "    def verify(self, response: str, expected: str, \n",
                "               tolerance: float = 1e-6) -> VerificationResult:\n",
                "        \"\"\"\n",
                "        Check if extracted answer matches expected.\n",
                "        \n",
                "        Args:\n",
                "            response: Model's full response\n",
                "            expected: Ground truth answer\n",
                "            tolerance: Numerical tolerance for float comparison\n",
                "        \n",
                "        Returns:\n",
                "            VerificationResult\n",
                "        \"\"\"\n",
                "        # Extract answer from response (look for \"Answer: X\" pattern)\n",
                "        extracted = self._extract_answer(response)\n",
                "        \n",
                "        if extracted is None:\n",
                "            return VerificationResult(\n",
                "                passed=False, score=0.0,\n",
                "                details=\"No answer found in response\"\n",
                "            )\n",
                "        \n",
                "        # Try numerical comparison\n",
                "        try:\n",
                "            extracted_num = float(extracted)\n",
                "            expected_num = float(expected)\n",
                "            passed = abs(extracted_num - expected_num) < tolerance\n",
                "        except ValueError:\n",
                "            # String comparison\n",
                "            passed = extracted.strip().lower() == expected.strip().lower()\n",
                "        \n",
                "        return VerificationResult(\n",
                "            passed=passed,\n",
                "            score=1.0 if passed else 0.0,\n",
                "            details=f\"Extracted '{extracted}', expected '{expected}'\"\n",
                "        )\n",
                "    \n",
                "    def _extract_answer(self, response: str) -> Optional[str]:\n",
                "        \"\"\"Extract final answer from response.\"\"\"\n",
                "        patterns = [\n",
                "            r'[Aa]nswer[:\\s]+([\\d\\.\\-]+)',\n",
                "            r'\\\\boxed{([^}]+)}',\n",
                "            r'= ([\\d\\.\\-]+)$',\n",
                "        ]\n",
                "        for pattern in patterns:\n",
                "            match = re.search(pattern, response)\n",
                "            if match:\n",
                "                return match.group(1)\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: Verifiable rewards\n",
                "\n",
                "code_verifier = CodeVerifier()\n",
                "math_verifier = MathVerifier()\n",
                "\n",
                "# Code verification test\n",
                "code = '''\n",
                "def add_numbers(x):\n",
                "    return x[0] + x[1]\n",
                "'''\n",
                "\n",
                "test_cases = [\n",
                "    {'input': [1, 2], 'expected': 3},\n",
                "    {'input': [5, 7], 'expected': 12},\n",
                "]\n",
                "\n",
                "code_result = code_verifier.verify(code, test_cases)\n",
                "print(f\"Code verification: {code_result}\")\n",
                "\n",
                "# Math verification test\n",
                "response = \"Let x = 5. Then 2x + 3 = 13. Answer: 13\"\n",
                "math_result = math_verifier.verify(response, \"13\")\n",
                "print(f\"Math verification: {math_result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Self-Critique Rubric System\n",
                "\n",
                "For open-ended tasks where objective verification is impossible."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class RubricCriterion:\n",
                "    \"\"\"Single criterion in a rubric.\"\"\"\n",
                "    name: str\n",
                "    description: str\n",
                "    weight: float = 1.0\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class RubricScore:\n",
                "    \"\"\"Score for a single criterion.\"\"\"\n",
                "    criterion: str\n",
                "    score: float  # 0-1\n",
                "    reasoning: str\n",
                "\n",
                "\n",
                "class SelfCritiqueRubric:\n",
                "    \"\"\"\n",
                "    Kimi K2's self-critique system using rubrics.\n",
                "    \n",
                "    The model evaluates its own outputs against\n",
                "    human-defined criteria, generating preference\n",
                "    signals for RL training.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, criteria: List[RubricCriterion]):\n",
                "        self.criteria = criteria\n",
                "    \n",
                "    def generate_critique_prompt(self, query: str, response: str) -> str:\n",
                "        \"\"\"\n",
                "        Generate prompt for self-critique.\n",
                "        \n",
                "        Args:\n",
                "            query: Original user query\n",
                "            response: Model's response to evaluate\n",
                "        \n",
                "        Returns:\n",
                "            Prompt for the critique model\n",
                "        \"\"\"\n",
                "        criteria_text = \"\\n\".join([\n",
                "            f\"- {c.name}: {c.description}\" \n",
                "            for c in self.criteria\n",
                "        ])\n",
                "        \n",
                "        return f\"\"\"Evaluate the following response against these criteria:\n",
                "\n",
                "CRITERIA:\n",
                "{criteria_text}\n",
                "\n",
                "QUERY: {query}\n",
                "\n",
                "RESPONSE: {response}\n",
                "\n",
                "For each criterion, provide:\n",
                "1. Score (0-10)\n",
                "2. Brief reasoning\n",
                "\n",
                "Format: [CRITERION]: [SCORE]/10 - [REASONING]\"\"\"\n",
                "    \n",
                "    def parse_critique(self, critique_output: str) -> List[RubricScore]:\n",
                "        \"\"\"\n",
                "        Parse critique model output into scores.\n",
                "        \n",
                "        Args:\n",
                "            critique_output: Raw model critique\n",
                "        \n",
                "        Returns:\n",
                "            List of RubricScore objects\n",
                "        \"\"\"\n",
                "        scores = []\n",
                "        \n",
                "        for criterion in self.criteria:\n",
                "            # Look for pattern: \"CriterionName: X/10 - reasoning\"\n",
                "            pattern = rf\"{criterion.name}[:\\s]+(\\d+)/10[\\s\\-]+(.+?)(?=\\n|$)\"\n",
                "            match = re.search(pattern, critique_output, re.IGNORECASE)\n",
                "            \n",
                "            if match:\n",
                "                score_val = int(match.group(1)) / 10.0\n",
                "                reasoning = match.group(2).strip()\n",
                "            else:\n",
                "                # Default middle score if parsing fails\n",
                "                score_val = 0.5\n",
                "                reasoning = \"Unable to parse\"\n",
                "            \n",
                "            scores.append(RubricScore(\n",
                "                criterion=criterion.name,\n",
                "                score=score_val,\n",
                "                reasoning=reasoning\n",
                "            ))\n",
                "        \n",
                "        return scores\n",
                "    \n",
                "    def compute_reward(self, scores: List[RubricScore]) -> float:\n",
                "        \"\"\"\n",
                "        Compute weighted reward from rubric scores.\n",
                "        \n",
                "        Args:\n",
                "            scores: List of RubricScore objects\n",
                "        \n",
                "        Returns:\n",
                "            Weighted average reward in [0, 1]\n",
                "        \"\"\"\n",
                "        total_weight = sum(c.weight for c in self.criteria)\n",
                "        weighted_sum = 0.0\n",
                "        \n",
                "        for score, criterion in zip(scores, self.criteria):\n",
                "            weighted_sum += score.score * criterion.weight\n",
                "        \n",
                "        return weighted_sum / total_weight"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define standard Kimi K2 rubric for helpfulness\n",
                "\n",
                "KIMI_HELPFULNESS_RUBRIC = SelfCritiqueRubric([\n",
                "    RubricCriterion(\n",
                "        name=\"Accuracy\",\n",
                "        description=\"Information is factually correct and up-to-date\",\n",
                "        weight=2.0\n",
                "    ),\n",
                "    RubricCriterion(\n",
                "        name=\"Relevance\",\n",
                "        description=\"Response directly addresses the user's query\",\n",
                "        weight=1.5\n",
                "    ),\n",
                "    RubricCriterion(\n",
                "        name=\"Completeness\",\n",
                "        description=\"All aspects of the question are covered\",\n",
                "        weight=1.5\n",
                "    ),\n",
                "    RubricCriterion(\n",
                "        name=\"Clarity\",\n",
                "        description=\"Response is well-organized and easy to understand\",\n",
                "        weight=1.0\n",
                "    ),\n",
                "    RubricCriterion(\n",
                "        name=\"Helpfulness\",\n",
                "        description=\"Response provides actionable or useful information\",\n",
                "        weight=1.0\n",
                "    ),\n",
                "])\n",
                "\n",
                "# Generate critique prompt\n",
                "query = \"How do I learn Python effectively?\"\n",
                "response = \"To learn Python: 1) Start with basics. 2) Practice daily. 3) Build projects.\"\n",
                "\n",
                "prompt = KIMI_HELPFULNESS_RUBRIC.generate_critique_prompt(query, response)\n",
                "print(\"Self-Critique Prompt:\")\n",
                "print(\"=\" * 50)\n",
                "print(prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate critique output and parse\n",
                "\n",
                "simulated_critique = \"\"\"\n",
                "Accuracy: 8/10 - The advice is generally correct but lacks specifics.\n",
                "Relevance: 9/10 - Directly addresses the question about learning Python.\n",
                "Completeness: 6/10 - Missing details on resources and timeline.\n",
                "Clarity: 9/10 - Well-structured numbered list format.\n",
                "Helpfulness: 7/10 - Provides a starting framework but could be more actionable.\n",
                "\"\"\"\n",
                "\n",
                "scores = KIMI_HELPFULNESS_RUBRIC.parse_critique(simulated_critique)\n",
                "reward = KIMI_HELPFULNESS_RUBRIC.compute_reward(scores)\n",
                "\n",
                "print(\"Parsed Scores:\")\n",
                "for s in scores:\n",
                "    print(f\"  {s.criterion}: {s.score:.1f} - {s.reasoning}\")\n",
                "print(f\"\\nWeighted Rubric Reward: {reward:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Joint Reward Combiner\n",
                "\n",
                "Kimi K2 combines verifiable and rubric rewards into unified RL signal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class JointRewardSystem:\n",
                "    \"\"\"\n",
                "    Kimi K2's joint reward system combining\n",
                "    verifiable and self-critique rewards.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 code_verifier: CodeVerifier,\n",
                "                 math_verifier: MathVerifier,\n",
                "                 rubric: SelfCritiqueRubric,\n",
                "                 verifiable_weight: float = 0.7,\n",
                "                 rubric_weight: float = 0.3):\n",
                "        \n",
                "        self.code_verifier = code_verifier\n",
                "        self.math_verifier = math_verifier\n",
                "        self.rubric = rubric\n",
                "        self.verifiable_weight = verifiable_weight\n",
                "        self.rubric_weight = rubric_weight\n",
                "    \n",
                "    def compute_reward(self,\n",
                "                       response: str,\n",
                "                       task_type: str,\n",
                "                       ground_truth: Optional[str] = None,\n",
                "                       test_cases: Optional[List[Dict]] = None,\n",
                "                       critique_output: Optional[str] = None) -> Dict:\n",
                "        \"\"\"\n",
                "        Compute joint reward for a response.\n",
                "        \n",
                "        Args:\n",
                "            response: Model's response\n",
                "            task_type: 'code', 'math', or 'open_ended'\n",
                "            ground_truth: Expected answer (for math)\n",
                "            test_cases: Test cases (for code)\n",
                "            critique_output: Self-critique result (for open-ended)\n",
                "        \n",
                "        Returns:\n",
                "            Dict with 'total_reward', 'verifiable_reward', 'rubric_reward'\n",
                "        \"\"\"\n",
                "        verifiable_reward = 0.0\n",
                "        rubric_reward = 0.0\n",
                "        has_verifiable = False\n",
                "        \n",
                "        # Verifiable rewards\n",
                "        if task_type == 'code' and test_cases:\n",
                "            result = self.code_verifier.verify(response, test_cases)\n",
                "            verifiable_reward = result.score\n",
                "            has_verifiable = True\n",
                "        \n",
                "        elif task_type == 'math' and ground_truth:\n",
                "            result = self.math_verifier.verify(response, ground_truth)\n",
                "            verifiable_reward = result.score\n",
                "            has_verifiable = True\n",
                "        \n",
                "        # Rubric rewards (always applicable)\n",
                "        if critique_output:\n",
                "            scores = self.rubric.parse_critique(critique_output)\n",
                "            rubric_reward = self.rubric.compute_reward(scores)\n",
                "        \n",
                "        # Combine rewards\n",
                "        if has_verifiable:\n",
                "            # Use weighted combination\n",
                "            total = (self.verifiable_weight * verifiable_reward + \n",
                "                     self.rubric_weight * rubric_reward)\n",
                "        else:\n",
                "            # Open-ended: only rubric\n",
                "            total = rubric_reward\n",
                "        \n",
                "        return {\n",
                "            'total_reward': total,\n",
                "            'verifiable_reward': verifiable_reward,\n",
                "            'rubric_reward': rubric_reward,\n",
                "            'task_type': task_type,\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: Joint reward system\n",
                "\n",
                "joint_system = JointRewardSystem(\n",
                "    code_verifier=CodeVerifier(),\n",
                "    math_verifier=MathVerifier(),\n",
                "    rubric=KIMI_HELPFULNESS_RUBRIC,\n",
                "    verifiable_weight=0.7,\n",
                "    rubric_weight=0.3,\n",
                ")\n",
                "\n",
                "# Code task\n",
                "code_response = \"def add(x): return x[0] + x[1]\"\n",
                "code_reward = joint_system.compute_reward(\n",
                "    response=code_response,\n",
                "    task_type='code',\n",
                "    test_cases=[{'input': [1, 2], 'expected': 3}],\n",
                "    critique_output=\"Accuracy: 10/10\\nRelevance: 10/10\\nCompleteness: 8/10\\nClarity: 9/10\\nHelpfulness: 9/10\"\n",
                ")\n",
                "print(f\"Code Task: {code_reward}\")\n",
                "\n",
                "# Math task\n",
                "math_response = \"2 + 2 = 4. Answer: 4\"\n",
                "math_reward = joint_system.compute_reward(\n",
                "    response=math_response,\n",
                "    task_type='math',\n",
                "    ground_truth=\"4\",\n",
                "    critique_output=\"Accuracy: 10/10\\nRelevance: 10/10\\nCompleteness: 10/10\\nClarity: 10/10\\nHelpfulness: 10/10\"\n",
                ")\n",
                "print(f\"Math Task: {math_reward}\")\n",
                "\n",
                "# Open-ended task\n",
                "open_response = \"Here's how to cook pasta...\"\n",
                "open_reward = joint_system.compute_reward(\n",
                "    response=open_response,\n",
                "    task_type='open_ended',\n",
                "    critique_output=\"Accuracy: 8/10\\nRelevance: 9/10\\nCompleteness: 7/10\\nClarity: 8/10\\nHelpfulness: 8/10\"\n",
                ")\n",
                "print(f\"Open-ended Task: {open_reward}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Agentic Tool-Use Reward\n",
                "\n",
                "Kimi K2 excels at multi-step tool orchestration (200-300 calls)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ToolCall:\n",
                "    \"\"\"Representation of a tool call.\"\"\"\n",
                "    tool_name: str\n",
                "    arguments: Dict\n",
                "    result: Optional[str] = None\n",
                "    success: bool = True\n",
                "\n",
                "\n",
                "class AgenticRewardSystem:\n",
                "    \"\"\"\n",
                "    Reward system for multi-step agentic tasks.\n",
                "    \n",
                "    Evaluates tool call sequences for correctness,\n",
                "    efficiency, and goal achievement.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 step_reward: float = 0.01,\n",
                "                 success_bonus: float = 1.0,\n",
                "                 failure_penalty: float = -0.1,\n",
                "                 efficiency_weight: float = 0.2):\n",
                "        \n",
                "        self.step_reward = step_reward\n",
                "        self.success_bonus = success_bonus\n",
                "        self.failure_penalty = failure_penalty\n",
                "        self.efficiency_weight = efficiency_weight\n",
                "    \n",
                "    def compute_trajectory_reward(\n",
                "        self,\n",
                "        tool_calls: List[ToolCall],\n",
                "        goal_achieved: bool,\n",
                "        optimal_steps: Optional[int] = None\n",
                "    ) -> Dict:\n",
                "        \"\"\"\n",
                "        Compute reward for a sequence of tool calls.\n",
                "        \n",
                "        Args:\n",
                "            tool_calls: List of tool calls made\n",
                "            goal_achieved: Whether the final goal was met\n",
                "            optimal_steps: Known optimal number of steps (for efficiency)\n",
                "        \n",
                "        Returns:\n",
                "            Reward breakdown\n",
                "        \"\"\"\n",
                "        num_steps = len(tool_calls)\n",
                "        successful_steps = sum(1 for t in tool_calls if t.success)\n",
                "        failed_steps = num_steps - successful_steps\n",
                "        \n",
                "        # Base reward for progress\n",
                "        step_rewards = successful_steps * self.step_reward\n",
                "        step_penalties = failed_steps * self.failure_penalty\n",
                "        \n",
                "        # Goal achievement bonus\n",
                "        goal_reward = self.success_bonus if goal_achieved else 0.0\n",
                "        \n",
                "        # Efficiency bonus (if optimal is known)\n",
                "        efficiency_reward = 0.0\n",
                "        if optimal_steps and goal_achieved:\n",
                "            efficiency = optimal_steps / max(num_steps, 1)\n",
                "            efficiency_reward = self.efficiency_weight * efficiency\n",
                "        \n",
                "        total = step_rewards + step_penalties + goal_reward + efficiency_reward\n",
                "        \n",
                "        return {\n",
                "            'total_reward': total,\n",
                "            'step_rewards': step_rewards,\n",
                "            'step_penalties': step_penalties,\n",
                "            'goal_reward': goal_reward,\n",
                "            'efficiency_reward': efficiency_reward,\n",
                "            'num_steps': num_steps,\n",
                "            'success_rate': successful_steps / max(num_steps, 1),\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: Agentic reward\n",
                "\n",
                "agentic_system = AgenticRewardSystem()\n",
                "\n",
                "# Simulate a successful multi-step task\n",
                "tool_calls = [\n",
                "    ToolCall(\"search\", {\"query\": \"Python tutorial\"}, \"Found 10 results\", True),\n",
                "    ToolCall(\"read_url\", {\"url\": \"python.org\"}, \"Content loaded\", True),\n",
                "    ToolCall(\"extract\", {\"selector\": \"code\"}, \"Code examples\", True),\n",
                "    ToolCall(\"write_file\", {\"path\": \"notes.md\"}, \"Saved\", True),\n",
                "]\n",
                "\n",
                "result = agentic_system.compute_trajectory_reward(\n",
                "    tool_calls=tool_calls,\n",
                "    goal_achieved=True,\n",
                "    optimal_steps=4\n",
                ")\n",
                "\n",
                "print(\"Agentic Task Reward:\")\n",
                "for k, v in result.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Complete Joint RL Training Loop\n",
                "\n",
                "Simplified version of Kimi K2's joint RL framework."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class KimiK2JointRLTrainer:\n",
                "    \"\"\"\n",
                "    Simplified Kimi K2 Joint RL Trainer.\n",
                "    \n",
                "    Combines:\n",
                "    - Verifiable rewards (code, math)\n",
                "    - Self-critique rubric rewards\n",
                "    - Agentic trajectory rewards\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 joint_reward: JointRewardSystem,\n",
                "                 agentic_reward: AgenticRewardSystem):\n",
                "        \n",
                "        self.joint_reward = joint_reward\n",
                "        self.agentic_reward = agentic_reward\n",
                "    \n",
                "    def compute_batch_rewards(\n",
                "        self,\n",
                "        batch: List[Dict]\n",
                "    ) -> List[float]:\n",
                "        \"\"\"\n",
                "        Compute rewards for a batch of samples.\n",
                "        \n",
                "        Args:\n",
                "            batch: List of dicts with task info\n",
                "        \n",
                "        Returns:\n",
                "            List of reward values\n",
                "        \"\"\"\n",
                "        rewards = []\n",
                "        \n",
                "        for sample in batch:\n",
                "            task_type = sample.get('task_type', 'open_ended')\n",
                "            \n",
                "            if task_type == 'agentic':\n",
                "                # Use agentic reward system\n",
                "                result = self.agentic_reward.compute_trajectory_reward(\n",
                "                    tool_calls=sample.get('tool_calls', []),\n",
                "                    goal_achieved=sample.get('goal_achieved', False),\n",
                "                )\n",
                "                rewards.append(result['total_reward'])\n",
                "            else:\n",
                "                # Use joint verifiable + rubric system\n",
                "                result = self.joint_reward.compute_reward(\n",
                "                    response=sample.get('response', ''),\n",
                "                    task_type=task_type,\n",
                "                    ground_truth=sample.get('ground_truth'),\n",
                "                    test_cases=sample.get('test_cases'),\n",
                "                    critique_output=sample.get('critique_output'),\n",
                "                )\n",
                "                rewards.append(result['total_reward'])\n",
                "        \n",
                "        return rewards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: Complete Joint RL system\n",
                "\n",
                "trainer = KimiK2JointRLTrainer(\n",
                "    joint_reward=joint_system,\n",
                "    agentic_reward=agentic_system,\n",
                ")\n",
                "\n",
                "# Mixed batch of tasks\n",
                "batch = [\n",
                "    {'task_type': 'math', 'response': 'Answer: 42', 'ground_truth': '42',\n",
                "     'critique_output': 'Accuracy: 10/10\\nRelevance: 10/10\\nCompleteness: 10/10\\nClarity: 10/10\\nHelpfulness: 10/10'},\n",
                "    {'task_type': 'code', 'response': 'def f(x): return x*2',\n",
                "     'test_cases': [{'input': [5], 'expected': 10}],\n",
                "     'critique_output': 'Accuracy: 8/10\\nRelevance: 9/10\\nCompleteness: 7/10\\nClarity: 8/10\\nHelpfulness: 8/10'},\n",
                "    {'task_type': 'agentic',\n",
                "     'tool_calls': [ToolCall('search', {}, 'ok', True), ToolCall('parse', {}, 'ok', True)],\n",
                "     'goal_achieved': True},\n",
                "]\n",
                "\n",
                "rewards = trainer.compute_batch_rewards(batch)\n",
                "\n",
                "print(\"Batch Rewards:\")\n",
                "for i, (sample, reward) in enumerate(zip(batch, rewards)):\n",
                "    print(f\"  Sample {i+1} ({sample['task_type']}): {reward:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary: Kimi K2 Joint RL\n",
                "\n",
                "| Component | Purpose | Implementation |\n",
                "|-----------|---------|----------------|\n",
                "| **Verifiable Rewards** | Objective tasks | Code tests, math checks |\n",
                "| **Self-Critique Rubrics** | Open-ended tasks | LLM evaluates own output |\n",
                "| **Agentic Rewards** | Tool orchestration | Trajectory success + efficiency |\n",
                "| **Joint Weighting** | Combine signals | 70% verifiable + 30% rubric |\n",
                "\n",
                "### Key Innovations\n",
                "\n",
                "1. **Dual-reward system** scales to all task types\n",
                "2. **Self-critique** provides scalable feedback without human labels\n",
                "3. **Online critic improvement** using verifiable task data\n",
                "4. **Agentic synthesis pipeline** for tool-use training data\n",
                "\n",
                "---\n",
                "**Tier 3 Progress:** Section 10 Complete!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}