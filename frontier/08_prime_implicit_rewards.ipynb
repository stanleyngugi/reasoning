{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 08 — PRIME: Implicit Process Rewards\n",
                "\n",
                "> **Purpose:** Implement dense token-level rewards without expensive process annotations. PRIME learns a Q-function from outcome supervision only.\n",
                "\n",
                "**Key insight:** Train an outcome reward model, then extract token-level rewards as log probability ratios.\n",
                "\n",
                "$$r_{implicit}(s_t, a_t) = \\log \\pi_{PRM}(a_t|s_t) - \\log \\pi_{ref}(a_t|s_t)$$\n",
                "\n",
                "| Method | Annotations Required | Dense Rewards | Online Updates |\n",
                "|--------|---------------------|---------------|----------------|\n",
                "| Explicit PRM | Per-step labels ($500K+) | ✅ | ❌ |\n",
                "| ORM only | Outcome only | ❌ Sparse | ❌ |\n",
                "| **PRIME** | Outcome only | ✅ | ✅ |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "import copy\n",
                "\n",
                "torch.manual_seed(42)\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Credit Assignment Problem\n",
                "\n",
                "**Sparse rewards make learning hard.** When only the final answer is rewarded, which tokens contributed?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_credit_assignment():\n",
                "    \"\"\"\n",
                "    Demonstrate the credit assignment problem with sparse vs dense rewards.\n",
                "    \"\"\"\n",
                "    # Example reasoning trace\n",
                "    tokens = [\n",
                "        \"Let\", \"x\", \"=\", \"5\",           # Step 1: Define variable\n",
                "        \"Then\", \"2x\", \"=\", \"10\",        # Step 2: Correct multiplication\n",
                "        \"So\", \"2x\", \"+\", \"3\", \"=\", \"13\", # Step 3: Correct addition\n",
                "        \"Answer:\", \"13\"                  # Final answer\n",
                "    ]\n",
                "    \n",
                "    # Sparse reward: only final token gets signal\n",
                "    sparse_rewards = [0.0] * (len(tokens) - 1) + [1.0]\n",
                "    \n",
                "    # Dense (ideal PRM) rewards: each step gets credit\n",
                "    dense_rewards = [\n",
                "        0.1, 0.1, 0.1, 0.1,  # Step 1\n",
                "        0.2, 0.2, 0.2, 0.2,  # Step 2\n",
                "        0.2, 0.2, 0.2, 0.2, 0.2, 0.2,  # Step 3\n",
                "        0.3, 1.0             # Final\n",
                "    ]\n",
                "    \n",
                "    print(\"Credit Assignment Comparison:\")\n",
                "    print(f\"{'Token':<10} {'Sparse':<10} {'Dense':<10}\")\n",
                "    print(\"-\" * 30)\n",
                "    for tok, s, d in zip(tokens, sparse_rewards, dense_rewards):\n",
                "        print(f\"{tok:<10} {s:<10.1f} {d:<10.1f}\")\n",
                "    \n",
                "    print(f\"\\nTotal sparse signal: {sum(sparse_rewards):.1f}\")\n",
                "    print(f\"Total dense signal: {sum(dense_rewards):.1f}\")\n",
                "    print(\"Dense rewards give each step credit for correctness!\")\n",
                "\n",
                "visualize_credit_assignment()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Setup: Policy, Reference, and Implicit PRM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TinyLM(nn.Module):\n",
                "    \"\"\"Minimal LM for PRIME experiments.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size=100, hidden_size=64):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
                "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
                "        self.head = nn.Linear(hidden_size, vocab_size)\n",
                "        self.vocab_size = vocab_size\n",
                "    \n",
                "    def forward(self, x):\n",
                "        h = self.embed(x)\n",
                "        h, _ = self.rnn(h)\n",
                "        return self.head(h)\n",
                "    \n",
                "    def get_token_log_probs(self, input_ids: torch.Tensor, \n",
                "                            labels: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Get log probability for each token.\n",
                "        \n",
                "        Returns:\n",
                "            log_probs: (batch, seq_len) per-token log probs\n",
                "        \"\"\"\n",
                "        logits = self(input_ids)\n",
                "        log_probs = F.log_softmax(logits, dim=-1)\n",
                "        return torch.gather(log_probs, dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
                "\n",
                "\n",
                "def create_reference_model(model: nn.Module) -> nn.Module:\n",
                "    \"\"\"Create frozen reference model.\"\"\"\n",
                "    ref = copy.deepcopy(model)\n",
                "    ref.eval()\n",
                "    for p in ref.parameters():\n",
                "        p.requires_grad = False\n",
                "    return ref"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Implicit Process Reward Computation\n",
                "\n",
                "The key PRIME formula: token-level reward from log probability ratio.\n",
                "\n",
                "$$r_{implicit}(t) = \\log \\pi_{PRM}(a_t|s_t) - \\log \\pi_{ref}(a_t|s_t)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_implicit_process_rewards(\n",
                "    prm_log_probs: torch.Tensor,\n",
                "    ref_log_probs: torch.Tensor,\n",
                "    mask: Optional[torch.Tensor] = None,\n",
                ") -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Compute token-level implicit process rewards.\n",
                "    \n",
                "    This is the core PRIME insight: the log probability ratio\n",
                "    between a trained model and reference acts as a Q-function,\n",
                "    providing dense rewards without explicit process labels.\n",
                "    \n",
                "    Args:\n",
                "        prm_log_probs: (batch, seq_len) log probs from implicit PRM\n",
                "        ref_log_probs: (batch, seq_len) log probs from frozen reference\n",
                "        mask: Optional (batch, seq_len) mask for valid tokens\n",
                "    \n",
                "    Returns:\n",
                "        implicit_rewards: (batch, seq_len) per-token rewards\n",
                "    \"\"\"\n",
                "    # Implicit reward = log ratio (acts as Q-value difference)\n",
                "    implicit_rewards = prm_log_probs - ref_log_probs\n",
                "    \n",
                "    if mask is not None:\n",
                "        implicit_rewards = implicit_rewards * mask\n",
                "    \n",
                "    return implicit_rewards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: Implicit rewards\n",
                "\n",
                "policy = TinyLM().to(device)\n",
                "ref_model = create_reference_model(policy).to(device)\n",
                "\n",
                "# Modify policy slightly to see difference\n",
                "with torch.no_grad():\n",
                "    policy.head.weight.add_(torch.randn_like(policy.head.weight) * 0.1)\n",
                "\n",
                "# Sample input\n",
                "batch_size, seq_len = 2, 10\n",
                "input_ids = torch.randint(0, 100, (batch_size, seq_len), device=device)\n",
                "labels = torch.randint(0, 100, (batch_size, seq_len), device=device)\n",
                "\n",
                "# Compute log probs\n",
                "prm_logps = policy.get_token_log_probs(input_ids, labels)\n",
                "ref_logps = ref_model.get_token_log_probs(input_ids, labels)\n",
                "\n",
                "# Compute implicit rewards\n",
                "implicit_rewards = compute_implicit_process_rewards(prm_logps, ref_logps)\n",
                "\n",
                "print(\"Implicit Process Rewards (per token):\")\n",
                "print(f\"  Shape: {implicit_rewards.shape}\")\n",
                "print(f\"  Mean: {implicit_rewards.mean().item():.4f}\")\n",
                "print(f\"  Std: {implicit_rewards.std().item():.4f}\")\n",
                "print(f\"  Sample: {implicit_rewards[0, :5].tolist()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Monte Carlo Advantage Estimation\n",
                "\n",
                "PRIME combines implicit process rewards with sparse outcome rewards."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_prime_advantages(\n",
                "    implicit_rewards: torch.Tensor,\n",
                "    outcome_rewards: torch.Tensor,\n",
                "    response_lengths: torch.Tensor,\n",
                "    gamma: float = 1.0,\n",
                "    lambda_implicit: float = 0.5,\n",
                ") -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Compute advantages combining implicit and outcome rewards.\n",
                "    \n",
                "    Uses Monte Carlo estimation: future return from each token.\n",
                "    \n",
                "    Args:\n",
                "        implicit_rewards: (batch, seq_len) per-token implicit rewards\n",
                "        outcome_rewards: (batch,) sparse outcome reward per sequence\n",
                "        response_lengths: (batch,) length of each response\n",
                "        gamma: Discount factor (1.0 for no discounting)\n",
                "        lambda_implicit: Weight for implicit vs outcome rewards\n",
                "    \n",
                "    Returns:\n",
                "        advantages: (batch, seq_len) per-token advantages\n",
                "    \"\"\"\n",
                "    batch_size, seq_len = implicit_rewards.shape\n",
                "    device = implicit_rewards.device\n",
                "    \n",
                "    # Combine rewards: spread outcome reward + implicit rewards\n",
                "    combined_rewards = implicit_rewards.clone() * lambda_implicit\n",
                "    \n",
                "    # Add outcome reward at the last token of each sequence\n",
                "    for i in range(batch_size):\n",
                "        end_idx = min(int(response_lengths[i].item()) - 1, seq_len - 1)\n",
                "        combined_rewards[i, end_idx] += outcome_rewards[i] * (1 - lambda_implicit)\n",
                "    \n",
                "    # Monte Carlo return: sum of future rewards from each position\n",
                "    # (Simplified: no discounting for gamma=1.0)\n",
                "    advantages = torch.zeros_like(combined_rewards)\n",
                "    \n",
                "    for i in range(batch_size):\n",
                "        end_idx = int(response_lengths[i].item())\n",
                "        cumsum = 0.0\n",
                "        for t in range(end_idx - 1, -1, -1):\n",
                "            cumsum = combined_rewards[i, t] + gamma * cumsum\n",
                "            advantages[i, t] = cumsum\n",
                "    \n",
                "    # Normalize advantages\n",
                "    mask = torch.arange(seq_len, device=device).unsqueeze(0) < response_lengths.unsqueeze(1)\n",
                "    valid_advantages = advantages[mask]\n",
                "    if len(valid_advantages) > 1:\n",
                "        mean = valid_advantages.mean()\n",
                "        std = valid_advantages.std() + 1e-8\n",
                "        advantages = (advantages - mean) / std\n",
                "    \n",
                "    return advantages * mask.float()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: PRIME advantages\n",
                "\n",
                "implicit_rewards = torch.randn(4, 20, device=device) * 0.1  # Small per-token\n",
                "outcome_rewards = torch.tensor([1.0, 0.0, 1.0, 0.0], device=device)  # Win/loss\n",
                "response_lengths = torch.tensor([15, 18, 12, 20], device=device)\n",
                "\n",
                "advantages = compute_prime_advantages(\n",
                "    implicit_rewards, outcome_rewards, response_lengths,\n",
                "    lambda_implicit=0.5\n",
                ")\n",
                "\n",
                "print(\"PRIME Advantages:\")\n",
                "print(f\"  Shape: {advantages.shape}\")\n",
                "for i in range(4):\n",
                "    print(f\"  Seq {i} (outcome={outcome_rewards[i]:.0f}): \"\n",
                "          f\"first 5 = {advantages[i, :5].tolist()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. PRIME Loss Function\n",
                "\n",
                "PPO-style policy loss with implicit process rewards."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_prime_policy_loss(\n",
                "    policy_log_probs: torch.Tensor,\n",
                "    old_log_probs: torch.Tensor,\n",
                "    advantages: torch.Tensor,\n",
                "    mask: torch.Tensor,\n",
                "    epsilon: float = 0.2,\n",
                ") -> Dict[str, torch.Tensor]:\n",
                "    \"\"\"\n",
                "    Compute PRIME policy loss (PPO-style with dense advantages).\n",
                "    \n",
                "    Args:\n",
                "        policy_log_probs: (batch, seq_len) current policy log probs\n",
                "        old_log_probs: (batch, seq_len) old policy log probs\n",
                "        advantages: (batch, seq_len) PRIME advantages\n",
                "        mask: (batch, seq_len) valid token mask\n",
                "        epsilon: PPO clip parameter\n",
                "    \n",
                "    Returns:\n",
                "        Dict with 'loss', 'clip_fraction'\n",
                "    \"\"\"\n",
                "    # Importance sampling ratio\n",
                "    log_ratio = policy_log_probs - old_log_probs\n",
                "    ratio = torch.exp(log_ratio)\n",
                "    \n",
                "    # Clipped ratio\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    \n",
                "    # PPO objective\n",
                "    obj1 = ratio * advantages\n",
                "    obj2 = clipped_ratio * advantages\n",
                "    policy_loss = -torch.min(obj1, obj2)\n",
                "    \n",
                "    # Apply mask and average\n",
                "    masked_loss = (policy_loss * mask).sum() / mask.sum()\n",
                "    \n",
                "    # Tracking metrics\n",
                "    clipped = (ratio < 1 - epsilon) | (ratio > 1 + epsilon)\n",
                "    clip_fraction = (clipped * mask).sum() / mask.sum()\n",
                "    \n",
                "    return {\n",
                "        'loss': masked_loss,\n",
                "        'clip_fraction': clip_fraction,\n",
                "        'mean_ratio': (ratio * mask).sum() / mask.sum(),\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Online PRM Update\n",
                "\n",
                "PRIME's key innovation: update the implicit PRM alongside the policy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_prm_update_loss(\n",
                "    prm_log_probs_correct: torch.Tensor,\n",
                "    prm_log_probs_incorrect: torch.Tensor,\n",
                "    correct_lengths: torch.Tensor,\n",
                "    incorrect_lengths: torch.Tensor,\n",
                ") -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Compute loss for online PRM update.\n",
                "    \n",
                "    Train implicit PRM to distinguish correct vs incorrect outcomes.\n",
                "    \n",
                "    Args:\n",
                "        prm_log_probs_*: (batch, seq_len) log probs for correct/incorrect\n",
                "        *_lengths: (batch,) sequence lengths\n",
                "    \n",
                "    Returns:\n",
                "        PRM update loss (preference-style)\n",
                "    \"\"\"\n",
                "    batch_size = prm_log_probs_correct.shape[0]\n",
                "    \n",
                "    # Sum log probs for each sequence\n",
                "    correct_sum = torch.zeros(batch_size, device=prm_log_probs_correct.device)\n",
                "    incorrect_sum = torch.zeros(batch_size, device=prm_log_probs_incorrect.device)\n",
                "    \n",
                "    for i in range(batch_size):\n",
                "        correct_sum[i] = prm_log_probs_correct[i, :int(correct_lengths[i])].sum()\n",
                "        incorrect_sum[i] = prm_log_probs_incorrect[i, :int(incorrect_lengths[i])].sum()\n",
                "    \n",
                "    # Preference loss: PRM should prefer correct over incorrect\n",
                "    logits = correct_sum - incorrect_sum\n",
                "    loss = -F.logsigmoid(logits).mean()\n",
                "    \n",
                "    return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Complete PRIME Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PRIMETrainer:\n",
                "    \"\"\"\n",
                "    PRIME: Process Reinforcement through Implicit Rewards.\n",
                "    \n",
                "    Key features:\n",
                "    - Implicit PRM initialized from SFT, updated online\n",
                "    - Dense token-level rewards from log probability ratio\n",
                "    - Monte Carlo advantage estimation\n",
                "    - PPO policy updates\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 policy: nn.Module,\n",
                "                 ref_model: nn.Module,\n",
                "                 implicit_prm: nn.Module,\n",
                "                 epsilon: float = 0.2,\n",
                "                 lambda_implicit: float = 0.5,\n",
                "                 policy_lr: float = 1e-5,\n",
                "                 prm_lr: float = 1e-5,\n",
                "                 update_prm: bool = True):\n",
                "        \n",
                "        self.policy = policy\n",
                "        self.ref_model = ref_model\n",
                "        self.implicit_prm = implicit_prm\n",
                "        self.epsilon = epsilon\n",
                "        self.lambda_implicit = lambda_implicit\n",
                "        self.update_prm = update_prm\n",
                "        \n",
                "        self.policy_optimizer = torch.optim.AdamW(policy.parameters(), lr=policy_lr)\n",
                "        self.prm_optimizer = torch.optim.AdamW(implicit_prm.parameters(), lr=prm_lr)\n",
                "    \n",
                "    def compute_implicit_rewards(self, input_ids: torch.Tensor, \n",
                "                                  labels: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Get implicit process rewards for a batch.\"\"\"\n",
                "        with torch.no_grad():\n",
                "            prm_logps = self.implicit_prm.get_token_log_probs(input_ids, labels)\n",
                "            ref_logps = self.ref_model.get_token_log_probs(input_ids, labels)\n",
                "        \n",
                "        return compute_implicit_process_rewards(prm_logps, ref_logps)\n",
                "    \n",
                "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
                "        \"\"\"\n",
                "        Single PRIME training step.\n",
                "        \n",
                "        Args:\n",
                "            batch: Dict with:\n",
                "                - 'input_ids': (batch, seq_len)\n",
                "                - 'labels': (batch, seq_len)\n",
                "                - 'outcome_rewards': (batch,) sparse outcome\n",
                "                - 'response_lengths': (batch,)\n",
                "                - 'old_log_probs': (batch, seq_len) for importance sampling\n",
                "        \n",
                "        Returns:\n",
                "            Training metrics\n",
                "        \"\"\"\n",
                "        self.policy.train()\n",
                "        \n",
                "        # Step 1: Compute implicit process rewards\n",
                "        implicit_rewards = self.compute_implicit_rewards(\n",
                "            batch['input_ids'], batch['labels']\n",
                "        )\n",
                "        \n",
                "        # Step 2: Compute PRIME advantages\n",
                "        advantages = compute_prime_advantages(\n",
                "            implicit_rewards,\n",
                "            batch['outcome_rewards'],\n",
                "            batch['response_lengths'],\n",
                "            lambda_implicit=self.lambda_implicit,\n",
                "        )\n",
                "        \n",
                "        # Step 3: Compute current policy log probs\n",
                "        policy_logps = self.policy.get_token_log_probs(\n",
                "            batch['input_ids'], batch['labels']\n",
                "        )\n",
                "        \n",
                "        # Create mask\n",
                "        seq_len = batch['input_ids'].shape[1]\n",
                "        mask = torch.arange(seq_len, device=batch['input_ids'].device).unsqueeze(0) \\\n",
                "               < batch['response_lengths'].unsqueeze(1)\n",
                "        mask = mask.float()\n",
                "        \n",
                "        # Step 4: Compute policy loss\n",
                "        loss_dict = compute_prime_policy_loss(\n",
                "            policy_logps,\n",
                "            batch['old_log_probs'],\n",
                "            advantages,\n",
                "            mask,\n",
                "            epsilon=self.epsilon,\n",
                "        )\n",
                "        \n",
                "        # Step 5: Update policy\n",
                "        self.policy_optimizer.zero_grad()\n",
                "        loss_dict['loss'].backward()\n",
                "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
                "        self.policy_optimizer.step()\n",
                "        \n",
                "        metrics = {k: v.item() for k, v in loss_dict.items()}\n",
                "        metrics['implicit_reward_mean'] = implicit_rewards.mean().item()\n",
                "        \n",
                "        return metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: PRIME Trainer\n",
                "\n",
                "policy = TinyLM().to(device)\n",
                "ref_model = create_reference_model(policy).to(device)\n",
                "implicit_prm = TinyLM().to(device)  # Separate model for PRM\n",
                "\n",
                "trainer = PRIMETrainer(\n",
                "    policy=policy,\n",
                "    ref_model=ref_model,\n",
                "    implicit_prm=implicit_prm,\n",
                "    lambda_implicit=0.5,\n",
                "    policy_lr=1e-4,\n",
                ")\n",
                "\n",
                "# Create synthetic batch\n",
                "batch_size, seq_len = 4, 30\n",
                "batch = {\n",
                "    'input_ids': torch.randint(0, 100, (batch_size, seq_len), device=device),\n",
                "    'labels': torch.randint(0, 100, (batch_size, seq_len), device=device),\n",
                "    'outcome_rewards': torch.tensor([1.0, 0.0, 1.0, 0.0], device=device),\n",
                "    'response_lengths': torch.tensor([25, 30, 20, 28], device=device),\n",
                "    'old_log_probs': torch.randn(batch_size, seq_len, device=device) - 5,\n",
                "}\n",
                "\n",
                "print(\"PRIME Training:\")\n",
                "print(f\"{'Step':>5} {'Loss':>10} {'Clip%':>10} {'ImplRwd':>12}\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for step in range(5):\n",
                "    metrics = trainer.train_step(batch)\n",
                "    print(f\"{step:>5} {metrics['loss']:>10.4f} \"\n",
                "          f\"{metrics['clip_fraction']*100:>9.1f}% \"\n",
                "          f\"{metrics['implicit_reward_mean']:>12.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Key Takeaways\n",
                "\n",
                "| Aspect | PRIME Approach |\n",
                "|--------|---------------|\n",
                "| **Dense Rewards** | Log ratio: `log π_prm - log π_ref` |\n",
                "| **No Process Labels** | Train PRM on outcomes, extract token rewards |\n",
                "| **Online Updates** | PRM evolves with policy, prevents reward hacking |\n",
                "| **Sample Efficiency** | 2.5x vs RLOO baseline |\n",
                "\n",
                "### Production Configuration\n",
                "\n",
                "```python\n",
                "config = {\n",
                "    'lambda_implicit': 0.5,    # Balance implicit vs outcome\n",
                "    'epsilon': 0.2,            # PPO clip\n",
                "    'policy_lr': 1e-5,         # Policy learning rate\n",
                "    'prm_lr': 1e-5,            # PRM learning rate\n",
                "    'gamma': 1.0,              # No discounting for reasoning\n",
                "    'update_prm_every': 1,     # Online PRM updates\n",
                "}\n",
                "```\n",
                "\n",
                "---\n",
                "**Tier 3 Started!** Next: Conceptual deep-dives on Kimi K2, DeepSeek V3.2, etc."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}