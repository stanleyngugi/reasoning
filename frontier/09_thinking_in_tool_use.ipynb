{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 09 — Thinking in Tool-Use (DeepSeek V3.2)\n",
                "\n",
                "> **Purpose:** Implement the \"reasoning while calling tools\" paradigm. The model maintains its thinking thread across multiple tool invocations.\n",
                "\n",
                "**Key insight:** Traditional models \"reboot\" their reasoning after each tool call. DeepSeek V3.2 keeps reasoning active throughout.\n",
                "\n",
                "| Approach | After Tool Call | Reasoning Continuity |\n",
                "|----------|-----------------|---------------------|\n",
                "| Traditional | Model restarts | ❌ Lost |\n",
                "| **Thinking-in-Tool-Use** | Resume reasoning | ✅ Preserved |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import re\n",
                "from typing import Dict, List, Optional, Callable, Any\n",
                "from dataclasses import dataclass, field\n",
                "from enum import Enum"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. DeepSeek V3.2 Architecture Overview\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────────────┐\n",
                "│                    DeepSeek V3.2                                 │\n",
                "│                    671B params (37B active)                      │\n",
                "├─────────────────────────────────────────────────────────────────┤\n",
                "│                                                                  │\n",
                "│    ┌─────────────┐     ┌─────────────┐     ┌─────────────┐      │\n",
                "│    │   THINK     │ ──► │   TOOL      │ ──► │   THINK     │      │\n",
                "│    │  (reason)   │     │  (call API) │     │  (continue) │      │\n",
                "│    └─────────────┘     └─────────────┘     └─────────────┘      │\n",
                "│          │                   │                   │               │\n",
                "│          └───────────────────┴───────────────────┘               │\n",
                "│                    Reasoning Thread Preserved                    │\n",
                "│                                                                  │\n",
                "├─────────────────────────────────────────────────────────────────┤\n",
                "│    Trained on: 1,800+ environments, 85,000+ instructions        │\n",
                "│    Modes: Thinking (CoT) | Non-Thinking (Direct)                │\n",
                "│    Context: 128K tokens (DSA attention)                         │\n",
                "└─────────────────────────────────────────────────────────────────┘\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tool Definition and Registry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ToolDefinition:\n",
                "    \"\"\"Definition of an available tool.\"\"\"\n",
                "    name: str\n",
                "    description: str\n",
                "    parameters: Dict[str, Any]\n",
                "    required: List[str] = field(default_factory=list)\n",
                "    \n",
                "    def to_schema(self) -> Dict:\n",
                "        \"\"\"Convert to OpenAI-style function schema.\"\"\"\n",
                "        return {\n",
                "            \"type\": \"function\",\n",
                "            \"function\": {\n",
                "                \"name\": self.name,\n",
                "                \"description\": self.description,\n",
                "                \"parameters\": {\n",
                "                    \"type\": \"object\",\n",
                "                    \"properties\": self.parameters,\n",
                "                    \"required\": self.required,\n",
                "                }\n",
                "            }\n",
                "        }\n",
                "\n",
                "\n",
                "class ToolRegistry:\n",
                "    \"\"\"Registry of available tools.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.tools: Dict[str, ToolDefinition] = {}\n",
                "        self.executors: Dict[str, Callable] = {}\n",
                "    \n",
                "    def register(self, definition: ToolDefinition, \n",
                "                 executor: Callable) -> None:\n",
                "        \"\"\"Register a tool with its executor.\"\"\"\n",
                "        self.tools[definition.name] = definition\n",
                "        self.executors[definition.name] = executor\n",
                "    \n",
                "    def get_schemas(self) -> List[Dict]:\n",
                "        \"\"\"Get all tool schemas for LLM.\"\"\"\n",
                "        return [t.to_schema() for t in self.tools.values()]\n",
                "    \n",
                "    def execute(self, name: str, arguments: Dict) -> str:\n",
                "        \"\"\"Execute a tool and return result.\"\"\"\n",
                "        if name not in self.executors:\n",
                "            return f\"Error: Unknown tool '{name}'\"\n",
                "        try:\n",
                "            result = self.executors[name](**arguments)\n",
                "            return json.dumps(result) if not isinstance(result, str) else result\n",
                "        except Exception as e:\n",
                "            return f\"Error executing {name}: {str(e)}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Register example tools\n",
                "\n",
                "registry = ToolRegistry()\n",
                "\n",
                "# Calculator tool\n",
                "registry.register(\n",
                "    ToolDefinition(\n",
                "        name=\"calculator\",\n",
                "        description=\"Perform arithmetic calculations\",\n",
                "        parameters={\n",
                "            \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"}\n",
                "        },\n",
                "        required=[\"expression\"]\n",
                "    ),\n",
                "    lambda expression: {\"result\": eval(expression)}  # Simplified; use safe eval in prod\n",
                ")\n",
                "\n",
                "# Search tool\n",
                "registry.register(\n",
                "    ToolDefinition(\n",
                "        name=\"search\",\n",
                "        description=\"Search for information on a topic\",\n",
                "        parameters={\n",
                "            \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n",
                "        },\n",
                "        required=[\"query\"]\n",
                "    ),\n",
                "    lambda query: {\"results\": [f\"Result for: {query}\"]}\n",
                ")\n",
                "\n",
                "# Get weather tool\n",
                "registry.register(\n",
                "    ToolDefinition(\n",
                "        name=\"get_weather\",\n",
                "        description=\"Get current weather for a location\",\n",
                "        parameters={\n",
                "            \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n",
                "        },\n",
                "        required=[\"location\"]\n",
                "    ),\n",
                "    lambda location: {\"temperature\": 22, \"condition\": \"sunny\", \"location\": location}\n",
                ")\n",
                "\n",
                "print(f\"Registered {len(registry.tools)} tools:\")\n",
                "for name in registry.tools:\n",
                "    print(f\"  - {name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Thinking Thread Management\n",
                "\n",
                "The key innovation: preserve reasoning state across tool calls."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ThinkingMode(Enum):\n",
                "    \"\"\"Model operating mode.\"\"\"\n",
                "    THINKING = \"thinking\"      # Full CoT reasoning\n",
                "    NON_THINKING = \"direct\"    # Direct response\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class ThinkingState:\n",
                "    \"\"\"State of the reasoning thread.\"\"\"\n",
                "    thoughts: List[str] = field(default_factory=list)\n",
                "    tool_calls: List[Dict] = field(default_factory=list)\n",
                "    tool_results: List[Dict] = field(default_factory=list)\n",
                "    is_complete: bool = False\n",
                "    final_answer: Optional[str] = None\n",
                "    \n",
                "    def add_thought(self, thought: str) -> None:\n",
                "        \"\"\"Add a reasoning step.\"\"\"\n",
                "        self.thoughts.append(thought)\n",
                "    \n",
                "    def add_tool_call(self, tool_name: str, arguments: Dict) -> None:\n",
                "        \"\"\"Record a tool call.\"\"\"\n",
                "        self.tool_calls.append({\n",
                "            \"tool\": tool_name,\n",
                "            \"arguments\": arguments,\n",
                "            \"step\": len(self.thoughts)\n",
                "        })\n",
                "    \n",
                "    def add_tool_result(self, tool_name: str, result: str) -> None:\n",
                "        \"\"\"Record a tool result.\"\"\"\n",
                "        self.tool_results.append({\n",
                "            \"tool\": tool_name,\n",
                "            \"result\": result,\n",
                "            \"step\": len(self.thoughts)\n",
                "        })\n",
                "    \n",
                "    def get_context(self) -> str:\n",
                "        \"\"\"Get full reasoning context for continuation.\"\"\"\n",
                "        lines = []\n",
                "        \n",
                "        for i, thought in enumerate(self.thoughts):\n",
                "            lines.append(f\"[Thought {i+1}] {thought}\")\n",
                "            \n",
                "            # Add any tool calls/results at this step\n",
                "            for tc in self.tool_calls:\n",
                "                if tc['step'] == i:\n",
                "                    lines.append(f\"  → Tool: {tc['tool']}({tc['arguments']})\")\n",
                "            \n",
                "            for tr in self.tool_results:\n",
                "                if tr['step'] == i:\n",
                "                    lines.append(f\"  ← Result: {tr['result']}\")\n",
                "        \n",
                "        return \"\\n\".join(lines)\n",
                "    \n",
                "    def __str__(self) -> str:\n",
                "        return self.get_context()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Thinking-Aware Agent\n",
                "\n",
                "Agent that maintains reasoning across tool invocations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ThinkingAgent:\n",
                "    \"\"\"\n",
                "    Agent that maintains reasoning thread across tool calls.\n",
                "    \n",
                "    Inspired by DeepSeek V3.2's thinking-in-tool-use approach.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 tool_registry: ToolRegistry,\n",
                "                 mode: ThinkingMode = ThinkingMode.THINKING,\n",
                "                 max_steps: int = 10):\n",
                "        \n",
                "        self.registry = tool_registry\n",
                "        self.mode = mode\n",
                "        self.max_steps = max_steps\n",
                "    \n",
                "    def think(self, query: str) -> ThinkingState:\n",
                "        \"\"\"\n",
                "        Process a query with maintained reasoning.\n",
                "        \n",
                "        Args:\n",
                "            query: User's question/task\n",
                "        \n",
                "        Returns:\n",
                "            ThinkingState with full reasoning trace\n",
                "        \"\"\"\n",
                "        state = ThinkingState()\n",
                "        \n",
                "        # Initial thought\n",
                "        state.add_thought(f\"I need to answer: {query}\")\n",
                "        \n",
                "        # Simulate reasoning loop\n",
                "        step = 0\n",
                "        while step < self.max_steps and not state.is_complete:\n",
                "            step += 1\n",
                "            \n",
                "            # Decide: think more, call tool, or answer\n",
                "            action = self._decide_action(query, state)\n",
                "            \n",
                "            if action['type'] == 'think':\n",
                "                state.add_thought(action['content'])\n",
                "            \n",
                "            elif action['type'] == 'tool_call':\n",
                "                # Record the decision to call tool\n",
                "                state.add_thought(\n",
                "                    f\"I need to use {action['tool']} to {action['reason']}\"\n",
                "                )\n",
                "                state.add_tool_call(action['tool'], action['arguments'])\n",
                "                \n",
                "                # Execute tool\n",
                "                result = self.registry.execute(\n",
                "                    action['tool'], action['arguments']\n",
                "                )\n",
                "                state.add_tool_result(action['tool'], result)\n",
                "                \n",
                "                # Continue reasoning with result\n",
                "                state.add_thought(\n",
                "                    f\"The {action['tool']} returned: {result}. \"\n",
                "                    f\"I can now continue my reasoning.\"\n",
                "                )\n",
                "            \n",
                "            elif action['type'] == 'answer':\n",
                "                state.add_thought(f\"I now have enough information to answer.\")\n",
                "                state.final_answer = action['content']\n",
                "                state.is_complete = True\n",
                "        \n",
                "        return state\n",
                "    \n",
                "    def _decide_action(self, query: str, state: ThinkingState) -> Dict:\n",
                "        \"\"\"\n",
                "        Decide next action based on current state.\n",
                "        \n",
                "        In production: this would be the LLM inference call.\n",
                "        Here: simplified simulation.\n",
                "        \"\"\"\n",
                "        num_thoughts = len(state.thoughts)\n",
                "        num_tool_calls = len(state.tool_calls)\n",
                "        \n",
                "        # Simulation logic\n",
                "        if \"weather\" in query.lower() and num_tool_calls == 0:\n",
                "            # Extract location (simplified)\n",
                "            location = \"London\"  # Would be extracted by LLM\n",
                "            return {\n",
                "                'type': 'tool_call',\n",
                "                'tool': 'get_weather',\n",
                "                'arguments': {'location': location},\n",
                "                'reason': 'get current weather data'\n",
                "            }\n",
                "        \n",
                "        elif \"calculate\" in query.lower() and num_tool_calls == 0:\n",
                "            # Extract expression (simplified)\n",
                "            expr = \"2 + 2\"  # Would be extracted by LLM\n",
                "            return {\n",
                "                'type': 'tool_call',\n",
                "                'tool': 'calculator',\n",
                "                'arguments': {'expression': expr},\n",
                "                'reason': 'perform the calculation'\n",
                "            }\n",
                "        \n",
                "        elif num_thoughts < 3:\n",
                "            return {\n",
                "                'type': 'think',\n",
                "                'content': f\"Analyzing the query (step {num_thoughts + 1})...\"\n",
                "            }\n",
                "        \n",
                "        else:\n",
                "            return {\n",
                "                'type': 'answer',\n",
                "                'content': f\"Based on my reasoning, here is the answer to '{query}'\"\n",
                "            }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: Thinking agent with tool use\n",
                "\n",
                "agent = ThinkingAgent(registry, mode=ThinkingMode.THINKING)\n",
                "\n",
                "# Query requiring tool use\n",
                "state = agent.think(\"What's the weather in London?\")\n",
                "\n",
                "print(\"Reasoning Trace:\")\n",
                "print(\"=\" * 50)\n",
                "print(state.get_context())\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nFinal Answer: {state.final_answer}\")\n",
                "print(f\"Tool Calls Made: {len(state.tool_calls)}\")\n",
                "print(f\"Reasoning Steps: {len(state.thoughts)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Multi-Step Tool Orchestration\n",
                "\n",
                "Handle complex tasks requiring multiple tool calls."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiStepOrchestrator:\n",
                "    \"\"\"\n",
                "    Orchestrate complex multi-step tool interactions.\n",
                "    \n",
                "    DeepSeek V3.2 can handle 100+ sequential tool calls\n",
                "    while maintaining reasoning coherence.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, agent: ThinkingAgent):\n",
                "        self.agent = agent\n",
                "    \n",
                "    def execute_plan(self, plan: List[Dict]) -> ThinkingState:\n",
                "        \"\"\"\n",
                "        Execute a multi-step plan.\n",
                "        \n",
                "        Args:\n",
                "            plan: List of steps, each with 'action' and 'params'\n",
                "        \n",
                "        Returns:\n",
                "            ThinkingState with full execution trace\n",
                "        \"\"\"\n",
                "        state = ThinkingState()\n",
                "        state.add_thought(f\"Executing plan with {len(plan)} steps\")\n",
                "        \n",
                "        for i, step in enumerate(plan):\n",
                "            state.add_thought(f\"Step {i+1}: {step.get('description', 'Executing...')}\")\n",
                "            \n",
                "            if step['action'] == 'tool_call':\n",
                "                tool_name = step['tool']\n",
                "                arguments = step['arguments']\n",
                "                \n",
                "                state.add_tool_call(tool_name, arguments)\n",
                "                result = self.agent.registry.execute(tool_name, arguments)\n",
                "                state.add_tool_result(tool_name, result)\n",
                "                \n",
                "                # Integrate result into reasoning\n",
                "                state.add_thought(\n",
                "                    f\"Step {i+1} complete. Result: {result[:100]}...\"\n",
                "                    if len(result) > 100 else f\"Step {i+1} complete. Result: {result}\"\n",
                "                )\n",
                "            \n",
                "            elif step['action'] == 'reason':\n",
                "                state.add_thought(step['content'])\n",
                "        \n",
                "        state.add_thought(\"Plan execution complete.\")\n",
                "        state.is_complete = True\n",
                "        \n",
                "        return state"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST: Multi-step orchestration\n",
                "\n",
                "orchestrator = MultiStepOrchestrator(agent)\n",
                "\n",
                "plan = [\n",
                "    {\n",
                "        'action': 'tool_call',\n",
                "        'description': 'Search for Python tutorials',\n",
                "        'tool': 'search',\n",
                "        'arguments': {'query': 'Python tutorial 2025'}\n",
                "    },\n",
                "    {\n",
                "        'action': 'reason',\n",
                "        'content': 'Now I need to analyze the search results'\n",
                "    },\n",
                "    {\n",
                "        'action': 'tool_call',\n",
                "        'description': 'Calculate estimated learning time',\n",
                "        'tool': 'calculator',\n",
                "        'arguments': {'expression': '30 * 7'}  # 30 min/day for 7 days\n",
                "    },\n",
                "    {\n",
                "        'action': 'reason',\n",
                "        'content': 'Based on search and calculation, I can provide recommendations'\n",
                "    },\n",
                "]\n",
                "\n",
                "result = orchestrator.execute_plan(plan)\n",
                "\n",
                "print(\"Multi-Step Execution:\")\n",
                "print(\"=\" * 50)\n",
                "print(result.get_context())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. API Integration Pattern\n",
                "\n",
                "How to enable thinking mode with DeepSeek V3.2 API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_deepseek_request(\n",
                "    messages: List[Dict],\n",
                "    tools: List[Dict],\n",
                "    thinking_enabled: bool = True,\n",
                ") -> Dict:\n",
                "    \"\"\"\n",
                "    Create DeepSeek V3.2 API request with thinking mode.\n",
                "    \n",
                "    Args:\n",
                "        messages: Conversation history\n",
                "        tools: Tool definitions\n",
                "        thinking_enabled: Whether to use thinking mode\n",
                "    \n",
                "    Returns:\n",
                "        API request payload\n",
                "    \"\"\"\n",
                "    request = {\n",
                "        \"model\": \"deepseek-v3.2\",\n",
                "        \"messages\": messages,\n",
                "        \"tools\": tools,\n",
                "        \"tool_choice\": \"auto\",\n",
                "    }\n",
                "    \n",
                "    if thinking_enabled:\n",
                "        # DeepSeek API thinking mode\n",
                "        request[\"thinking\"] = {\"type\": \"enabled\"}\n",
                "    \n",
                "    return request\n",
                "\n",
                "\n",
                "def create_vllm_request(\n",
                "    messages: List[Dict],\n",
                "    tools: List[Dict],\n",
                "    thinking_enabled: bool = True,\n",
                ") -> Dict:\n",
                "    \"\"\"\n",
                "    Create vLLM request with thinking mode.\n",
                "    \n",
                "    Args:\n",
                "        messages: Conversation history\n",
                "        tools: Tool definitions\n",
                "        thinking_enabled: Whether to use thinking mode\n",
                "    \n",
                "    Returns:\n",
                "        vLLM request payload\n",
                "    \"\"\"\n",
                "    request = {\n",
                "        \"model\": \"deepseek-ai/DeepSeek-V3-0324\",\n",
                "        \"messages\": messages,\n",
                "        \"tools\": tools,\n",
                "    }\n",
                "    \n",
                "    if thinking_enabled:\n",
                "        # vLLM thinking mode via chat template\n",
                "        request[\"chat_template_kwargs\"] = {\"thinking\": True}\n",
                "    \n",
                "    return request"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example API request\n",
                "\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": \"What's the weather in Tokyo and convert 25°C to °F?\"}\n",
                "]\n",
                "\n",
                "tools = registry.get_schemas()\n",
                "\n",
                "# DeepSeek API request\n",
                "deepseek_request = create_deepseek_request(messages, tools, thinking_enabled=True)\n",
                "\n",
                "print(\"DeepSeek V3.2 API Request:\")\n",
                "print(json.dumps(deepseek_request, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Response Parsing\n",
                "\n",
                "Parse DeepSeek V3.2 responses with separate reasoning and content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ThinkingResponse:\n",
                "    \"\"\"Parsed response with thinking content.\"\"\"\n",
                "    reasoning_content: str  # Internal chain-of-thought\n",
                "    content: str            # Final answer\n",
                "    tool_calls: List[Dict]  # Any tool calls made\n",
                "    \n",
                "\n",
                "def parse_thinking_response(response: Dict) -> ThinkingResponse:\n",
                "    \"\"\"\n",
                "    Parse DeepSeek V3.2 response.\n",
                "    \n",
                "    The model separates:\n",
                "    - reasoning_content: Internal thinking (can be exposed)\n",
                "    - content: Final response to user\n",
                "    \"\"\"\n",
                "    message = response.get(\"choices\", [{}])[0].get(\"message\", {})\n",
                "    \n",
                "    return ThinkingResponse(\n",
                "        reasoning_content=message.get(\"reasoning_content\", \"\"),\n",
                "        content=message.get(\"content\", \"\"),\n",
                "        tool_calls=message.get(\"tool_calls\", []),\n",
                "    )\n",
                "\n",
                "\n",
                "# Simulated response\n",
                "simulated_response = {\n",
                "    \"choices\": [{\n",
                "        \"message\": {\n",
                "            \"reasoning_content\": (\n",
                "                \"I need to get weather for Tokyo. \"\n",
                "                \"Then convert temperature. \"\n",
                "                \"Formula is °F = °C × 9/5 + 32.\"\n",
                "            ),\n",
                "            \"content\": \"The weather in Tokyo is sunny, 25°C (77°F).\",\n",
                "            \"tool_calls\": [\n",
                "                {\"function\": {\"name\": \"get_weather\", \"arguments\": '{\"location\": \"Tokyo\"}'}},\n",
                "                {\"function\": {\"name\": \"calculator\", \"arguments\": '{\"expression\": \"25 * 9/5 + 32\"}'}},\n",
                "            ]\n",
                "        }\n",
                "    }]\n",
                "}\n",
                "\n",
                "parsed = parse_thinking_response(simulated_response)\n",
                "\n",
                "print(\"Parsed Response:\")\n",
                "print(f\"  Reasoning: {parsed.reasoning_content}\")\n",
                "print(f\"  Answer: {parsed.content}\")\n",
                "print(f\"  Tool Calls: {len(parsed.tool_calls)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Summary: Thinking in Tool-Use\n",
                "\n",
                "| Aspect | DeepSeek V3.2 Approach |\n",
                "|--------|------------------------|\n",
                "| **Reasoning Persistence** | Thread maintained across tool calls |\n",
                "| **Mode Toggle** | `thinking: enabled/disabled` |\n",
                "| **Output Structure** | Separate `reasoning_content` + `content` |\n",
                "| **Training Data** | 1,800+ environments, 85K instructions |\n",
                "| **Max Tool Calls** | 100+ sequential without context loss |\n",
                "\n",
                "### Key Implementation Points\n",
                "\n",
                "1. **Preserve state** between tool calls\n",
                "2. **Separate reasoning** from final output\n",
                "3. **Plan before action** using CoT\n",
                "4. **Integrate results** back into reasoning thread\n",
                "\n",
                "---\n",
                "**Tier 3 Section 09 Complete!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}