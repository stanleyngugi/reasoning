# Image-to-Math: A Deterministic Paradigm for Visual Generation

## 1. Executive Summary

This document describes a research direction toward a new paradigm for image generation: **extracting deterministic mathematical formulas from images** rather than generating pixels through probabilistic diffusion.

**The core claim:** Images are mathematical objects. Every photograph, illustration, or graphic can be represented as a function f(x,y) → RGB. The question is not whether such a representation exists — it does, trivially — but whether we can extract compact, efficient, learnable mathematical descriptions from visual data.

**The goal:** Build a pipeline that accepts an image and outputs the mathematical formula, vector primitives, or procedural code required to reproduce it. This is not positioned as "interpretable AI for niche use cases" — it is positioned as a potential **new standard** for image generation that could address fundamental limitations of diffusion models.

**Current status:** Most components exist. The gap is integration and engineering, not fundamental research.

---

## 2. The Core Insight: A Meta-Pattern

This work shares a meta-pattern with Trace-to-Lean (a system for verified mathematical reasoning):

> **Don't make neural networks do the hard part. Find the deterministic/verifiable core. Use LLMs only for what they're good at. Verify by computation.**

| Domain | Neural Net Role | Deterministic Core | Verification |
|--------|-----------------|-------------------|--------------|
| Math Reasoning (Trace-to-Lean) | Code generation | Berlekamp-Massey pattern mining | `native_decide` — binary correct/incorrect |
| Image Generation (Image-to-Math) | Visual perception, code proposal | Formula extraction, rendering | Render → compare to original image |

Both systems avoid asking neural networks to do things they're bad at:
- Trace-to-Lean: Don't ask LLMs to do proof search
- Image-to-Math: Don't ask LLMs to judge aesthetics or verify correctness

Both use computation as the oracle.

---

## 3. The Ground Truth Principle

**The key insight connecting both systems:**

| System | Ground Truth | Verification |
|--------|--------------|--------------|
| Trace-to-Lean | Mathematical correctness (objective) | `native_decide` — the answer is right or wrong |
| Image-to-Math | **The original image itself** | Render the formula → compare to original |

In Image-to-Math, **the image IS the ground truth**. There is no need for:
- External labels
- Human aesthetic judgment
- Separate verification datasets

The verification loop is self-contained:
```
Formula → Render → Compare to Original → Gradient Update → Repeat
```

If the rendered formula matches the original image, the formula is correct. This is a computable, differentiable signal that provides the training objective.

---

## 4. Why This Could Be a New Standard (Not a Niche)

### 4.1 Fundamental Limitations of Diffusion Models

| Limitation | Description |
|------------|-------------|
| Fixed resolution | Outputs are raster grids; upscaling is a hack |
| No semantic structure | Cannot "edit the radius" of a generated circle |
| Stochastic | Different outputs each run; no reproducibility |
| Slow inference | Iterative denoising is inherently expensive |
| No compression | Knowledge is in billions of parameters, not compact representations |
| Black box | Cannot explain why an output looks the way it does |

A "circle" generated by diffusion is not a geometric locus of points equidistant from a center — it is a statistical cluster of pixels that phenomenologically resembles a circle.

### 4.2 Advantages of Mathematical Representation

| Advantage | Description |
|-----------|-------------|
| Infinite resolution | Formulas scale; pixels don't |
| Native editability | Change parameters → predictable changes |
| Deterministic | Same formula → same image, always |
| Fast inference | Once you have the formula, rendering is trivial |
| Compact | The image IS the formula |
| Interpretable | Byproduct, not selling point — you get source code for free |

### 4.3 The Photorealism Question

This is not limited to geometric/stylized art. The theoretical foundation supports photorealism:

1. **SIREN** (Sinusoidal Representation Networks) already represents photorealistic images as nested trigonometric functions
2. **Procedural textures** (Perlin noise, SDFs) power photorealistic games and films today
3. **Differentiable rendering** can optimize formulas to match photographs

The challenge is making extraction fast, compact, and learnable — not proving it's possible.

---

## 5. Technical Landscape: What Exists

### 5.1 Image → Mathematical Representation (Extraction)

| Tool/Method | What It Does | Maturity |
|-------------|--------------|----------|
| **DiffVG** | Differentiable SVG rasterizer — optimize Bezier curves to match any image | Production-ready |
| **Bezier Splatting** (NeurIPS 2025) | 30-150x faster than DiffVG; samples curves into 2D Gaussians | Research, breakthrough |
| **Im2Vec** | Neural network outputs SVG from raster (no vector supervision needed) | Research, code available |
| **LIVE** | Layer-wise image vectorization with topology preservation | Research |
| **SIREN** | Represents image as nested sine compositions — weights ARE the formula | Proven |
| **Potrace / Vectorizer.AI** | Image → Bezier curves, detects circles/ellipses | Production |
| **CSGNet** | Image → CSG program (boolean ops on primitives) | Research |
| **L-system inference** | Neural network infers fractal grammars from images | Research |

**Key finding:** DiffVG + Bezier Splatting + Im2Vec can already convert arbitrary images to mathematical representations.

### 5.2 Formula → Image (Rendering)

| Tool | Formula Format | Status |
|------|----------------|--------|
| Shadertoy | GLSL code with SDFs, noise, trig functions | 50K+ public shaders |
| Desmos API | LaTeX-like parametric equations | Full API, programmatic |
| Processing / p5.js | Code = formula | Mature ecosystem |
| Matplotlib | Python math expressions | Production |
| Blender Geometry Nodes | Node graphs = formulas | Production |

**Key finding:** Rendering is solved. Any mathematical formula can be rendered programmatically.

### 5.3 Datasets

| Dataset | Size | Content |
|---------|------|---------|
| **StarVector text2svg-stack** | 2.1M SVGs | Image-to-SVG pairs with text descriptions |
| **Shaders21k** | 21,000+ | GLSL code ↔ rendered frames |
| **VLMaterial** | 550,000 | Blender node graphs ↔ renders |
| **LayerSVG** | 20,000 | Layered SVGs with topology |
| **UniSVG** | 525K items | SVG generation + understanding |
| **Google Fonts (VecFusion)** | 324K glyphs | Bezier curve representations |
| **Yeganeh's formulas** | Documented | Full parametric equations (AMS) |

**Key finding:** Large-scale (image, formula) datasets already exist. The 2.1M SVG pairs alone are sufficient to begin training.

### 5.4 The "Desmos Gap"

No public ML-ready dataset of Desmos graphs exists. However, construction is straightforward:
- `Calc.getState()` → JSON representation of all expressions
- `Calc.screenshot()` → rendered image
- Sources: Reddit r/desmos, Discord communities, Desmos Art Contest

Building "Desmos1M" would be novel contribution + moat.

### 5.5 Neurosymbolic Architecture Components

| Component | Description | Status |
|-----------|-------------|--------|
| **DreamCoder** | Wake-sleep learning that evolves a DSL; library learning for abstraction | Proven architecture |
| **NESYDM** (Neurosymbolic Diffusion) | Diffusion in program latent space; generates discrete code via continuous process | Emerging 2025 |
| **VLMaterial** | VLM fine-tuned to output Blender Python code from texture images | Production-grade results |
| **DeTikZify** | Multimodal LLM → TikZ code with MCTS refinement | NeurIPS 2024 Spotlight |

---

## 6. Proposed Architecture

### Stage 1: Semantic Perceiver (VLM-Encoder)
- **Input:** Raster image
- **Model:** Fine-tuned VLM (LLaVA, GPT-4V, etc.)
- **Output:** Semantic embedding + structural description
- **Function:** Extract mathematical intent, not just visual features

### Stage 2: Neurosymbolic Decoder (NESYDM)
- **Input:** Semantic embedding
- **Model:** Neurosymbolic Diffusion Model trained on domain DSL
- **Output:** Candidate program P
  - Branch A: Bezier control points (SVG)
  - Branch B: LaTeX formulas
  - Branch C: Python/GLSL code

### Stage 3: Differentiable Validation Loop
- **Renderer:** Bezier Splatting (vectors) or differentiable shader interpreter
- **Loss Functions:**
  - Pixel loss (MSE) — exact alignment
  - Perceptual loss (LPIPS) — structural similarity
  - Code efficiency loss (MDL) — penalize program length
- **Mechanism:** Gradients flow from rendered image back to program parameters

### Stage 4: Abstraction & Canonicalization
- **Mechanism:** DreamCoder-style refactoring — identify patterns, replace magic numbers with parameters
- **Output:** Clean, human-readable, editable file (SVG, Desmos link, .blend, etc.)

```
                    ┌─────────────────┐
                    │  INPUT IMAGE    │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │  VLM-Encoder    │
                    │  (Perception)   │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │  NESYDM         │
                    │  (Code Proposal)│
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              ▼              ▼              ▼
           [SVG]         [LaTeX]        [GLSL]
              │              │              │
              └──────────────┼──────────────┘
                             │
                    ┌────────▼────────┐
                    │  Differentiable │
                    │  Renderer       │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │  Compare to     │
                    │  Original Image │◄──── Ground Truth
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │  Gradient       │
                    │  Update         │
                    └────────┬────────┘
                             │
                             ▼
                         [ITERATE]
                             │
                    ┌────────▼────────┐
                    │  Abstraction &  │
                    │  Canonicalization│
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │  FINAL OUTPUT   │
                    │  (Editable Math)│
                    └─────────────────┘
```

---

## 7. The Bezier Splatting Breakthrough

The historical bottleneck was speed. DiffVG was too slow for real-time use (hours per high-res image).

**Bezier Splatting (NeurIPS 2025)** changes this:

| Metric | Improvement |
|--------|-------------|
| Forward pass | 30x faster |
| Backward pass | 150x faster |

**Mechanism:**
1. Sample Bezier curve into 2D anisotropic Gaussians
2. Splat Gaussians onto canvas (fast, differentiable)
3. Gradients flow: pixels → Gaussians → curve parameters

This transforms differentiable vector graphics from offline optimization into a viable real-time decoder.

---

## 8. Comparison: Determinism vs. Diffusion

| Feature | Diffusion Models | Image-to-Math |
|---------|------------------|---------------|
| Fundamental unit | Pixel (stochastic) | Primitive (deterministic) |
| Resolution | Fixed (raster) | Infinite (vector/formula) |
| Editability | Low (inpainting/prompting) | High (variable manipulation) |
| Interpretability | None (black box) | High (source code) |
| Texture fidelity | Extremely high | Variable (procedural) |
| Training data | Billions of images | Millions of code/render pairs |
| Inference speed | Slow (iterative denoising) | Fast (single-pass or optimization) |

**This is not positioned as "better for engineering."** It is positioned as a potential new standard that addresses fundamental limitations of the diffusion paradigm.

The interpretability is a byproduct, not the selling point. People don't switch architectures for interpretability — they switch because the new thing is better.

---

## 9. Progressive Development Philosophy

This document describes a direction, not a final architecture.

**Key principles:**
1. Start with what exists
2. Build the first version
3. Hit unexpected walls
4. Discover unexpected shortcuts
5. Iterate

Building will reveal insights that cannot be predicted in advance. The architecture will evolve.

**What we know:**
- The theoretical foundation is sound (images ARE math)
- Most components exist
- The gap is integration, not invention

**What we don't know:**
- Which image types need hybrid approaches
- Where the practical limits of compactness lie
- What shortcuts will emerge during implementation
- How the decoder architecture will need to adapt

---

## 10. Concrete Next Steps

### Phase 1: Data Infrastructure
- [ ] Acquire StarVector dataset (2.1M SVGs)
- [ ] Set up Shaders21k pipeline
- [ ] Build Desmos scraper (getState + screenshot)
- [ ] Create unified (image, formula) training format

### Phase 2: Extraction Pipeline
- [ ] Implement Bezier Splatting renderer
- [ ] Test DiffVG/Im2Vec on diverse image types
- [ ] Benchmark SIREN fitting for photographic images
- [ ] Identify failure modes and image type boundaries

### Phase 3: Neural Decoder
- [ ] Fine-tune VLM on (image, code) pairs
- [ ] Experiment with NESYDM architecture
- [ ] Implement analysis-by-synthesis training loop
- [ ] Add abstraction/refactoring stage

### Phase 4: Iteration
- [ ] Measure reconstruction quality vs. formula compactness
- [ ] Identify domains where hybrid (formula + diffusion) is needed
- [ ] Scale to higher resolution and more complex images
- [ ] Refine architecture based on empirical findings

---

## 11. Summary

**Vision:** A new paradigm for image generation where images are represented as mathematical objects, verified by reconstruction against the original, and the current diffusion approach becomes legacy.

**Core insight:** The original image is the ground truth. Verification is computational: render the formula, compare to the image. No external oracle needed.

**Status:** Components exist. The work is integration and engineering.

**Positioning:** This is not "interpretable AI for niche use cases." This is a potential new standard. Interpretability is a byproduct.

**Philosophy:** Progressive development. Build → discover → iterate. The final architecture will emerge from the work itself.

---

## Appendix: Key References

### Differentiable Rendering
- DiffVG (SIGGRAPH Asia 2020)
- LIVE (CVPR 2022)
- Bezier Splatting (NeurIPS 2025)
- LayerTracer (2025)

### Neural Implicit Representations
- SIREN — Sinusoidal Representation Networks
- Fourier Feature Networks (NeurIPS 2020)
- WIRE, FINER, DINER (2024)

### Image-to-Vector Methods
- Im2Vec (CVPR 2021)
- VectorFusion (2023)
- StarVector (2024)
- SVGDreamer (2024)

### Neurosymbolic Systems
- DreamCoder (MIT/Cornell)
- DeTikZify (NeurIPS 2024)
- NESYDM — Neurosymbolic Diffusion Models (2025)

### Inverse Procedural Modeling
- VLMaterial (ICLR 2025)
- MultiMat (2025)
- CSGNet (CVPR 2018)
- Shaders21k dataset (NeurIPS 2022)

### Datasets
- StarVector text2svg-stack: 2.1M SVGs
- Shaders21k: 21K GLSL programs
- VLMaterial: 550K material graphs
- LayerSVG: 20K layered vectors
- Yeganeh formulas: AMS Mathematical Imagery
