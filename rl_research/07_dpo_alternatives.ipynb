{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 07 — DPO & RL-Free Alternatives\n",
                "\n",
                "> **Purpose:** Implement preference optimization without complex RL loops. DPO and its variants directly optimize policies from preference data.\n",
                "\n",
                "**Key insight:** DPO reparameterizes the RLHF objective to eliminate the reward model and RL training.\n",
                "\n",
                "| Algorithm | Reference Model | Data Format | Key Feature |\n",
                "|-----------|-----------------|-------------|-------------|\n",
                "| DPO | Required | Paired | Bradley-Terry loss |\n",
                "| ORPO | None | Paired | Odds ratio + SFT |\n",
                "| SimPO | None | Paired | Length normalization |\n",
                "| KTO | Required | Unpaired | Prospect theory |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "import copy\n",
                "\n",
                "torch.manual_seed(42)\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. TinyLM + Reference Model\n",
                "\n",
                "DPO requires computing log probabilities under both current and reference policies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TinyLM(nn.Module):\n",
                "    \"\"\"Minimal LM for preference optimization experiments.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size=100, hidden_size=64):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
                "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
                "        self.head = nn.Linear(hidden_size, vocab_size)\n",
                "        self.vocab_size = vocab_size\n",
                "    \n",
                "    def forward(self, x):\n",
                "        h = self.embed(x)\n",
                "        h, _ = self.rnn(h)\n",
                "        return self.head(h)\n",
                "    \n",
                "    def get_sequence_log_prob(self, input_ids: torch.Tensor, \n",
                "                               labels: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Compute sum of log probabilities for a sequence.\n",
                "        \n",
                "        Args:\n",
                "            input_ids: (batch, seq_len) prompt + response tokens\n",
                "            labels: (batch, seq_len) target tokens\n",
                "        \n",
                "        Returns:\n",
                "            log_probs: (batch,) sum of log probs per sequence\n",
                "        \"\"\"\n",
                "        logits = self(input_ids)  # (batch, seq, vocab)\n",
                "        log_probs = F.log_softmax(logits, dim=-1)\n",
                "        \n",
                "        # Gather log probs for target tokens\n",
                "        token_log_probs = torch.gather(\n",
                "            log_probs, dim=-1, index=labels.unsqueeze(-1)\n",
                "        ).squeeze(-1)  # (batch, seq)\n",
                "        \n",
                "        # Sum over sequence\n",
                "        return token_log_probs.sum(dim=-1)  # (batch,)\n",
                "\n",
                "\n",
                "def create_reference_model(policy: nn.Module) -> nn.Module:\n",
                "    \"\"\"Create frozen copy for DPO reference.\"\"\"\n",
                "    ref = copy.deepcopy(policy)\n",
                "    ref.eval()\n",
                "    for p in ref.parameters():\n",
                "        p.requires_grad = False\n",
                "    return ref"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. DPO Loss Implementation\n",
                "\n",
                "$$\\mathcal{L}_{DPO} = -\\mathbb{E}\\left[\\log \\sigma\\left(\\beta \\left(\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right)\\right]$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_dpo_loss(\n",
                "    policy_chosen_logps: torch.Tensor,\n",
                "    policy_rejected_logps: torch.Tensor,\n",
                "    ref_chosen_logps: torch.Tensor,\n",
                "    ref_rejected_logps: torch.Tensor,\n",
                "    beta: float = 0.1,\n",
                ") -> Dict[str, torch.Tensor]:\n",
                "    \"\"\"\n",
                "    Compute DPO loss from log probabilities.\n",
                "    \n",
                "    Args:\n",
                "        policy_chosen_logps: (batch,) log P(y_w|x) under policy\n",
                "        policy_rejected_logps: (batch,) log P(y_l|x) under policy\n",
                "        ref_chosen_logps: (batch,) log P(y_w|x) under reference\n",
                "        ref_rejected_logps: (batch,) log P(y_l|x) under reference\n",
                "        beta: Temperature parameter controlling divergence strength\n",
                "    \n",
                "    Returns:\n",
                "        Dict with 'loss', 'chosen_reward', 'rejected_reward', 'accuracy'\n",
                "    \"\"\"\n",
                "    # Compute log ratios (implicit rewards)\n",
                "    chosen_log_ratio = policy_chosen_logps - ref_chosen_logps\n",
                "    rejected_log_ratio = policy_rejected_logps - ref_rejected_logps\n",
                "    \n",
                "    # DPO loss: -log sigmoid(beta * (chosen_reward - rejected_reward))\n",
                "    logits = beta * (chosen_log_ratio - rejected_log_ratio)\n",
                "    loss = -F.logsigmoid(logits).mean()\n",
                "    \n",
                "    # Metrics\n",
                "    chosen_reward = beta * chosen_log_ratio\n",
                "    rejected_reward = beta * rejected_log_ratio\n",
                "    accuracy = (chosen_reward > rejected_reward).float().mean()\n",
                "    \n",
                "    return {\n",
                "        'loss': loss,\n",
                "        'chosen_reward': chosen_reward.mean(),\n",
                "        'rejected_reward': rejected_reward.mean(),\n",
                "        'accuracy': accuracy,\n",
                "        'margin': (chosen_reward - rejected_reward).mean(),\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "DPO Loss Test:\n",
                        "  loss: 0.4864\n",
                        "  chosen_reward: 0.1758\n",
                        "  rejected_reward: -0.3045\n",
                        "  accuracy: 1.0000\n",
                        "  margin: 0.4802\n"
                    ]
                }
            ],
            "source": [
                "# TEST: DPO loss\n",
                "\n",
                "batch_size = 8\n",
                "\n",
                "# Simulate log probabilities\n",
                "policy_chosen = torch.randn(batch_size) - 50  # Log probs are negative\n",
                "policy_rejected = torch.randn(batch_size) - 55  # Slightly worse\n",
                "ref_chosen = torch.randn(batch_size) - 52\n",
                "ref_rejected = torch.randn(batch_size) - 52\n",
                "\n",
                "result = compute_dpo_loss(\n",
                "    policy_chosen, policy_rejected, \n",
                "    ref_chosen, ref_rejected,\n",
                "    beta=0.1\n",
                ")\n",
                "\n",
                "print(\"DPO Loss Test:\")\n",
                "for k, v in result.items():\n",
                "    print(f\"  {k}: {v.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ORPO: Reference-Free with Odds Ratio\n",
                "\n",
                "ORPO combines SFT and alignment in one step using odds ratio:\n",
                "\n",
                "$$\\mathcal{L}_{ORPO} = \\mathcal{L}_{SFT}(y_w) - \\lambda \\log\\sigma\\left(\\log\\frac{\\text{odds}(y_w)}{\\text{odds}(y_l)}\\right)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_orpo_loss(\n",
                "    policy_chosen_logps: torch.Tensor,\n",
                "    policy_rejected_logps: torch.Tensor,\n",
                "    chosen_lengths: torch.Tensor,\n",
                "    rejected_lengths: torch.Tensor,\n",
                "    lambda_weight: float = 1.0,\n",
                ") -> Dict[str, torch.Tensor]:\n",
                "    \"\"\"\n",
                "    Compute ORPO loss (reference-free).\n",
                "    \n",
                "    Key: Uses odds ratio instead of log probability ratio.\n",
                "    odds(y) = P(y) / (1 - P(y)) ≈ exp(log P(y)) for small P\n",
                "    \n",
                "    Args:\n",
                "        policy_chosen_logps: (batch,) sum of log probs for chosen\n",
                "        policy_rejected_logps: (batch,) sum of log probs for rejected\n",
                "        chosen_lengths: (batch,) length of chosen responses\n",
                "        rejected_lengths: (batch,) length of rejected responses\n",
                "        lambda_weight: Weight for preference term\n",
                "    \n",
                "    Returns:\n",
                "        Dict with 'loss', 'sft_loss', 'preference_loss'\n",
                "    \"\"\"\n",
                "    # Average log prob per token\n",
                "    chosen_avg_logp = policy_chosen_logps / chosen_lengths\n",
                "    rejected_avg_logp = policy_rejected_logps / rejected_lengths\n",
                "    \n",
                "    # SFT loss: maximize log prob of chosen\n",
                "    sft_loss = -policy_chosen_logps.mean()\n",
                "    \n",
                "    # Odds ratio (in log space)\n",
                "    # log(odds) = log(P / (1-P)) ≈ log P for small P\n",
                "    # We use average log prob to be length-normalized\n",
                "    log_odds_chosen = chosen_avg_logp\n",
                "    log_odds_rejected = rejected_avg_logp\n",
                "    \n",
                "    # Preference loss: prefer chosen over rejected\n",
                "    preference_logits = log_odds_chosen - log_odds_rejected\n",
                "    preference_loss = -F.logsigmoid(preference_logits).mean()\n",
                "    \n",
                "    # Combined loss\n",
                "    total_loss = sft_loss + lambda_weight * preference_loss\n",
                "    \n",
                "    return {\n",
                "        'loss': total_loss,\n",
                "        'sft_loss': sft_loss,\n",
                "        'preference_loss': preference_loss,\n",
                "        'accuracy': (chosen_avg_logp > rejected_avg_logp).float().mean(),\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ORPO Loss Test:\n",
                        "  loss: 100.6832\n",
                        "  sft_loss: 100.0453\n",
                        "  preference_loss: 0.6379\n",
                        "  accuracy: 0.3750\n"
                    ]
                }
            ],
            "source": [
                "# TEST: ORPO loss\n",
                "\n",
                "chosen_logps = torch.randn(8) - 100\n",
                "rejected_logps = torch.randn(8) - 120  # Worse (more negative)\n",
                "chosen_lens = torch.randint(20, 100, (8,)).float()\n",
                "rejected_lens = torch.randint(20, 100, (8,)).float()\n",
                "\n",
                "orpo_result = compute_orpo_loss(\n",
                "    chosen_logps, rejected_logps,\n",
                "    chosen_lens, rejected_lens,\n",
                "    lambda_weight=1.0\n",
                ")\n",
                "\n",
                "print(\"ORPO Loss Test:\")\n",
                "for k, v in orpo_result.items():\n",
                "    print(f\"  {k}: {v.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. SimPO: Length-Normalized Rewards\n",
                "\n",
                "SimPO prevents length hacking by normalizing rewards:\n",
                "\n",
                "$$r_{SimPO}(x, y) = \\frac{\\beta}{|y|} \\log \\pi_\\theta(y|x)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_simpo_loss(\n",
                "    policy_chosen_logps: torch.Tensor,\n",
                "    policy_rejected_logps: torch.Tensor,\n",
                "    chosen_lengths: torch.Tensor,\n",
                "    rejected_lengths: torch.Tensor,\n",
                "    beta: float = 2.0,\n",
                "    gamma: float = 0.5,  # Target margin\n",
                ") -> Dict[str, torch.Tensor]:\n",
                "    \"\"\"\n",
                "    Compute SimPO loss (reference-free, length-normalized).\n",
                "    \n",
                "    Key: Reward is (beta/length) * log_prob, preventing length bias.\n",
                "    \n",
                "    Args:\n",
                "        policy_chosen_logps: (batch,) sum of log probs for chosen\n",
                "        policy_rejected_logps: (batch,) sum of log probs for rejected\n",
                "        chosen_lengths: (batch,) length of chosen responses\n",
                "        rejected_lengths: (batch,) length of rejected responses\n",
                "        beta: Reward scaling\n",
                "        gamma: Target margin for preference\n",
                "    \n",
                "    Returns:\n",
                "        Dict with 'loss', 'chosen_reward', 'rejected_reward'\n",
                "    \"\"\"\n",
                "    # Length-normalized rewards\n",
                "    chosen_reward = (beta / chosen_lengths) * policy_chosen_logps\n",
                "    rejected_reward = (beta / rejected_lengths) * policy_rejected_logps\n",
                "    \n",
                "    # SimPO loss with margin\n",
                "    logits = chosen_reward - rejected_reward - gamma\n",
                "    loss = -F.logsigmoid(logits).mean()\n",
                "    \n",
                "    return {\n",
                "        'loss': loss,\n",
                "        'chosen_reward': chosen_reward.mean(),\n",
                "        'rejected_reward': rejected_reward.mean(),\n",
                "        'margin': (chosen_reward - rejected_reward).mean(),\n",
                "        'accuracy': (chosen_reward > rejected_reward).float().mean(),\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Length Bias Test:\n",
                        "  Chosen: 25 tokens, logP=-50\n",
                        "  Rejected: 100 tokens, logP=-150\n",
                        "\n",
                        "SimPO (length-normalized):\n",
                        "  Chosen reward: -4.00\n",
                        "  Rejected reward: -3.00\n",
                        "  Prefers: Rejected ✗\n"
                    ]
                }
            ],
            "source": [
                "# TEST: SimPO vs DPO on length bias\n",
                "\n",
                "# Scenario: chosen is short but good, rejected is long but worse\n",
                "chosen_logps = torch.tensor([-50.0])  # Short, high per-token prob\n",
                "rejected_logps = torch.tensor([-150.0])  # Long, but sum is much lower\n",
                "chosen_lens = torch.tensor([25.0])\n",
                "rejected_lens = torch.tensor([100.0])\n",
                "\n",
                "# SimPO rewards (length-normalized)\n",
                "simpo_chosen_r = (2.0 / chosen_lens) * chosen_logps\n",
                "simpo_rejected_r = (2.0 / rejected_lens) * rejected_logps\n",
                "\n",
                "print(\"Length Bias Test:\")\n",
                "print(f\"  Chosen: {chosen_lens.item():.0f} tokens, logP={chosen_logps.item():.0f}\")\n",
                "print(f\"  Rejected: {rejected_lens.item():.0f} tokens, logP={rejected_logps.item():.0f}\")\n",
                "print(f\"\\nSimPO (length-normalized):\")\n",
                "print(f\"  Chosen reward: {simpo_chosen_r.item():.2f}\")\n",
                "print(f\"  Rejected reward: {simpo_rejected_r.item():.2f}\")\n",
                "print(f\"  Prefers: {'Chosen ✓' if simpo_chosen_r > simpo_rejected_r else 'Rejected ✗'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. KTO: Unpaired Preference Data\n",
                "\n",
                "KTO uses prospect theory to learn from unpaired \"desirable\" vs \"undesirable\" labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_kto_loss(\n",
                "    policy_desirable_logps: torch.Tensor,\n",
                "    policy_undesirable_logps: torch.Tensor,\n",
                "    ref_desirable_logps: torch.Tensor,\n",
                "    ref_undesirable_logps: torch.Tensor,\n",
                "    kl_desirable: torch.Tensor,\n",
                "    kl_undesirable: torch.Tensor,\n",
                "    beta: float = 0.1,\n",
                ") -> Dict[str, torch.Tensor]:\n",
                "    \"\"\"\n",
                "    Compute KTO loss for unpaired preference data.\n",
                "    \n",
                "    Based on prospect theory: loss aversion means\n",
                "    undesirable samples are weighted more heavily.\n",
                "    \n",
                "    Args:\n",
                "        policy_*_logps: Log probs under current policy\n",
                "        ref_*_logps: Log probs under reference policy\n",
                "        kl_*: KL divergence estimates (for regularization)\n",
                "        beta: Temperature parameter\n",
                "    \n",
                "    Returns:\n",
                "        Dict with 'loss', 'desirable_loss', 'undesirable_loss'\n",
                "    \"\"\"\n",
                "    # Compute log ratios\n",
                "    desirable_ratio = policy_desirable_logps - ref_desirable_logps\n",
                "    undesirable_ratio = policy_undesirable_logps - ref_undesirable_logps\n",
                "    \n",
                "    # KL regularization (expected KL from desirable distribution)\n",
                "    kl_ref = kl_desirable.mean().detach()\n",
                "    \n",
                "    # Desirable: maximize (want high ratio)\n",
                "    # Uses sigmoid to bound the utility\n",
                "    desirable_utility = beta * desirable_ratio - beta * kl_ref\n",
                "    desirable_loss = -F.logsigmoid(desirable_utility).mean()\n",
                "    \n",
                "    # Undesirable: minimize (want low ratio)\n",
                "    # Loss aversion: undesirable weighted ~2x (from prospect theory)\n",
                "    undesirable_utility = beta * kl_ref - beta * undesirable_ratio\n",
                "    undesirable_loss = -F.logsigmoid(undesirable_utility).mean()\n",
                "    \n",
                "    # Combined loss\n",
                "    total_loss = desirable_loss + undesirable_loss\n",
                "    \n",
                "    return {\n",
                "        'loss': total_loss,\n",
                "        'desirable_loss': desirable_loss,\n",
                "        'undesirable_loss': undesirable_loss,\n",
                "        'desirable_ratio': desirable_ratio.mean(),\n",
                "        'undesirable_ratio': undesirable_ratio.mean(),\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Complete DPO Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DPOTrainer:\n",
                "    \"\"\"\n",
                "    Complete DPO trainer with configurable loss variants.\n",
                "    \n",
                "    Supports:\n",
                "    - DPO (default): Requires reference model\n",
                "    - ORPO: No reference, combines SFT + preference\n",
                "    - SimPO: No reference, length-normalized\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 policy: nn.Module,\n",
                "                 ref_model: Optional[nn.Module] = None,\n",
                "                 loss_type: str = 'dpo',\n",
                "                 beta: float = 0.1,\n",
                "                 lr: float = 1e-5):\n",
                "        \n",
                "        self.policy = policy\n",
                "        self.ref_model = ref_model\n",
                "        self.loss_type = loss_type\n",
                "        self.beta = beta\n",
                "        \n",
                "        self.optimizer = torch.optim.AdamW(policy.parameters(), lr=lr)\n",
                "    \n",
                "    def compute_logps(self, model: nn.Module, \n",
                "                      input_ids: torch.Tensor,\n",
                "                      labels: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Compute sequence log probabilities.\"\"\"\n",
                "        return model.get_sequence_log_prob(input_ids, labels)\n",
                "    \n",
                "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
                "        \"\"\"\n",
                "        Single training step.\n",
                "        \n",
                "        Args:\n",
                "            batch: Dict with 'chosen_ids', 'rejected_ids', 'chosen_labels',\n",
                "                   'rejected_labels', 'chosen_lengths', 'rejected_lengths'\n",
                "        \n",
                "        Returns:\n",
                "            Training metrics\n",
                "        \"\"\"\n",
                "        self.policy.train()\n",
                "        \n",
                "        # Get policy log probs\n",
                "        policy_chosen_logps = self.compute_logps(\n",
                "            self.policy, batch['chosen_ids'], batch['chosen_labels']\n",
                "        )\n",
                "        policy_rejected_logps = self.compute_logps(\n",
                "            self.policy, batch['rejected_ids'], batch['rejected_labels']\n",
                "        )\n",
                "        \n",
                "        # Compute loss based on type\n",
                "        if self.loss_type == 'dpo':\n",
                "            # Need reference log probs\n",
                "            with torch.no_grad():\n",
                "                ref_chosen_logps = self.compute_logps(\n",
                "                    self.ref_model, batch['chosen_ids'], batch['chosen_labels']\n",
                "                )\n",
                "                ref_rejected_logps = self.compute_logps(\n",
                "                    self.ref_model, batch['rejected_ids'], batch['rejected_labels']\n",
                "                )\n",
                "            \n",
                "            loss_dict = compute_dpo_loss(\n",
                "                policy_chosen_logps, policy_rejected_logps,\n",
                "                ref_chosen_logps, ref_rejected_logps,\n",
                "                beta=self.beta\n",
                "            )\n",
                "        \n",
                "        elif self.loss_type == 'orpo':\n",
                "            loss_dict = compute_orpo_loss(\n",
                "                policy_chosen_logps, policy_rejected_logps,\n",
                "                batch['chosen_lengths'], batch['rejected_lengths']\n",
                "            )\n",
                "        \n",
                "        elif self.loss_type == 'simpo':\n",
                "            loss_dict = compute_simpo_loss(\n",
                "                policy_chosen_logps, policy_rejected_logps,\n",
                "                batch['chosen_lengths'], batch['rejected_lengths'],\n",
                "                beta=self.beta\n",
                "            )\n",
                "        \n",
                "        else:\n",
                "            raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n",
                "        \n",
                "        # Backward pass\n",
                "        self.optimizer.zero_grad()\n",
                "        loss_dict['loss'].backward()\n",
                "        self.optimizer.step()\n",
                "        \n",
                "        return {k: v.item() for k, v in loss_dict.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "DPO Training:\n",
                        " Step       Loss   Accuracy     Margin\n",
                        "----------------------------------------\n",
                        "    0     0.6931       0.00    -0.0000\n",
                        "    1     0.6843       1.00     0.0178\n",
                        "    2     0.6755       1.00     0.0357\n",
                        "    3     0.6668       1.00     0.0535\n",
                        "    4     0.6581       1.00     0.0713\n"
                    ]
                }
            ],
            "source": [
                "# TEST: DPO Trainer\n",
                "\n",
                "policy = TinyLM().to(device)\n",
                "ref_model = create_reference_model(policy).to(device)\n",
                "\n",
                "trainer = DPOTrainer(\n",
                "    policy=policy,\n",
                "    ref_model=ref_model,\n",
                "    loss_type='dpo',\n",
                "    beta=0.1,\n",
                "    lr=1e-4\n",
                ")\n",
                "\n",
                "# Create synthetic batch\n",
                "batch_size, seq_len = 4, 50\n",
                "batch = {\n",
                "    'chosen_ids': torch.randint(0, 100, (batch_size, seq_len), device=device),\n",
                "    'rejected_ids': torch.randint(0, 100, (batch_size, seq_len), device=device),\n",
                "    'chosen_labels': torch.randint(0, 100, (batch_size, seq_len), device=device),\n",
                "    'rejected_labels': torch.randint(0, 100, (batch_size, seq_len), device=device),\n",
                "    'chosen_lengths': torch.full((batch_size,), seq_len, dtype=torch.float, device=device),\n",
                "    'rejected_lengths': torch.full((batch_size,), seq_len, dtype=torch.float, device=device),\n",
                "}\n",
                "\n",
                "# Training loop\n",
                "print(\"DPO Training:\")\n",
                "print(f\"{'Step':>5} {'Loss':>10} {'Accuracy':>10} {'Margin':>10}\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for step in range(5):\n",
                "    metrics = trainer.train_step(batch)\n",
                "    print(f\"{step:>5} {metrics['loss']:>10.4f} {metrics['accuracy']:>10.2f} {metrics['margin']:>10.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Algorithm Selection Guide\n",
                "\n",
                "| Scenario | Recommended | Why |\n",
                "|----------|-------------|-----|\n",
                "| Standard alignment, paired data | **DPO** | Simple, proven |\n",
                "| Memory constrained | **ORPO** or **SimPO** | No reference model |\n",
                "| Length hacking issues | **SimPO** | Built-in normalization |\n",
                "| Only binary labels | **KTO** | Works with unpaired |\n",
                "| Iterative improvement | **Online DPO** | Fresh on-policy data |\n",
                "\n",
                "### Production β Values\n",
                "\n",
                "```python\n",
                "beta_recommendations = {\n",
                "    'dpo': 0.1,      # Standard\n",
                "    'dpo_strong': 0.5,  # More aggressive\n",
                "    'simpo': 2.0,    # Higher for length norm\n",
                "    'kto': 0.1,      # Similar to DPO\n",
                "}\n",
                "```\n",
                "\n",
                "---\n",
                "**Tier 2 Complete!** You now have implementations for all major preference optimization algorithms."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
