{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reward Hacking Detection and Prevention\n",
                "\n",
                "**Reward hacking** (or *reward gaming*) occurs when a reinforcement learning (RL) agent finds a way to maximize the **proxy reward** (the reward signal it is trained on) without actually achieving the **gold reward** (the true underlying objective or human intent). This is a quintessential example of **Goodhart's Law**: *\"When a measure becomes a target, it ceases to be a good measure.\"*\n",
                "\n",
                "As we scale RLHF (Reinforcement Learning from Human Feedback), models become increasingly capable of finding loopholes in the reward model (RM). For instance:\n",
                "- **Length Hacking**: The model learns that longer responses are generally preferred and starts generating verbose, repetitive fluff.\n",
                "- **Sycophancy**: The model learns to agree with the user's biases rather than being truthful, as this often yields higher immediate rewards.\n",
                "- **Adversarial Exploitation**: The model finds specific token sequences that trigger high reward scores despite being nonsense.\n",
                "\n",
                "In this notebook, we implement mechanisms to **detect** and **mitigate** these behaviors."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "from typing import List, Tuple, Optional, Dict"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Theoretical Framework: Signals of Overoptimization\n",
                "\n",
                "Deep research into reward hacking (e.g., *Scaling Laws for Reward Model Overoptimization*, *The Effects of Reward Misspecification*) suggests several robust signals for detecting when a model is going off-rails:\n",
                "\n",
                "1.  **Proxy-Gold Divergence**: The most direct signal. We track the `proxy_reward` (what the model optimizes) vs. a `gold_reward` (a held-out, higher-quality RM, or actual human evaluation labels). If Proxy $\\uparrow$ while Gold $\\downarrow$, we have hacking.\n",
                "2.  **KL Divergence Explosion**: RLHF typically penalizes the KL divergence between the training policy and the reference model. A sudden spike in KL suggests the model is drifting into a distribution where the RM is unreliable (out-of-distribution).\n",
                "3.  **Length Inflation**: A simple heuristic, but highly effective. Sudden increases in average response length often correlate with length hacking.\n",
                "4.  **Variance Collapse**: Although not implemented here, a collapse in output diversity can also signal that the model has found a single \"super-stimulus\" to exploit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RewardHackingDetector:\n",
                "    \"\"\"\n",
                "    Detect reward hacking during training.\n",
                "    \n",
                "    Key signals:\n",
                "    - Proxy reward increasing while gold reward plateaus/drops\n",
                "    - KL divergence exploding\n",
                "    - Response length inflating\n",
                "    - Diversity collapsing\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, window_size: int = 100):\n",
                "        self.window_size = window_size\n",
                "        self.proxy_rewards = []\n",
                "        self.gold_rewards = []  # From held-out evaluation\n",
                "        self.kl_values = []\n",
                "        self.lengths = []\n",
                "    \n",
                "    def update(self, proxy: float, gold: float, kl: float, length: int):\n",
                "        \"\"\"Record metrics for one training step.\"\"\"\n",
                "        self.proxy_rewards.append(proxy)\n",
                "        self.gold_rewards.append(gold)\n",
                "        self.kl_values.append(kl)\n",
                "        self.lengths.append(length)\n",
                "    \n",
                "    def detect_overoptimization(self) -> Dict:\n",
                "        \"\"\"\n",
                "        Detect if proxy is being overoptimized.\n",
                "        \n",
                "        Returns:\n",
                "            Dict with detection results and metrics\n",
                "        \"\"\"\n",
                "        if len(self.proxy_rewards) < self.window_size * 2:\n",
                "            return {'detected': False, 'reason': 'Insufficient data'}\n",
                "        \n",
                "        # Compare recent window to earlier window\n",
                "        early_proxy = np.mean(self.proxy_rewards[:self.window_size])\n",
                "        late_proxy = np.mean(self.proxy_rewards[-self.window_size:])\n",
                "        \n",
                "        early_gold = np.mean(self.gold_rewards[:self.window_size])\n",
                "        late_gold = np.mean(self.gold_rewards[-self.window_size:])\n",
                "        \n",
                "        proxy_delta = late_proxy - early_proxy\n",
                "        gold_delta = late_gold - early_gold\n",
                "        \n",
                "        # Hacking detected if proxy goes up but gold goes down\n",
                "        detected = proxy_delta > 0.1 and gold_delta < -0.05\n",
                "        \n",
                "        return {\n",
                "            'detected': detected,\n",
                "            'proxy_delta': proxy_delta,\n",
                "            'gold_delta': gold_delta,\n",
                "            'reason': 'Proxy \u2191 while gold \u2193' if detected else 'OK'\n",
                "        }\n",
                "    \n",
                "    def detect_length_hacking(self, baseline_length: float = 200) -> Dict:\n",
                "        \"\"\"Detect if model is gaming length.\"\"\"\n",
                "        if len(self.lengths) < self.window_size:\n",
                "            return {'detected': False, 'reason': 'Insufficient data'}\n",
                "        \n",
                "        avg_length = np.mean(self.lengths[-self.window_size:])\n",
                "        ratio = avg_length / baseline_length\n",
                "        \n",
                "        detected = ratio > 2.0  # More than 2x baseline\n",
                "        \n",
                "        return {\n",
                "            'detected': detected,\n",
                "            'avg_length': avg_length,\n",
                "            'length_ratio': ratio,\n",
                "            'reason': f'Length {ratio:.1f}x baseline' if detected else 'OK'\n",
                "        }\n",
                "    \n",
                "    def detect_kl_explosion(self, threshold: float = 15.0) -> Dict:\n",
                "        \"\"\"Detect if KL divergence is exploding.\"\"\"\n",
                "        if len(self.kl_values) < 10:\n",
                "            return {'detected': False, 'reason': 'Insufficient data'}\n",
                "        \n",
                "        recent_kl = np.mean(self.kl_values[-10:])\n",
                "        detected = recent_kl > threshold\n",
                "        \n",
                "        return {\n",
                "            'detected': detected,\n",
                "            'recent_kl': recent_kl,\n",
                "            'reason': f'KL={recent_kl:.1f} > {threshold}' if detected else 'OK'\n",
                "        }\n",
                "    \n",
                "    def should_early_stop(self) -> Tuple[bool, str]:\n",
                "        \"\"\"Determine if training should stop due to hacking.\"\"\"\n",
                "        overopt = self.detect_overoptimization()\n",
                "        length = self.detect_length_hacking()\n",
                "        kl = self.detect_kl_explosion()\n",
                "        \n",
                "        if overopt['detected']:\n",
                "            return True, f\"Overoptimization: {overopt['reason']}\"\n",
                "        if kl['detected']:\n",
                "            return True, f\"KL explosion: {kl['reason']}\"\n",
                "        if length['detected']:\n",
                "            return True, f\"Length hacking: {length['reason']}\"\n",
                "        \n",
                "        return False, \"Training healthy\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Simulation: Goodhart's Law\n",
                "\n",
                "The following function simulates the training dynamics where a model exploits the proxy reward. We observe three distinct phases:\n",
                "1.  **Alignment Phase (Steps 0-50):** Both metrics improve. This is the ideal training state.\n",
                "2.  **Plateau Phase (Steps 50-100):** The model hits the limit of the gold objective, but continues to optimize the proxy.\n",
                "3.  **Hacking Phase (Steps 100+):** The proxy reward continues to rise (the model \"thinks\" it's doing better), but the gold reward crashes. This is where we must alert or stop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_goodharts_law(steps: int = 200):\n",
                "    \"\"\"\n",
                "    Simulate what happens when proxy reward is overoptimized.\n",
                "    \n",
                "    Early training: Both proxy and gold improve together\n",
                "    Mid training: Gold plateaus\n",
                "    Late training: Proxy keeps rising, gold drops (HACKING!)\n",
                "    \"\"\"\n",
                "    np.random.seed(42)\n",
                "    detector = RewardHackingDetector(window_size=30)\n",
                "    \n",
                "    print(\"Simulating Goodhart's Law (Overoptimization):\")\n",
                "    print(f\"{'Step':>6} {'Proxy':>8} {'Gold':>8} {'Status':>20}\")\n",
                "    print(\"-\" * 45)\n",
                "    \n",
                "    for step in range(steps):\n",
                "        # Simulate training dynamics\n",
                "        if step < 50:  # Early: Both improve\n",
                "            proxy = 0.3 + step * 0.01 + np.random.normal(0, 0.02)\n",
                "            gold = 0.3 + step * 0.008 + np.random.normal(0, 0.02)\n",
                "        elif step < 100:  # Mid: Gold plateaus\n",
                "            proxy = 0.8 + (step - 50) * 0.005 + np.random.normal(0, 0.02)\n",
                "            gold = 0.7 + np.random.normal(0, 0.02)  # Plateau!\n",
                "        else:  # Late: Gold drops (HACKING)\n",
                "            proxy = 1.0 + (step - 100) * 0.003 + np.random.normal(0, 0.02)\n",
                "            gold = 0.7 - (step - 100) * 0.003 + np.random.normal(0, 0.02)  # Drops!\n",
                "        \n",
                "        kl = 3.0 + step * 0.05 + np.random.normal(0, 0.5)\n",
                "        length = int(200 + step * 1.5)\n",
                "        \n",
                "        detector.update(proxy, gold, kl, length)\n",
                "        \n",
                "        if step % 40 == 0 or step == steps - 1:\n",
                "            should_stop, reason = detector.should_early_stop()\n",
                "            status = f\"\u26a0\ufe0f {reason}\" if should_stop else \"\u2713 OK\"\n",
                "            print(f\"{step:>6} {proxy:>8.3f} {gold:>8.3f} {status:>20}\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 45)\n",
                "    final_check = detector.detect_overoptimization()\n",
                "    print(f\"Final verdict: {'\ud83d\udea8 HACKING DETECTED' if final_check['detected'] else '\u2713 OK'}\")\n",
                "    print(f\"  Proxy \u0394: {final_check.get('proxy_delta', 0):.3f}\")\n",
                "    print(f\"  Gold \u0394: {final_check.get('gold_delta', 0):.3f}\")\n",
                "\n",
                "simulate_goodharts_law()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Mitigation Strategy: Ensemble Reward Models\n",
                "\n",
                "A powerful mitigation strategy is **Reward Ensembling**. Instead of training a single reward model, we train multiple models (with different seeds, architectures, or data splits). \n",
                "\n",
                "The intuition is that while a hack might fool one model, it is less likely to fool an entire committee. We can use the **Uncertainty-Aware Reward**:\n",
                "\n",
                "$$ R_{conservative}(x) = \\mu(R(x)) - \\alpha \\cdot \\sigma(R(x)) $$\n",
                "\n",
                "Where $\\mu$ is the mean reward across the ensemble and $\\sigma$ is the standard deviation. This penalizes inputs where the models disagree, which often corresponds to out-of-distribution or \"hacky\" examples.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EnsembleRewardModel:\n",
                "    \"\"\"\n",
                "    Combine multiple reward models to resist hacking.\n",
                "    \n",
                "    Intuition: Harder to fool multiple models simultaneously.\n",
                "    Each RM has different biases \u2192 average cancels them out.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, reward_models: List[nn.Module]):\n",
                "        self.models = reward_models\n",
                "        self.n_models = len(reward_models)\n",
                "    \n",
                "    def compute_reward(self, input_ids: torch.Tensor) -> Dict:\n",
                "        \"\"\"\n",
                "        Compute ensemble reward.\n",
                "        \n",
                "        Returns:\n",
                "            Dict with 'mean', 'std', 'individual' rewards\n",
                "        \"\"\"\n",
                "        rewards = []\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for model in self.models:\n",
                "                r = model(input_ids)\n",
                "                rewards.append(r)\n",
                "        \n",
                "        stacked = torch.stack(rewards, dim=0)  # (n_models, batch)\n",
                "        \n",
                "        return {\n",
                "            'mean': stacked.mean(dim=0),  # Average across models\n",
                "            'std': stacked.std(dim=0),    # Uncertainty\n",
                "            'individual': rewards,\n",
                "        }\n",
                "    \n",
                "    def compute_conservative_reward(self, input_ids: torch.Tensor,\n",
                "                                    pessimism: float = 1.0) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Conservative reward: mean - pessimism * std\n",
                "        \n",
                "        Higher pessimism \u2192 more conservative \u2192 less hackable\n",
                "        \"\"\"\n",
                "        result = self.compute_reward(input_ids)\n",
                "        return result['mean'] - pessimism * result['std']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Complete Mitigation Pipeline\n",
                "\n",
                "Here we combine all defenses into a single `RewardHackingMitigation` system:\n",
                "1.  **Length Penalty**: Penalize responses that exceed a target length ratio.\n",
                "2.  **KL Penalty**: The standard PPO mechanism ($R_{total} = R_{model} - \\beta \\log \\frac{\\pi}{\\pi_{ref}}$).\n",
                "3.  **Hacking Detection**: Constant monitoring to trigger early stopping."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RewardHackingMitigation:\n",
                "    \"\"\"\n",
                "    Complete anti-hacking system combining all strategies:\n",
                "    \n",
                "    1. KL penalty: Prevent drift from reference\n",
                "    2. Length penalty: Prevent verbosity gaming\n",
                "    3. Ensemble averaging: Reduce exploitability\n",
                "    4. Early stopping: Detect and halt overoptimization\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 kl_beta: float = 0.1,\n",
                "                 target_length: int = 200,\n",
                "                 detector_window: int = 100):\n",
                "        \n",
                "        self.kl_beta = kl_beta\n",
                "        self.target_length = target_length\n",
                "        self.detector = RewardHackingDetector(window_size=detector_window)\n",
                "    \n",
                "    def compute_safe_reward(self,\n",
                "                            raw_reward: float,\n",
                "                            response_length: int,\n",
                "                            kl_divergence: float,\n",
                "                            gold_reward: Optional[float] = None) -> Dict:\n",
                "        \"\"\"\n",
                "        Compute reward with all anti-hacking measures applied.\n",
                "        \"\"\"\n",
                "        result = {'raw_reward': raw_reward}\n",
                "        \n",
                "        # 1. Length penalty (ratio method)\n",
                "        if response_length <= self.target_length:\n",
                "            length_penalized = raw_reward\n",
                "        else:\n",
                "            ratio = self.target_length / response_length\n",
                "            length_penalized = raw_reward * ratio\n",
                "        result['after_length_penalty'] = length_penalized\n",
                "        \n",
                "        # 2. KL penalty\n",
                "        kl_penalty = self.kl_beta * kl_divergence\n",
                "        kl_penalized = length_penalized - kl_penalty\n",
                "        result['after_kl_penalty'] = kl_penalized\n",
                "        \n",
                "        # 3. Update detector\n",
                "        # In production: gold_reward comes from held-out evaluation\n",
                "        if gold_reward is None:\n",
                "            gold_reward = raw_reward * 0.9  # Simulated\n",
                "        self.detector.update(raw_reward, gold_reward, kl_divergence, response_length)\n",
                "        \n",
                "        # 4. Check for early stopping\n",
                "        should_stop, reason = self.detector.should_early_stop()\n",
                "        result['should_stop'] = should_stop\n",
                "        result['stop_reason'] = reason\n",
                "        \n",
                "        result['final_reward'] = kl_penalized\n",
                "        return result"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Demonstration\n",
                "\n",
                "The demo below shows the mitigation system in action. The `raw_reward` increases (simulating hacking), but the `after_kl_penalty` stays stable or decreases because the KL divergence is rising. Eventually, the `RewardHackingDetector` triggers an early stop when the system detects the anomaly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def demo_mitigation_system():\n",
                "    \"\"\"Test the complete mitigation system.\"\"\"\n",
                "    \n",
                "    mitigation = RewardHackingMitigation(\n",
                "        kl_beta=0.1,\n",
                "        target_length=200,\n",
                "        detector_window=20,\n",
                "    )\n",
                "    \n",
                "    print(\"Anti-Hacking Reward Pipeline:\")\n",
                "    print(f\"{'Step':>5} {'Raw':>8} {'After Len':>10} {'After KL':>10} {'Status':>15}\")\n",
                "    print(\"-\" * 55)\n",
                "    \n",
                "    for step in range(50):\n",
                "        # Simulate increasingly hacky behavior\n",
                "        raw_reward = 0.5 + step * 0.02\n",
                "        length = 200 + step * 10  # Growing length\n",
                "        kl = 3.0 + step * 0.2     # Growing KL\n",
                "        \n",
                "        result = mitigation.compute_safe_reward(\n",
                "            raw_reward=raw_reward,\n",
                "            response_length=length,\n",
                "            kl_divergence=kl,\n",
                "        )\n",
                "        \n",
                "        if step % 10 == 0 or result['should_stop']:\n",
                "            status = '\u26a0\ufe0f STOP' if result['should_stop'] else '\u2713 OK'\n",
                "            print(f\"{step:>5} {raw_reward:>8.3f} {result['after_length_penalty']:>10.3f} \"\n",
                "                  f\"{result['after_kl_penalty']:>10.3f} {status:>15}\")\n",
                "            if result['should_stop']:\n",
                "                print(f\"\\n\ud83d\udea8 Early stop triggered: {result['stop_reason']}\")\n",
                "                break\n",
                "\n",
                "demo_mitigation_system()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}