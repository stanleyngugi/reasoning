{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05 â€” GRPO Implementation: Group Relative Policy Optimization\n",
                "\n",
                "> **Purpose:** Build a complete GRPO trainer from scratch. GRPO is DeepSeek's critic-free algorithm that powers R1 and DeepSeekMath.\n",
                "\n",
                "**Key innovation:** Eliminate the value network by computing advantages relative to a group of sampled responses.\n",
                "\n",
                "$$J_{GRPO}(\\theta) = \\mathbb{E}\\left[\\min(r_t A_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t)\\right] - \\beta D_{KL}(\\pi_\\theta \\| \\pi_{ref})$$\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.distributions import Categorical\n",
                "from typing import List, Dict, Tuple, Optional\n",
                "import copy\n",
                "import numpy as np\n",
                "\n",
                "torch.manual_seed(42)\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. TinyLM + Reference Model Wrapper\n",
                "\n",
                "GRPO requires a frozen reference model to compute KL divergence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TinyLM(nn.Module):\n",
                "    \"\"\"Minimal language model for RL experiments.\"\"\"\n",
                "    def __init__(self, vocab_size=100, hidden_size=64, num_layers=2):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
                "        self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
                "        self.head = nn.Linear(hidden_size, vocab_size)\n",
                "        self.vocab_size = vocab_size\n",
                "    \n",
                "    def forward(self, x):\n",
                "        h = self.embed(x)\n",
                "        h, _ = self.rnn(h)\n",
                "        return self.head(h)\n",
                "    \n",
                "    def get_log_probs(self, input_ids: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Compute log probabilities for each token in labels.\n",
                "        \n",
                "        Args:\n",
                "            input_ids: (batch, seq_len) input token IDs\n",
                "            labels: (batch, seq_len) target token IDs\n",
                "        \n",
                "        Returns:\n",
                "            log_probs: (batch, seq_len) log probability of each label token\n",
                "        \"\"\"\n",
                "        logits = self(input_ids)  # (batch, seq_len, vocab)\n",
                "        log_probs = F.log_softmax(logits, dim=-1)\n",
                "        \n",
                "        # Gather log probs for the actual tokens\n",
                "        token_log_probs = torch.gather(\n",
                "            log_probs, dim=-1, index=labels.unsqueeze(-1)\n",
                "        ).squeeze(-1)\n",
                "        \n",
                "        return token_log_probs\n",
                "\n",
                "\n",
                "class ReferenceModel:\n",
                "    \"\"\"\n",
                "    Wrapper for frozen reference model.\n",
                "    Used to compute KL divergence penalty.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model: nn.Module):\n",
                "        self.model = copy.deepcopy(model)\n",
                "        self.model.eval()\n",
                "        for param in self.model.parameters():\n",
                "            param.requires_grad = False\n",
                "    \n",
                "    def get_log_probs(self, input_ids: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Get log probs from frozen reference model.\"\"\"\n",
                "        with torch.no_grad():\n",
                "            return self.model.get_log_probs(input_ids, labels)\n",
                "    \n",
                "    def to(self, device):\n",
                "        self.model = self.model.to(device)\n",
                "        return self"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Policy log probs shape: torch.Size([4, 20])\n",
                        "Reference log probs shape: torch.Size([4, 20])\n",
                        "Initially equal (same model): True\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Model and reference model\n",
                "\n",
                "policy = TinyLM().to(device)\n",
                "ref_model = ReferenceModel(policy)\n",
                "\n",
                "# Sample batch\n",
                "batch_size, seq_len = 4, 20\n",
                "input_ids = torch.randint(0, 100, (batch_size, seq_len), device=device)\n",
                "labels = torch.randint(0, 100, (batch_size, seq_len), device=device)\n",
                "\n",
                "policy_log_probs = policy.get_log_probs(input_ids, labels)\n",
                "ref_log_probs = ref_model.get_log_probs(input_ids, labels)\n",
                "\n",
                "print(f\"Policy log probs shape: {policy_log_probs.shape}\")\n",
                "print(f\"Reference log probs shape: {ref_log_probs.shape}\")\n",
                "print(f\"Initially equal (same model): {torch.allclose(policy_log_probs, ref_log_probs)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Group-Relative Advantage Computation\n",
                "\n",
                "The core innovation: compute advantages by comparing responses within a group.\n",
                "\n",
                "$$A_i = \\frac{r_i - \\bar{r}}{\\sigma_r + \\delta}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_grpo_advantages(rewards: torch.Tensor, \n",
                "                            group_size: int,\n",
                "                            normalize: str = 'local') -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Compute group-relative advantages for GRPO.\n",
                "    \n",
                "    Args:\n",
                "        rewards: (batch_size,) reward for each response\n",
                "        group_size: Number of responses per prompt (K)\n",
                "        normalize: 'local' (per-group) or 'global' (batch-level std)\n",
                "    \n",
                "    Returns:\n",
                "        advantages: (batch_size,) normalized advantages\n",
                "    \"\"\"\n",
                "    batch_size = rewards.shape[0]\n",
                "    num_groups = batch_size // group_size\n",
                "    eps = 1e-8\n",
                "    \n",
                "    # Reshape to (num_groups, group_size)\n",
                "    grouped = rewards.view(num_groups, group_size)\n",
                "    \n",
                "    # Compute group mean (baseline)\n",
                "    group_mean = grouped.mean(dim=1, keepdim=True)\n",
                "    \n",
                "    if normalize == 'local':\n",
                "        # GRPO original: per-group std\n",
                "        group_std = grouped.std(dim=1, keepdim=True)\n",
                "        advantages = (grouped - group_mean) / (group_std + eps)\n",
                "    elif normalize == 'global':\n",
                "        # Hybrid: use batch-level std for stability\n",
                "        batch_std = rewards.std()\n",
                "        advantages = (grouped - group_mean) / (batch_std + eps)\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown normalize: {normalize}\")\n",
                "    \n",
                "    return advantages.view(batch_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Rewards per prompt:\n",
                        "  Prompt 1: [1.  0.8 0.6 0.4]\n",
                        "  Prompt 2: [1. 1. 1. 1.]\n",
                        "  Prompt 3: [0. 0. 0. 0.]\n",
                        "  Prompt 4: [1.  0.  0.5 0.5]\n",
                        "\n",
                        "Local normalization (max |A|): 1.22\n",
                        "Global normalization (max |A|): 1.15\n",
                        "\n",
                        "Local explodes on easy/hard prompts (std=0), global stays stable\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Group-relative advantages\n",
                "\n",
                "# 4 prompts, 4 responses each = 16 total\n",
                "rewards = torch.tensor([\n",
                "    1.0, 0.8, 0.6, 0.4,   # Prompt 1: varied\n",
                "    1.0, 1.0, 1.0, 1.0,   # Prompt 2: all correct (easy)\n",
                "    0.0, 0.0, 0.0, 0.0,   # Prompt 3: all wrong (hard)\n",
                "    1.0, 0.0, 0.5, 0.5,   # Prompt 4: mixed\n",
                "])\n",
                "\n",
                "adv_local = compute_grpo_advantages(rewards, group_size=4, normalize='local')\n",
                "adv_global = compute_grpo_advantages(rewards, group_size=4, normalize='global')\n",
                "\n",
                "print(\"Rewards per prompt:\")\n",
                "for i in range(4):\n",
                "    print(f\"  Prompt {i+1}: {rewards[i*4:(i+1)*4].numpy()}\")\n",
                "\n",
                "print(f\"\\nLocal normalization (max |A|): {adv_local.abs().max():.2f}\")\n",
                "print(f\"Global normalization (max |A|): {adv_global.abs().max():.2f}\")\n",
                "print(\"\\nLocal explodes on easy/hard prompts (std=0), global stays stable\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. KL Divergence Computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_kl_divergence(policy_log_probs: torch.Tensor,\n",
                "                          ref_log_probs: torch.Tensor,\n",
                "                          mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Compute KL divergence between policy and reference.\n",
                "    \n",
                "    KL(policy || ref) = E_policy[log(policy) - log(ref)]\n",
                "    \n",
                "    Args:\n",
                "        policy_log_probs: (batch, seq_len) log probs from policy\n",
                "        ref_log_probs: (batch, seq_len) log probs from reference\n",
                "        mask: Optional (batch, seq_len) mask for valid tokens\n",
                "    \n",
                "    Returns:\n",
                "        kl: (batch,) per-sequence KL divergence\n",
                "    \"\"\"\n",
                "    kl_per_token = policy_log_probs - ref_log_probs  # (batch, seq_len)\n",
                "    \n",
                "    if mask is not None:\n",
                "        kl_per_token = kl_per_token * mask\n",
                "        kl_per_sequence = kl_per_token.sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
                "    else:\n",
                "        kl_per_sequence = kl_per_token.mean(dim=1)\n",
                "    \n",
                "    return kl_per_sequence"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. GRPO Loss Function\n",
                "\n",
                "Clipped objective with KL penalty and token-level or sequence-level aggregation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_grpo_loss(policy_log_probs: torch.Tensor,\n",
                "                      old_log_probs: torch.Tensor,\n",
                "                      ref_log_probs: torch.Tensor,\n",
                "                      advantages: torch.Tensor,\n",
                "                      response_lengths: torch.Tensor,\n",
                "                      epsilon: float = 0.2,\n",
                "                      beta: float = 0.1,\n",
                "                      loss_aggregation: str = 'token') -> Dict[str, torch.Tensor]:\n",
                "    \"\"\"\n",
                "    Compute complete GRPO loss with clipping and KL penalty.\n",
                "    \n",
                "    Args:\n",
                "        policy_log_probs: (batch, seq_len) current policy log probs\n",
                "        old_log_probs: (batch, seq_len) log probs when responses were generated\n",
                "        ref_log_probs: (batch, seq_len) reference model log probs\n",
                "        advantages: (batch,) per-response advantages\n",
                "        response_lengths: (batch,) length of each response\n",
                "        epsilon: Clipping parameter\n",
                "        beta: KL penalty coefficient\n",
                "        loss_aggregation: 'token' or 'sequence'\n",
                "    \n",
                "    Returns:\n",
                "        Dict with 'loss', 'policy_loss', 'kl_loss', 'clip_fraction'\n",
                "    \"\"\"\n",
                "    batch_size, seq_len = policy_log_probs.shape\n",
                "    \n",
                "    # Create mask for valid tokens\n",
                "    mask = torch.arange(seq_len, device=policy_log_probs.device).unsqueeze(0) < response_lengths.unsqueeze(1)\n",
                "    mask = mask.float()\n",
                "    \n",
                "    # Expand advantages to token level\n",
                "    advantages_expanded = advantages.unsqueeze(1)  # (batch, 1)\n",
                "    \n",
                "    # Compute probability ratio\n",
                "    log_ratio = policy_log_probs - old_log_probs\n",
                "    ratio = torch.exp(log_ratio)  # (batch, seq_len)\n",
                "    \n",
                "    # Clipped ratio\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    \n",
                "    # PPO-style objective: min(ratio * A, clipped_ratio * A)\n",
                "    obj1 = ratio * advantages_expanded\n",
                "    obj2 = clipped_ratio * advantages_expanded\n",
                "    ppo_obj = torch.min(obj1, obj2)\n",
                "    \n",
                "    # Track clip fraction (for monitoring)\n",
                "    clipped = (ratio < 1 - epsilon) | (ratio > 1 + epsilon)\n",
                "    clip_fraction = (clipped * mask).sum() / mask.sum()\n",
                "    \n",
                "    # Aggregate policy loss\n",
                "    if loss_aggregation == 'token':\n",
                "        # Each token contributes equally\n",
                "        policy_loss = -(ppo_obj * mask).sum() / mask.sum()\n",
                "    elif loss_aggregation == 'sequence':\n",
                "        # Each sequence contributes equally\n",
                "        per_seq_loss = -(ppo_obj * mask).sum(dim=1) / response_lengths.float()\n",
                "        policy_loss = per_seq_loss.mean()\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown loss_aggregation: {loss_aggregation}\")\n",
                "    \n",
                "    # KL divergence penalty\n",
                "    kl = compute_kl_divergence(policy_log_probs, ref_log_probs, mask)\n",
                "    kl_loss = beta * kl.mean()\n",
                "    \n",
                "    # Total loss\n",
                "    total_loss = policy_loss + kl_loss\n",
                "    \n",
                "    return {\n",
                "        'loss': total_loss,\n",
                "        'policy_loss': policy_loss,\n",
                "        'kl_loss': kl_loss,\n",
                "        'kl': kl.mean(),\n",
                "        'clip_fraction': clip_fraction,\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Experience Buffer for Multiple PPO Epochs\n",
                "\n",
                "Store generated experiences to train on them multiple times."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ExperienceBuffer:\n",
                "    \"\"\"\n",
                "    Store rollout data for multiple PPO epochs.\n",
                "    \n",
                "    Each entry contains:\n",
                "    - input_ids: Prompt + response tokens\n",
                "    - labels: Response tokens (for log prob computation)\n",
                "    - old_log_probs: Log probs when response was generated\n",
                "    - rewards: Reward for each response\n",
                "    - response_lengths: Length of each response\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.data = []\n",
                "    \n",
                "    def add(self, input_ids: torch.Tensor, labels: torch.Tensor,\n",
                "            old_log_probs: torch.Tensor, rewards: torch.Tensor,\n",
                "            response_lengths: torch.Tensor):\n",
                "        \"\"\"Add a batch of experiences.\"\"\"\n",
                "        self.data.append({\n",
                "            'input_ids': input_ids.detach(),\n",
                "            'labels': labels.detach(),\n",
                "            'old_log_probs': old_log_probs.detach(),\n",
                "            'rewards': rewards.detach(),\n",
                "            'response_lengths': response_lengths.detach(),\n",
                "        })\n",
                "    \n",
                "    def get_all(self) -> Dict[str, torch.Tensor]:\n",
                "        \"\"\"Concatenate all stored experiences.\"\"\"\n",
                "        if not self.data:\n",
                "            return None\n",
                "        \n",
                "        return {\n",
                "            key: torch.cat([d[key] for d in self.data], dim=0)\n",
                "            for key in self.data[0].keys()\n",
                "        }\n",
                "    \n",
                "    def clear(self):\n",
                "        \"\"\"Clear all stored experiences.\"\"\"\n",
                "        self.data = []\n",
                "    \n",
                "    def __len__(self):\n",
                "        return sum(d['input_ids'].shape[0] for d in self.data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Complete GRPO Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GRPOTrainer:\n",
                "    \"\"\"\n",
                "    Full GRPO trainer implementation.\n",
                "    \n",
                "    Features:\n",
                "    - Group sampling with configurable group size\n",
                "    - Group-relative advantage computation\n",
                "    - Reference model KL penalty\n",
                "    - Multiple PPO epochs per rollout\n",
                "    - Configurable loss aggregation\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self,\n",
                "                 policy: nn.Module,\n",
                "                 ref_model: ReferenceModel,\n",
                "                 reward_fn,\n",
                "                 group_size: int = 4,\n",
                "                 epsilon: float = 0.2,\n",
                "                 beta: float = 0.1,\n",
                "                 ppo_epochs: int = 4,\n",
                "                 mini_batch_size: int = 8,\n",
                "                 loss_aggregation: str = 'token',\n",
                "                 normalize_advantages: str = 'global',\n",
                "                 lr: float = 1e-5,\n",
                "                 max_grad_norm: float = 1.0):\n",
                "        \n",
                "        self.policy = policy\n",
                "        self.ref_model = ref_model\n",
                "        self.reward_fn = reward_fn\n",
                "        self.group_size = group_size\n",
                "        self.epsilon = epsilon\n",
                "        self.beta = beta\n",
                "        self.ppo_epochs = ppo_epochs\n",
                "        self.mini_batch_size = mini_batch_size\n",
                "        self.loss_aggregation = loss_aggregation\n",
                "        self.normalize_advantages = normalize_advantages\n",
                "        self.max_grad_norm = max_grad_norm\n",
                "        \n",
                "        self.optimizer = torch.optim.AdamW(policy.parameters(), lr=lr)\n",
                "        self.experience_buffer = ExperienceBuffer()\n",
                "    \n",
                "    def collect_rollouts(self, prompts: torch.Tensor, max_length: int = 50) -> Dict:\n",
                "        \"\"\"\n",
                "        Generate responses for prompts and collect experience.\n",
                "        \n",
                "        Args:\n",
                "            prompts: (num_prompts, prompt_len) prompt token IDs\n",
                "            max_length: Maximum response length\n",
                "        \n",
                "        Returns:\n",
                "            Stats dict with generation metrics\n",
                "        \"\"\"\n",
                "        num_prompts = prompts.shape[0]\n",
                "        device = prompts.device\n",
                "        \n",
                "        # Repeat each prompt group_size times\n",
                "        expanded_prompts = prompts.repeat_interleave(self.group_size, dim=0)\n",
                "        \n",
                "        # Generate responses\n",
                "        self.policy.eval()\n",
                "        all_tokens = []\n",
                "        all_log_probs = []\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            tokens = expanded_prompts.clone()\n",
                "            for _ in range(max_length):\n",
                "                logits = self.policy(tokens)[:, -1, :]\n",
                "                dist = Categorical(logits=logits)\n",
                "                next_token = dist.sample()\n",
                "                log_prob = dist.log_prob(next_token)\n",
                "                \n",
                "                all_log_probs.append(log_prob)\n",
                "                tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)\n",
                "        \n",
                "        # Stack log probs\n",
                "        response_log_probs = torch.stack(all_log_probs, dim=1)  # (batch, max_length)\n",
                "        \n",
                "        # For simplicity, use full length as response_lengths\n",
                "        response_lengths = torch.full((tokens.shape[0],), max_length, device=device)\n",
                "        \n",
                "        # Get rewards\n",
                "        rewards = self.reward_fn(tokens)\n",
                "        \n",
                "        # Store in buffer\n",
                "        self.experience_buffer.add(\n",
                "            input_ids=tokens,\n",
                "            labels=tokens,  # Simplified: same as input\n",
                "            old_log_probs=response_log_probs,\n",
                "            rewards=rewards,\n",
                "            response_lengths=response_lengths,\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            'mean_reward': rewards.mean().item(),\n",
                "            'num_responses': tokens.shape[0],\n",
                "        }\n",
                "    \n",
                "    def train_step(self) -> Dict:\n",
                "        \"\"\"\n",
                "        Run multiple PPO epochs on collected experience.\n",
                "        \n",
                "        Returns:\n",
                "            Training stats\n",
                "        \"\"\"\n",
                "        experience = self.experience_buffer.get_all()\n",
                "        if experience is None:\n",
                "            return {}\n",
                "        \n",
                "        self.policy.train()\n",
                "        \n",
                "        # Compute advantages once (fixed for all epochs)\n",
                "        advantages = compute_grpo_advantages(\n",
                "            experience['rewards'],\n",
                "            self.group_size,\n",
                "            normalize=self.normalize_advantages\n",
                "        )\n",
                "        \n",
                "        # Get reference log probs\n",
                "        ref_log_probs = self.ref_model.get_log_probs(\n",
                "            experience['input_ids'][:, :-1],\n",
                "            experience['labels'][:, 1:]\n",
                "        )\n",
                "        \n",
                "        # Truncate old_log_probs to match\n",
                "        max_len = min(experience['old_log_probs'].shape[1], ref_log_probs.shape[1])\n",
                "        old_log_probs = experience['old_log_probs'][:, :max_len]\n",
                "        ref_log_probs = ref_log_probs[:, :max_len]\n",
                "        \n",
                "        stats = {'loss': 0, 'kl': 0, 'clip_fraction': 0}\n",
                "        num_updates = 0\n",
                "        \n",
                "        # Multiple PPO epochs\n",
                "        batch_size = experience['input_ids'].shape[0]\n",
                "        indices = np.arange(batch_size)\n",
                "        \n",
                "        for epoch in range(self.ppo_epochs):\n",
                "            np.random.shuffle(indices)\n",
                "            \n",
                "            # Mini-batch updates\n",
                "            for start in range(0, batch_size, self.mini_batch_size):\n",
                "                end = min(start + self.mini_batch_size, batch_size)\n",
                "                mb_indices = indices[start:end]\n",
                "                \n",
                "                # Get mini-batch data\n",
                "                mb_input_ids = experience['input_ids'][mb_indices]\n",
                "                mb_labels = experience['labels'][mb_indices]\n",
                "                mb_old_log_probs = old_log_probs[mb_indices]\n",
                "                mb_advantages = advantages[mb_indices]\n",
                "                mb_response_lengths = torch.full((len(mb_indices),), max_len, device=mb_input_ids.device)\n",
                "                mb_ref_log_probs = ref_log_probs[mb_indices]\n",
                "                \n",
                "                # Forward pass\n",
                "                policy_log_probs = self.policy.get_log_probs(\n",
                "                    mb_input_ids[:, :-1], mb_labels[:, 1:]\n",
                "                )[:, :max_len]\n",
                "                \n",
                "                # Compute loss\n",
                "                loss_dict = compute_grpo_loss(\n",
                "                    policy_log_probs=policy_log_probs,\n",
                "                    old_log_probs=mb_old_log_probs,\n",
                "                    ref_log_probs=mb_ref_log_probs,\n",
                "                    advantages=mb_advantages,\n",
                "                    response_lengths=mb_response_lengths,\n",
                "                    epsilon=self.epsilon,\n",
                "                    beta=self.beta,\n",
                "                    loss_aggregation=self.loss_aggregation,\n",
                "                )\n",
                "                \n",
                "                # Backward pass\n",
                "                self.optimizer.zero_grad()\n",
                "                loss_dict['loss'].backward()\n",
                "                \n",
                "                # Gradient clipping\n",
                "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
                "                \n",
                "                self.optimizer.step()\n",
                "                \n",
                "                # Accumulate stats\n",
                "                stats['loss'] += loss_dict['loss'].item()\n",
                "                stats['kl'] += loss_dict['kl'].item()\n",
                "                stats['clip_fraction'] += loss_dict['clip_fraction'].item()\n",
                "                num_updates += 1\n",
                "        \n",
                "        # Average stats\n",
                "        for key in stats:\n",
                "            stats[key] /= max(num_updates, 1)\n",
                "        \n",
                "        # Clear buffer after training\n",
                "        self.experience_buffer.clear()\n",
                "        \n",
                "        return stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training GRPO...\n",
                        " Step     Reward       Loss         KL      Clip%\n",
                        "--------------------------------------------------\n",
                        "    0     0.0600    -0.0033    -0.0006      16.1%\n",
                        "    1     0.0975    -0.0011     0.0009      14.5%\n",
                        "    2     0.0625     0.0010     0.0020      10.0%\n",
                        "    3     0.0725     0.0031     0.0024      13.0%\n",
                        "    4     0.0825     0.0030     0.0046      12.3%\n"
                    ]
                }
            ],
            "source": [
                "# TEST: GRPO Trainer on toy problem\n",
                "\n",
                "# Simple reward: higher if response contains more 1s\n",
                "def simple_reward(tokens):\n",
                "    return (tokens == 1).float().sum(dim=1) / tokens.shape[1]\n",
                "\n",
                "# Initialize\n",
                "policy = TinyLM(vocab_size=10).to(device)\n",
                "ref_model = ReferenceModel(policy).to(device)\n",
                "\n",
                "trainer = GRPOTrainer(\n",
                "    policy=policy,\n",
                "    ref_model=ref_model,\n",
                "    reward_fn=simple_reward,\n",
                "    group_size=4,\n",
                "    ppo_epochs=2,\n",
                "    mini_batch_size=4,\n",
                "    lr=1e-4,\n",
                "    beta=0.1,\n",
                ")\n",
                "\n",
                "# Training loop\n",
                "print(\"Training GRPO...\")\n",
                "print(f\"{'Step':>5} {'Reward':>10} {'Loss':>10} {'KL':>10} {'Clip%':>10}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for step in range(5):\n",
                "    # Generate prompts (just fixed tokens)\n",
                "    prompts = torch.zeros((4, 5), dtype=torch.long, device=device)\n",
                "    \n",
                "    # Collect rollouts\n",
                "    rollout_stats = trainer.collect_rollouts(prompts, max_length=20)\n",
                "    \n",
                "    # Train\n",
                "    train_stats = trainer.train_step()\n",
                "    \n",
                "    print(f\"{step:>5} {rollout_stats['mean_reward']:>10.4f} \"\n",
                "          f\"{train_stats.get('loss', 0):>10.4f} \"\n",
                "          f\"{train_stats.get('kl', 0):>10.4f} \"\n",
                "          f\"{train_stats.get('clip_fraction', 0)*100:>9.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Production Hyperparameters Reference\n",
                "\n",
                "Based on DeepSeek R1 and production systems:\n",
                "\n",
                "```python\n",
                "config = {\n",
                "    # Group sampling\n",
                "    'group_size': 8,              # Responses per prompt (K)\n",
                "    \n",
                "    # PPO parameters\n",
                "    'epsilon': 0.2,               # Clip range\n",
                "    'ppo_epochs': 4,              # Epochs per rollout\n",
                "    'mini_batch_size': 64,        # Per mini-batch\n",
                "    \n",
                "    # KL penalty\n",
                "    'beta': 0.1,                  # KL coefficient\n",
                "    'target_kl': 6.0,             # For adaptive KL\n",
                "    \n",
                "    # Optimization\n",
                "    'learning_rate': 1e-5,        # Policy LR\n",
                "    'max_grad_norm': 1.0,         # Gradient clipping\n",
                "    \n",
                "    # Aggregation (model-dependent)\n",
                "    'loss_aggregation': 'token',  # For base models\n",
                "    'normalize_advantages': 'global',  # Stable\n",
                "    \n",
                "    # Generation\n",
                "    'max_response_length': 8192,\n",
                "    'temperature': 1.0,\n",
                "}\n",
                "```\n",
                "\n",
                "---\n",
                "**Next:** `06_infrastructure_frameworks.ipynb` (OpenRLHF, Ray, vLLM)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
