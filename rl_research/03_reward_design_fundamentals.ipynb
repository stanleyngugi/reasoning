{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 — Reward Design Fundamentals: Real Implementations\n",
                "\n",
                "> **Purpose:** Implement reward functions for LLM reasoning training. Each component is production-ready code.\n",
                "\n",
                "| Component | What It Does |\n",
                "|-----------|-------------|\n",
                "| Verifiable Rewards | Binary accuracy + format checking |\n",
                "| Outcome Reward Model | Learned scalar from (prompt, response) |\n",
                "| RLAIF Scorer | Extract soft labels from LLM logits |\n",
                "| Reward Hacking Prevention | Length penalty, KL control |\n",
                "| Production Stack | Layered reward system |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import re\n",
                "import numpy as np\n",
                "from typing import List, Tuple, Optional, Dict\n",
                "\n",
                "torch.manual_seed(42)\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Verifiable Reward Model (VRM)\n",
                "\n",
                "DeepSeek R1's exact reward: `1.0 × accuracy + 0.2 × format`\n",
                "\n",
                "**Key insight:** No neural network in the reward loop → unhackable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VerifiableRewardModel:\n",
                "    \"\"\"\n",
                "    Rule-based reward model for verifiable tasks.\n",
                "    Based on DeepSeek R1's approach.\n",
                "    \n",
                "    Components:\n",
                "    - Accuracy reward: Binary (correct or not)\n",
                "    - Format reward: Structure compliance\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, accuracy_weight: float = 1.0, format_weight: float = 0.2):\n",
                "        self.accuracy_weight = accuracy_weight\n",
                "        self.format_weight = format_weight\n",
                "        \n",
                "        # Expected format: <think>...</think><answer>...</answer>\n",
                "        self.format_pattern = re.compile(\n",
                "            r'<think>.*?</think>\\s*<answer>.*?</answer>',\n",
                "            re.DOTALL\n",
                "        )\n",
                "        \n",
                "        # Answer extraction pattern\n",
                "        self.answer_pattern = re.compile(\n",
                "            r'<answer>(.*?)</answer>',\n",
                "            re.DOTALL\n",
                "        )\n",
                "    \n",
                "    def extract_answer(self, response: str) -> Optional[str]:\n",
                "        \"\"\"Extract the answer from structured response.\"\"\"\n",
                "        match = self.answer_pattern.search(response)\n",
                "        if match:\n",
                "            return match.group(1).strip()\n",
                "        \n",
                "        # Fallback: try to extract last number or boxed answer\n",
                "        boxed = re.search(r'\\\\boxed{([^}]+)}', response)\n",
                "        if boxed:\n",
                "            return boxed.group(1).strip()\n",
                "        \n",
                "        # Last number in response\n",
                "        numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', response)\n",
                "        return numbers[-1] if numbers else None\n",
                "    \n",
                "    def check_accuracy(self, response: str, ground_truth: str) -> float:\n",
                "        \"\"\"Check if extracted answer matches ground truth.\"\"\"\n",
                "        predicted = self.extract_answer(response)\n",
                "        if predicted is None:\n",
                "            return 0.0\n",
                "        \n",
                "        # Normalize for comparison\n",
                "        predicted = predicted.lower().strip()\n",
                "        ground_truth = str(ground_truth).lower().strip()\n",
                "        \n",
                "        # Exact match\n",
                "        if predicted == ground_truth:\n",
                "            return 1.0\n",
                "        \n",
                "        # Numeric comparison (handle floating point)\n",
                "        try:\n",
                "            if abs(float(predicted) - float(ground_truth)) < 1e-6:\n",
                "                return 1.0\n",
                "        except (ValueError, TypeError):\n",
                "            pass\n",
                "        \n",
                "        return 0.0\n",
                "    \n",
                "    def check_format(self, response: str) -> float:\n",
                "        \"\"\"Check if response follows expected format.\"\"\"\n",
                "        if self.format_pattern.search(response):\n",
                "            return 1.0\n",
                "        return 0.0\n",
                "    \n",
                "    def compute_reward(self, response: str, ground_truth: str) -> Dict[str, float]:\n",
                "        \"\"\"\n",
                "        Compute total reward with breakdown.\n",
                "        \n",
                "        Returns:\n",
                "            Dict with 'accuracy', 'format', and 'total' rewards\n",
                "        \"\"\"\n",
                "        accuracy = self.check_accuracy(response, ground_truth)\n",
                "        format_score = self.check_format(response)\n",
                "        \n",
                "        total = (self.accuracy_weight * accuracy + \n",
                "                 self.format_weight * format_score)\n",
                "        \n",
                "        return {\n",
                "            'accuracy': accuracy,\n",
                "            'format': format_score,\n",
                "            'total': total\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Response (truncated)                                  GT   Acc   Fmt  Total\n",
                        "---------------------------------------------------------------------------\n",
                        "<think>2 + 2 = 4</think><answer>4</answer>             4   1.0   1.0   1.20\n",
                        "<think>Let me compute...</think><answer>5</answer>     4   0.0   1.0   0.20\n",
                        "The answer is 4.                                       4   1.0   0.0   1.00\n",
                        "Therefore, \\boxed{42}                                 42   1.0   0.0   1.00\n",
                        "I don't know how to solve this.                        4   0.0   0.0   0.00\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Verifiable Reward Model\n",
                "\n",
                "vrm = VerifiableRewardModel()\n",
                "\n",
                "test_cases = [\n",
                "    # Good format, correct answer\n",
                "    (\"<think>2 + 2 = 4</think><answer>4</answer>\", \"4\"),\n",
                "    # Good format, wrong answer\n",
                "    (\"<think>Let me compute...</think><answer>5</answer>\", \"4\"),\n",
                "    # Bad format, correct answer (fallback extraction)\n",
                "    (\"The answer is 4.\", \"4\"),\n",
                "    # LaTeX boxed answer\n",
                "    (\"Therefore, \\\\boxed{42}\", \"42\"),\n",
                "    # No answer extractable\n",
                "    (\"I don't know how to solve this.\", \"4\"),\n",
                "]\n",
                "\n",
                "print(f\"{'Response (truncated)':<50} {'GT':>5} {'Acc':>5} {'Fmt':>5} {'Total':>6}\")\n",
                "print(\"-\" * 75)\n",
                "for response, gt in test_cases:\n",
                "    result = vrm.compute_reward(response, gt)\n",
                "    display = response[:47] + \"...\" if len(response) > 50 else response\n",
                "    print(f\"{display:<50} {gt:>5} {result['accuracy']:>5.1f} {result['format']:>5.1f} {result['total']:>6.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Code Execution Reward\n",
                "\n",
                "For programming tasks: execute code against test cases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import tempfile\n",
                "import os\n",
                "\n",
                "class CodeExecutionReward:\n",
                "    \"\"\"\n",
                "    Reward model for code generation tasks.\n",
                "    Executes code against test cases.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, timeout: float = 5.0):\n",
                "        self.timeout = timeout\n",
                "    \n",
                "    def extract_code(self, response: str) -> Optional[str]:\n",
                "        \"\"\"Extract Python code from response.\"\"\"\n",
                "        # Try to find code blocks\n",
                "        patterns = [\n",
                "            r'```python\\n(.*?)```',\n",
                "            r'```\\n(.*?)```',\n",
                "            r'<code>(.*?)</code>',\n",
                "        ]\n",
                "        for pattern in patterns:\n",
                "            match = re.search(pattern, response, re.DOTALL)\n",
                "            if match:\n",
                "                return match.group(1).strip()\n",
                "        return None\n",
                "    \n",
                "    def execute_with_tests(self, code: str, test_cases: List[Tuple[str, str]]) -> Dict:\n",
                "        \"\"\"\n",
                "        Execute code and run test cases.\n",
                "        \n",
                "        Args:\n",
                "            code: Python code to execute\n",
                "            test_cases: List of (input, expected_output) tuples\n",
                "        \n",
                "        Returns:\n",
                "            Dict with 'passed', 'total', 'reward'\n",
                "        \"\"\"\n",
                "        passed = 0\n",
                "        total = len(test_cases)\n",
                "        \n",
                "        for test_input, expected in test_cases:\n",
                "            # Create test script\n",
                "            test_script = f\"\"\"\n",
                "{code}\n",
                "\n",
                "# Run test\n",
                "result = solution({test_input})\n",
                "print(result)\n",
                "\"\"\"\n",
                "            try:\n",
                "                with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
                "                    f.write(test_script)\n",
                "                    temp_path = f.name\n",
                "                \n",
                "                result = subprocess.run(\n",
                "                    ['python', temp_path],\n",
                "                    capture_output=True,\n",
                "                    text=True,\n",
                "                    timeout=self.timeout\n",
                "                )\n",
                "                \n",
                "                os.unlink(temp_path)\n",
                "                \n",
                "                if result.returncode == 0:\n",
                "                    output = result.stdout.strip()\n",
                "                    if output == str(expected).strip():\n",
                "                        passed += 1\n",
                "            except Exception:\n",
                "                pass\n",
                "        \n",
                "        return {\n",
                "            'passed': passed,\n",
                "            'total': total,\n",
                "            'reward': passed / total if total > 0 else 0.0\n",
                "        }\n",
                "    \n",
                "    def compute_reward(self, response: str, test_cases: List[Tuple]) -> Dict:\n",
                "        \"\"\"Compute reward for code response.\"\"\"\n",
                "        code = self.extract_code(response)\n",
                "        if code is None:\n",
                "            return {'passed': 0, 'total': len(test_cases), 'reward': 0.0}\n",
                "        \n",
                "        return self.execute_with_tests(code, test_cases)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Outcome Reward Model (ORM)\n",
                "\n",
                "A neural network that scores (prompt, response) pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class OutcomeRewardModel(nn.Module):\n",
                "    \"\"\"\n",
                "    Learned reward model that scores complete responses.\n",
                "    \n",
                "    Architecture:\n",
                "    - Embed prompt + response\n",
                "    - Pool to fixed size\n",
                "    - Project to scalar reward\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size: int = 50000, hidden_size: int = 256):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
                "        self.encoder = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
                "        self.reward_head = nn.Linear(hidden_size, 1)\n",
                "    \n",
                "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Compute reward for input sequences.\n",
                "        \n",
                "        Args:\n",
                "            input_ids: (batch, seq_len) token IDs for prompt + response\n",
                "        \n",
                "        Returns:\n",
                "            rewards: (batch,) scalar rewards\n",
                "        \"\"\"\n",
                "        h = self.embed(input_ids)  # (batch, seq_len, hidden)\n",
                "        _, h_final = self.encoder(h)  # (1, batch, hidden)\n",
                "        h_final = h_final.squeeze(0)  # (batch, hidden)\n",
                "        rewards = self.reward_head(h_final).squeeze(-1)  # (batch,)\n",
                "        return rewards\n",
                "    \n",
                "    def compute_preference_loss(self, chosen_ids: torch.Tensor, \n",
                "                                 rejected_ids: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Bradley-Terry preference loss for training.\n",
                "        \n",
                "        Args:\n",
                "            chosen_ids: Token IDs for preferred responses\n",
                "            rejected_ids: Token IDs for rejected responses\n",
                "        \n",
                "        Returns:\n",
                "            loss: Scalar loss to minimize\n",
                "        \"\"\"\n",
                "        r_chosen = self.forward(chosen_ids)\n",
                "        r_rejected = self.forward(rejected_ids)\n",
                "        \n",
                "        # Log-sigmoid of reward difference\n",
                "        # Loss: -log(sigmoid(r_chosen - r_rejected))\n",
                "        loss = -F.logsigmoid(r_chosen - r_rejected).mean()\n",
                "        return loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input shape: torch.Size([4, 100])\n",
                        "Output rewards: [0.2161513  0.23310584 0.133057   0.15649924]\n",
                        "Reward shape: torch.Size([4])\n"
                    ]
                }
            ],
            "source": [
                "# TEST: ORM forward pass\n",
                "\n",
                "orm = OutcomeRewardModel(vocab_size=1000, hidden_size=64).to(device)\n",
                "\n",
                "# Simulate batch of (prompt + response) sequences\n",
                "batch_size = 4\n",
                "seq_len = 100\n",
                "input_ids = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
                "\n",
                "rewards = orm(input_ids)\n",
                "print(f\"Input shape: {input_ids.shape}\")\n",
                "print(f\"Output rewards: {rewards.detach().cpu().numpy()}\")\n",
                "print(f\"Reward shape: {rewards.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. RLAIF: AI Feedback with Soft Labels\n",
                "\n",
                "Use an LLM to score responses on a 1-10 scale."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RLAIFScorer:\n",
                "    \"\"\"\n",
                "    RLAIF/d-RLAIF scorer using LLM logits.\n",
                "    \n",
                "    Key techniques:\n",
                "    - Soft labels (probability distribution over scores)\n",
                "    - Position bias mitigation (average both orderings)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, score_tokens: List[str] = None):\n",
                "        # Token IDs for scores 1-10\n",
                "        # In practice, this maps to actual tokenizer IDs\n",
                "        self.score_tokens = score_tokens or ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
                "    \n",
                "    def compute_expected_score(self, logits: torch.Tensor, \n",
                "                                score_token_ids: List[int]) -> float:\n",
                "        \"\"\"\n",
                "        Compute expected score from logits.\n",
                "        \n",
                "        Args:\n",
                "            logits: (vocab_size,) logits for next token\n",
                "            score_token_ids: Token IDs for '1', '2', ..., '10'\n",
                "        \n",
                "        Returns:\n",
                "            Expected score in [1, 10]\n",
                "        \"\"\"\n",
                "        # Extract logits for score tokens\n",
                "        score_logits = logits[score_token_ids]\n",
                "        \n",
                "        # Convert to probabilities\n",
                "        probs = F.softmax(score_logits, dim=0)\n",
                "        \n",
                "        # Compute expected score: sum(i * P(i))\n",
                "        scores = torch.arange(1, len(score_token_ids) + 1, dtype=torch.float, device=logits.device)\n",
                "        expected = (scores * probs).sum()\n",
                "        \n",
                "        return expected.item()\n",
                "    \n",
                "    def normalize_score(self, score: float, min_val: float = 1.0, \n",
                "                        max_val: float = 10.0) -> float:\n",
                "        \"\"\"\n",
                "        Normalize score to [-1, 1] for RL.\n",
                "        \"\"\"\n",
                "        return 2 * (score - min_val) / (max_val - min_val) - 1\n",
                "    \n",
                "    def mitigate_position_bias(self, score_a_first: float, \n",
                "                                score_b_first: float) -> Tuple[float, float]:\n",
                "        \"\"\"\n",
                "        Mitigate position bias by averaging both orderings.\n",
                "        \n",
                "        When comparing A vs B:\n",
                "        - First inference: A presented first → get score_A1\n",
                "        - Second inference: B presented first → get score_A2\n",
                "        - Final: average(score_A1, 1 - score_B_when_first)\n",
                "        \"\"\"\n",
                "        # For pairwise comparison, we'd run both orderings\n",
                "        # Here we just demonstrate the averaging\n",
                "        avg = (score_a_first + score_b_first) / 2\n",
                "        return avg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RLAIF Soft Label Extraction:\n",
                        "  Raw logits (scores 1-10): [-1.07 -0.09 -0.82 -0.41  0.48 -0.83  3.    2.5   0.17  0.66]\n",
                        "  Probabilities: [0.009 0.023 0.011 0.017 0.041 0.011 0.505 0.306 0.03  0.049]\n",
                        "  Expected score: 7.16\n",
                        "  Normalized reward: 0.369\n"
                    ]
                }
            ],
            "source": [
                "# DEMONSTRATION: Soft label extraction\n",
                "\n",
                "def demo_soft_labels():\n",
                "    \"\"\"Show how soft labels are extracted from LLM logits.\"\"\"\n",
                "    \n",
                "    # Simulate LLM logits for next token (vocab size 100)\n",
                "    vocab_size = 100\n",
                "    logits = torch.randn(vocab_size)\n",
                "    \n",
                "    # Assume tokens 0-9 correspond to scores '1'-'10'\n",
                "    score_token_ids = list(range(10))\n",
                "    \n",
                "    # Make the model \"prefer\" scores 7-8\n",
                "    logits[6] = 3.0  # Score 7\n",
                "    logits[7] = 2.5  # Score 8\n",
                "    \n",
                "    scorer = RLAIFScorer()\n",
                "    expected = scorer.compute_expected_score(logits, score_token_ids)\n",
                "    normalized = scorer.normalize_score(expected)\n",
                "    \n",
                "    print(\"RLAIF Soft Label Extraction:\")\n",
                "    print(f\"  Raw logits (scores 1-10): {logits[:10].numpy().round(2)}\")\n",
                "    probs = F.softmax(logits[:10], dim=0).numpy()\n",
                "    print(f\"  Probabilities: {probs.round(3)}\")\n",
                "    print(f\"  Expected score: {expected:.2f}\")\n",
                "    print(f\"  Normalized reward: {normalized:.3f}\")\n",
                "\n",
                "demo_soft_labels()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Reward Hacking Prevention\n",
                "\n",
                "### 5.1 Length Penalty"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LengthPenalizedReward:\n",
                "    \"\"\"\n",
                "    Prevent length hacking by penalizing excessive verbosity.\n",
                "    \n",
                "    Methods:\n",
                "    - Linear penalty: reward - alpha * length\n",
                "    - Quadratic penalty: reward - alpha * (length - target)^2\n",
                "    - Ratio penalty: reward * min(1, target_length / actual_length)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, method: str = 'ratio', target_length: int = 500, alpha: float = 0.001):\n",
                "        self.method = method\n",
                "        self.target_length = target_length\n",
                "        self.alpha = alpha\n",
                "    \n",
                "    def apply_penalty(self, reward: float, response_length: int) -> float:\n",
                "        \"\"\"\n",
                "        Apply length penalty to reward.\n",
                "        \n",
                "        Args:\n",
                "            reward: Raw reward score\n",
                "            response_length: Number of tokens in response\n",
                "        \n",
                "        Returns:\n",
                "            Penalized reward\n",
                "        \"\"\"\n",
                "        if self.method == 'linear':\n",
                "            # Simple linear penalty\n",
                "            penalty = self.alpha * max(0, response_length - self.target_length)\n",
                "            return reward - penalty\n",
                "        \n",
                "        elif self.method == 'quadratic':\n",
                "            # Quadratic penalty for deviation from target\n",
                "            diff = response_length - self.target_length\n",
                "            penalty = self.alpha * diff ** 2 if diff > 0 else 0\n",
                "            return reward - penalty\n",
                "        \n",
                "        elif self.method == 'ratio':\n",
                "            # Scale reward by efficiency ratio\n",
                "            if response_length <= self.target_length:\n",
                "                return reward\n",
                "            ratio = self.target_length / response_length\n",
                "            return reward * ratio\n",
                "        \n",
                "        return reward"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Length Penalty Demo (target=500 tokens):\n",
                        "    Length   Raw Reward    Penalized    Reduction\n",
                        "--------------------------------------------------\n",
                        "       200         1.00         1.00         0.0%\n",
                        "       500         1.00         1.00         0.0%\n",
                        "      1000         1.00         0.50        50.0%\n",
                        "      2000         1.00         0.25        75.0%\n",
                        "      5000         1.00         0.10        90.0%\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Length penalty effects\n",
                "\n",
                "lp = LengthPenalizedReward(method='ratio', target_length=500)\n",
                "\n",
                "print(\"Length Penalty Demo (target=500 tokens):\")\n",
                "print(f\"{'Length':>10} {'Raw Reward':>12} {'Penalized':>12} {'Reduction':>12}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for length in [200, 500, 1000, 2000, 5000]:\n",
                "    raw = 1.0\n",
                "    penalized = lp.apply_penalty(raw, length)\n",
                "    reduction = (1 - penalized / raw) * 100\n",
                "    print(f\"{length:>10} {raw:>12.2f} {penalized:>12.2f} {reduction:>11.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Adaptive KL Controller"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AdaptiveKLController:\n",
                "    \"\"\"\n",
                "    Adaptive KL penalty coefficient.\n",
                "    \n",
                "    Adjusts beta (KL coefficient) to keep KL divergence\n",
                "    within a target range. Based on Anthropic's approach.\n",
                "    \n",
                "    If KL > target: increase beta (penalize more)\n",
                "    If KL < target: decrease beta (allow more exploration)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, init_beta: float = 0.1, target_kl: float = 6.0,\n",
                "                 horizon: int = 10000):\n",
                "        self.beta = init_beta\n",
                "        self.target_kl = target_kl\n",
                "        self.horizon = horizon  # Steps to adjust over\n",
                "    \n",
                "    def update(self, current_kl: float) -> float:\n",
                "        \"\"\"\n",
                "        Update beta based on current KL divergence.\n",
                "        \n",
                "        Args:\n",
                "            current_kl: Measured KL divergence\n",
                "        \n",
                "        Returns:\n",
                "            Updated beta value\n",
                "        \"\"\"\n",
                "        # Proportional control\n",
                "        error = current_kl - self.target_kl\n",
                "        adjustment = 1 + error / self.horizon\n",
                "        \n",
                "        # Clamp adjustment to prevent instability\n",
                "        adjustment = max(0.5, min(2.0, adjustment))\n",
                "        \n",
                "        self.beta *= adjustment\n",
                "        \n",
                "        # Clamp beta to reasonable range\n",
                "        self.beta = max(0.001, min(1.0, self.beta))\n",
                "        \n",
                "        return self.beta\n",
                "    \n",
                "    def compute_kl_penalty(self, log_probs: torch.Tensor, \n",
                "                           ref_log_probs: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Compute KL penalty for given log probabilities.\n",
                "        \n",
                "        KL(policy || reference) ≈ E[log(policy) - log(reference)]\n",
                "        \n",
                "        Args:\n",
                "            log_probs: (batch, seq_len) current policy log probs\n",
                "            ref_log_probs: (batch, seq_len) reference model log probs\n",
                "        \n",
                "        Returns:\n",
                "            kl_penalty: (batch,) per-sequence KL penalty\n",
                "        \"\"\"\n",
                "        kl_per_token = log_probs - ref_log_probs  # (batch, seq_len)\n",
                "        kl_per_sequence = kl_per_token.mean(dim=1)  # (batch,)\n",
                "        \n",
                "        return self.beta * kl_per_sequence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Adaptive KL Controller Demo:\n",
                        "  Step   Current KL       Beta          Action\n",
                        "--------------------------------------------------\n",
                        "     0          3.0     0.1000 ↓ relax penalty\n",
                        "     1          4.5     0.1000 ↓ relax penalty\n",
                        "     2          7.0     0.1000 ↑ penalize more\n",
                        "     3         10.0     0.1000 ↑ penalize more\n",
                        "     4          8.0     0.1000 ↑ penalize more\n",
                        "     5          6.5     0.1000 ↑ penalize more\n",
                        "     6          6.0     0.1000      → maintain\n",
                        "     7          5.5     0.1000 ↓ relax penalty\n",
                        "     8          6.0     0.1000      → maintain\n",
                        "     9          6.2     0.1000 ↑ penalize more\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Adaptive KL controller\n",
                "\n",
                "kl_ctrl = AdaptiveKLController(init_beta=0.1, target_kl=6.0)\n",
                "\n",
                "print(\"Adaptive KL Controller Demo:\")\n",
                "print(f\"{'Step':>6} {'Current KL':>12} {'Beta':>10} {'Action':>15}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "# Simulate KL divergence over training\n",
                "simulated_kl = [3.0, 4.5, 7.0, 10.0, 8.0, 6.5, 6.0, 5.5, 6.0, 6.2]\n",
                "\n",
                "for step, kl in enumerate(simulated_kl):\n",
                "    old_beta = kl_ctrl.beta\n",
                "    new_beta = kl_ctrl.update(kl)\n",
                "    \n",
                "    if kl > kl_ctrl.target_kl:\n",
                "        action = \"↑ penalize more\"\n",
                "    elif kl < kl_ctrl.target_kl:\n",
                "        action = \"↓ relax penalty\"\n",
                "    else:\n",
                "        action = \"→ maintain\"\n",
                "    \n",
                "    print(f\"{step:>6} {kl:>12.1f} {new_beta:>10.4f} {action:>15}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Production Reward Stack\n",
                "\n",
                "Combine all components into a layered reward system."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ProductionRewardStack:\n",
                "    \"\"\"\n",
                "    Three-layer reward architecture:\n",
                "    \n",
                "    Layer 1 (Foundation): Verifiable rewards (unhackable)\n",
                "    Layer 2 (Quality): Learned preferences (RLAIF/ORM)\n",
                "    Layer 3 (Weighting): Dynamic combination based on training phase\n",
                "    \n",
                "    Plus: KL penalty and length penalty\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, \n",
                "                 vrm: VerifiableRewardModel,\n",
                "                 orm: Optional[OutcomeRewardModel] = None,\n",
                "                 kl_controller: Optional[AdaptiveKLController] = None,\n",
                "                 length_penalty: Optional[LengthPenalizedReward] = None,\n",
                "                 verifiable_weight: float = 0.9,\n",
                "                 quality_weight: float = 0.1):\n",
                "        \n",
                "        self.vrm = vrm\n",
                "        self.orm = orm\n",
                "        self.kl_controller = kl_controller or AdaptiveKLController()\n",
                "        self.length_penalty = length_penalty or LengthPenalizedReward()\n",
                "        \n",
                "        self.verifiable_weight = verifiable_weight\n",
                "        self.quality_weight = quality_weight\n",
                "    \n",
                "    def compute_reward(self, \n",
                "                       response: str,\n",
                "                       ground_truth: str,\n",
                "                       response_ids: Optional[torch.Tensor] = None,\n",
                "                       log_probs: Optional[torch.Tensor] = None,\n",
                "                       ref_log_probs: Optional[torch.Tensor] = None) -> Dict:\n",
                "        \"\"\"\n",
                "        Compute full reward stack.\n",
                "        \n",
                "        Returns:\n",
                "            Dict with all reward components and total\n",
                "        \"\"\"\n",
                "        result = {}\n",
                "        \n",
                "        # Layer 1: Verifiable rewards\n",
                "        vrm_result = self.vrm.compute_reward(response, ground_truth)\n",
                "        result['verifiable'] = vrm_result['total']\n",
                "        result['accuracy'] = vrm_result['accuracy']\n",
                "        result['format'] = vrm_result['format']\n",
                "        \n",
                "        # Layer 2: Quality rewards (if ORM provided)\n",
                "        if self.orm is not None and response_ids is not None:\n",
                "            with torch.no_grad():\n",
                "                quality = self.orm(response_ids).item()\n",
                "            result['quality'] = quality\n",
                "        else:\n",
                "            result['quality'] = 0.0\n",
                "        \n",
                "        # Dynamic weighting\n",
                "        combined = (self.verifiable_weight * result['verifiable'] + \n",
                "                   self.quality_weight * result['quality'])\n",
                "        \n",
                "        # Apply length penalty\n",
                "        response_length = len(response.split())  # Simple word count\n",
                "        combined = self.length_penalty.apply_penalty(combined, response_length)\n",
                "        result['length_penalized'] = combined\n",
                "        \n",
                "        # KL penalty (if log probs provided)\n",
                "        if log_probs is not None and ref_log_probs is not None:\n",
                "            kl_penalty = self.kl_controller.compute_kl_penalty(\n",
                "                log_probs, ref_log_probs\n",
                "            )\n",
                "            result['kl_penalty'] = kl_penalty.item()\n",
                "            combined = combined - result['kl_penalty']\n",
                "        else:\n",
                "            result['kl_penalty'] = 0.0\n",
                "        \n",
                "        result['total'] = combined\n",
                "        return result\n",
                "    \n",
                "    def adjust_weights(self, training_progress: float):\n",
                "        \"\"\"\n",
                "        Dynamically adjust layer weights based on training progress.\n",
                "        \n",
                "        Early training: Rely more on verifiable (90%)\n",
                "        Late training: Allow more quality influence (70%)\n",
                "        \n",
                "        Args:\n",
                "            training_progress: Float in [0, 1], 0=start, 1=end\n",
                "        \"\"\"\n",
                "        # Linear interpolation\n",
                "        early_v = 0.9\n",
                "        late_v = 0.7\n",
                "        \n",
                "        self.verifiable_weight = early_v - (early_v - late_v) * training_progress\n",
                "        self.quality_weight = 1.0 - self.verifiable_weight"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Production Reward Stack Demo:\n",
                        "      Case   Accuracy   Format   Length    Total\n",
                        "--------------------------------------------------\n",
                        "         1        1.0      1.0     1.08     1.08\n",
                        "         2        1.0      1.0     0.72     0.72\n",
                        "         3        0.0      1.0     0.18     0.18\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Production Reward Stack\n",
                "\n",
                "# Initialize components\n",
                "vrm = VerifiableRewardModel()\n",
                "lp = LengthPenalizedReward(target_length=100)  # Short target for demo\n",
                "kl = AdaptiveKLController()\n",
                "\n",
                "stack = ProductionRewardStack(\n",
                "    vrm=vrm,\n",
                "    length_penalty=lp,\n",
                "    kl_controller=kl,\n",
                "    verifiable_weight=0.9,\n",
                "    quality_weight=0.1\n",
                ")\n",
                "\n",
                "# Test responses\n",
                "test_cases = [\n",
                "    (\"<think>2+2=4</think><answer>4</answer>\", \"4\"),\n",
                "    (\"<think>\" + \"Let me think... \" * 50 + \"</think><answer>4</answer>\", \"4\"),  # Verbose\n",
                "    (\"<think>Wrong calculation</think><answer>5</answer>\", \"4\"),  # Wrong answer\n",
                "]\n",
                "\n",
                "print(\"Production Reward Stack Demo:\")\n",
                "print(f\"{'Case':>10} {'Accuracy':>10} {'Format':>8} {'Length':>8} {'Total':>8}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for i, (response, gt) in enumerate(test_cases):\n",
                "    result = stack.compute_reward(response, gt)\n",
                "    print(f\"{i+1:>10} {result['accuracy']:>10.1f} {result['format']:>8.1f} \"\n",
                "          f\"{result['length_penalized']:>8.2f} {result['total']:>8.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary: Reward Design Principles\n",
                "\n",
                "**Implementation Hierarchy:**\n",
                "\n",
                "```python\n",
                "# 1. Start with verifiable rewards (if applicable)\n",
                "vrm = VerifiableRewardModel()\n",
                "\n",
                "# 2. Add quality signals for subjective aspects\n",
                "orm = OutcomeRewardModel()  # or RLAIFScorer()\n",
                "\n",
                "# 3. Prevent hacking\n",
                "length_penalty = LengthPenalizedReward()\n",
                "kl_controller = AdaptiveKLController()\n",
                "\n",
                "# 4. Combine into production stack\n",
                "stack = ProductionRewardStack(vrm, orm, kl_controller, length_penalty)\n",
                "```\n",
                "\n",
                "**Key Principles:**\n",
                "\n",
                "1. **Verifiable > Learned** — Ground in objective truth when possible\n",
                "2. **Simple > Complex** — DeepSeek R1 uses just 2 signals\n",
                "3. **Prevent hacking** — KL penalty + length penalty are essential\n",
                "4. **Dynamic weighting** — Trust verifiable more early in training\n",
                "\n",
                "---\n",
                "**Tier 1 Complete!** You now have the foundational understanding for production RL training of reasoning models."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
