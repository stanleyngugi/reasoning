{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 — The Four Technical Pillars: Real Implementations\n",
                "\n",
                "> **Purpose:** Implement the four critical RL training techniques as real, runnable code. Each technique is a modular function you can plug into any training loop.\n",
                "\n",
                "**Prerequisites:** This notebook builds on `01_rl_training_loop_foundations.ipynb`. We reuse the TinyLM model.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.distributions import Categorical\n",
                "import numpy as np\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup: TinyLM from Section 01\n",
                "\n",
                "A minimal language model for teaching purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Parameters: 62,820\n"
                    ]
                }
            ],
            "source": [
                "class TinyLM(nn.Module):\n",
                "    \"\"\"Minimal language model for RL experiments.\"\"\"\n",
                "    def __init__(self, vocab_size=100, hidden_size=64, num_layers=2):\n",
                "        super().__init__()\n",
                "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
                "        self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
                "        self.head = nn.Linear(hidden_size, vocab_size)\n",
                "        self.vocab_size = vocab_size\n",
                "    \n",
                "    def forward(self, x):\n",
                "        h = self.embed(x)\n",
                "        h, _ = self.rnn(h)\n",
                "        return self.head(h)\n",
                "    \n",
                "    def generate(self, prompt, max_len=20, temperature=1.0):\n",
                "        \"\"\"Generate tokens autoregressively.\"\"\"\n",
                "        self.eval()\n",
                "        tokens = prompt.clone()\n",
                "        log_probs = []\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for _ in range(max_len):\n",
                "                logits = self(tokens)[:, -1, :] / temperature\n",
                "                dist = Categorical(logits=logits)\n",
                "                next_token = dist.sample()\n",
                "                log_probs.append(dist.log_prob(next_token))\n",
                "                tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)\n",
                "        \n",
                "        return tokens, torch.stack(log_probs, dim=1)\n",
                "\n",
                "# Instantiate\n",
                "model = TinyLM().to(device)\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Advantage Normalization: Three Real Implementations\n",
                "\n",
                "Each function takes rewards and returns normalized advantages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_grpo_local(rewards, group_size):\n",
                "    \"\"\"\n",
                "    GRPO-style local normalization.\n",
                "    Normalizes within each prompt's group of responses.\n",
                "    \n",
                "    Args:\n",
                "        rewards: Tensor of shape (batch_size,) - reward per response\n",
                "        group_size: int - number of responses per prompt (K)\n",
                "    \n",
                "    Returns:\n",
                "        advantages: Tensor of shape (batch_size,)\n",
                "    \"\"\"\n",
                "    batch_size = rewards.shape[0]\n",
                "    num_groups = batch_size // group_size\n",
                "    eps = 1e-8\n",
                "    \n",
                "    # Reshape to (num_groups, group_size)\n",
                "    grouped = rewards.view(num_groups, group_size)\n",
                "    \n",
                "    # Compute per-group statistics\n",
                "    group_mean = grouped.mean(dim=1, keepdim=True)  # (num_groups, 1)\n",
                "    group_std = grouped.std(dim=1, keepdim=True)    # (num_groups, 1)\n",
                "    \n",
                "    # Normalize within each group\n",
                "    advantages = (grouped - group_mean) / (group_std + eps)\n",
                "    \n",
                "    return advantages.view(batch_size)\n",
                "\n",
                "\n",
                "def normalize_global(rewards):\n",
                "    \"\"\"\n",
                "    REINFORCE++-style global normalization.\n",
                "    Uses batch-level mean and std.\n",
                "    \n",
                "    Args:\n",
                "        rewards: Tensor of shape (batch_size,)\n",
                "    \n",
                "    Returns:\n",
                "        advantages: Tensor of shape (batch_size,)\n",
                "    \"\"\"\n",
                "    eps = 1e-8\n",
                "    batch_mean = rewards.mean()\n",
                "    batch_std = rewards.std()\n",
                "    \n",
                "    return (rewards - batch_mean) / (batch_std + eps)\n",
                "\n",
                "\n",
                "def normalize_hybrid(rewards, group_size):\n",
                "    \"\"\"\n",
                "    Hybrid normalization: local mean + global std.\n",
                "    The production-recommended approach.\n",
                "    \n",
                "    Args:\n",
                "        rewards: Tensor of shape (batch_size,)\n",
                "        group_size: int - number of responses per prompt (K)\n",
                "    \n",
                "    Returns:\n",
                "        advantages: Tensor of shape (batch_size,)\n",
                "    \"\"\"\n",
                "    batch_size = rewards.shape[0]\n",
                "    num_groups = batch_size // group_size\n",
                "    eps = 1e-8\n",
                "    \n",
                "    # Global std (stable, never near zero)\n",
                "    batch_std = rewards.std()\n",
                "    \n",
                "    # Reshape to (num_groups, group_size)\n",
                "    grouped = rewards.view(num_groups, group_size)\n",
                "    \n",
                "    # Local mean (preserves intra-prompt competition)\n",
                "    group_mean = grouped.mean(dim=1, keepdim=True)\n",
                "    \n",
                "    # Hybrid: local mean, global std\n",
                "    advantages = (grouped - group_mean) / (batch_std + eps)\n",
                "    \n",
                "    return advantages.view(batch_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Rewards: [1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
                        "\n",
                        "Method                  Max |Advantage|      Prompt 1 (easy)\n",
                        "------------------------------------------------------------\n",
                        "GRPO (local)                       1.50 [0. 0. 0. 0.]\n",
                        "Global                             1.10 [1.0978876 1.0978876 1.0978876 1.0978876]\n",
                        "Hybrid                             1.46 [0. 0. 0. 0.]\n",
                        "\n",
                        "GRPO explodes on Prompt 1 (all-correct) because local std = 0\n",
                        "Hybrid stays stable because it uses global std\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Compare normalization strategies on easy data (gradient explosion case)\n",
                "\n",
                "# Scenario: 4 prompts, 4 responses each. First prompt is \"easy\" (all correct).\n",
                "rewards = torch.tensor([\n",
                "    1.0, 1.0, 1.0, 1.0,   # Prompt 1: all correct (std=0!)\n",
                "    1.0, 0.0, 1.0, 0.0,   # Prompt 2: mixed\n",
                "    0.0, 0.0, 1.0, 0.0,   # Prompt 3: mostly wrong\n",
                "    0.0, 0.0, 0.0, 0.0,   # Prompt 4: all wrong (std=0!)\n",
                "])\n",
                "\n",
                "group_size = 4\n",
                "\n",
                "adv_local = normalize_grpo_local(rewards, group_size)\n",
                "adv_global = normalize_global(rewards)\n",
                "adv_hybrid = normalize_hybrid(rewards, group_size)\n",
                "\n",
                "print(\"Rewards:\", rewards.numpy())\n",
                "print()\n",
                "print(f\"{'Method':<20} {'Max |Advantage|':>18} {'Prompt 1 (easy)':>20}\")\n",
                "print(\"-\" * 60)\n",
                "print(f\"{'GRPO (local)':<20} {adv_local.abs().max().item():>18.2f} {adv_local[:4].numpy()}\")\n",
                "print(f\"{'Global':<20} {adv_global.abs().max().item():>18.2f} {adv_global[:4].numpy()}\")\n",
                "print(f\"{'Hybrid':<20} {adv_hybrid.abs().max().item():>18.2f} {adv_hybrid[:4].numpy()}\")\n",
                "print()\n",
                "print(\"GRPO explodes on Prompt 1 (all-correct) because local std = 0\")\n",
                "print(\"Hybrid stays stable because it uses global std\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Clip-Higher: Asymmetric PPO Clipping\n",
                "\n",
                "Standard PPO clips ratio to `[1-ε, 1+ε]`. Clip-Higher uses `[1-ε_low, 1+ε_high]` where `ε_high > ε_low`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_ppo_loss_symmetric(log_probs, old_log_probs, advantages, epsilon=0.2):\n",
                "    \"\"\"\n",
                "    Standard PPO loss with symmetric clipping.\n",
                "    \n",
                "    Args:\n",
                "        log_probs: Current policy log probs, shape (batch, seq_len)\n",
                "        old_log_probs: Old policy log probs, shape (batch, seq_len)\n",
                "        advantages: Advantage values, shape (batch,) or (batch, 1)\n",
                "        epsilon: Clipping parameter (default 0.2)\n",
                "    \n",
                "    Returns:\n",
                "        loss: Scalar tensor (to minimize, so we negate the objective)\n",
                "    \"\"\"\n",
                "    # Expand advantages to match token dimension\n",
                "    if advantages.dim() == 1:\n",
                "        advantages = advantages.unsqueeze(1)  # (batch, 1)\n",
                "    \n",
                "    # Probability ratio\n",
                "    ratio = torch.exp(log_probs - old_log_probs)  # (batch, seq_len)\n",
                "    \n",
                "    # Clipped ratio (symmetric)\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
                "    \n",
                "    # PPO objective: min of clipped and unclipped\n",
                "    obj1 = ratio * advantages\n",
                "    obj2 = clipped_ratio * advantages\n",
                "    ppo_obj = torch.min(obj1, obj2)\n",
                "    \n",
                "    # Loss is negative objective (we minimize loss)\n",
                "    return -ppo_obj.mean()\n",
                "\n",
                "\n",
                "def compute_ppo_loss_clip_higher(log_probs, old_log_probs, advantages, \n",
                "                                  epsilon_low=0.2, epsilon_high=0.28):\n",
                "    \"\"\"\n",
                "    PPO loss with asymmetric clipping (Clip-Higher).\n",
                "    Allows more growth for advantageous tokens.\n",
                "    \n",
                "    Args:\n",
                "        log_probs: Current policy log probs\n",
                "        old_log_probs: Old policy log probs\n",
                "        advantages: Advantage values\n",
                "        epsilon_low: Lower bound clipping (default 0.2)\n",
                "        epsilon_high: Upper bound clipping (default 0.28)\n",
                "    \n",
                "    Returns:\n",
                "        loss: Scalar tensor\n",
                "    \"\"\"\n",
                "    if advantages.dim() == 1:\n",
                "        advantages = advantages.unsqueeze(1)\n",
                "    \n",
                "    ratio = torch.exp(log_probs - old_log_probs)\n",
                "    \n",
                "    # Asymmetric clipping: lower bound is tighter than upper bound\n",
                "    clipped_ratio = torch.clamp(ratio, 1 - epsilon_low, 1 + epsilon_high)\n",
                "    \n",
                "    obj1 = ratio * advantages\n",
                "    obj2 = clipped_ratio * advantages\n",
                "    ppo_obj = torch.min(obj1, obj2)\n",
                "    \n",
                "    return -ppo_obj.mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Desired ratio: 4.95x (model wants to 5x the probability)\n",
                        "\n",
                        "Symmetric (ε=0.2): clipped to 1.20x\n",
                        "Clip-Higher (ε_high=0.28): clipped to 1.28x\n",
                        "\n",
                        "Clip-Higher allows 7% more update per step\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Clip-Higher allows more growth for low-probability tokens\n",
                "\n",
                "# Simulate a scenario: token with low old probability, positive advantage\n",
                "# Model wants to increase probability significantly\n",
                "\n",
                "old_log_prob = torch.tensor([[-3.9]])  # exp(-3.9) ≈ 0.02 (2% probability)\n",
                "new_log_prob = torch.tensor([[-2.3]])  # exp(-2.3) ≈ 0.10 (10% probability)\n",
                "advantage = torch.tensor([1.0])        # Positive advantage\n",
                "\n",
                "ratio = torch.exp(new_log_prob - old_log_prob).item()\n",
                "print(f\"Desired ratio: {ratio:.2f}x (model wants to 5x the probability)\")\n",
                "print()\n",
                "\n",
                "# Symmetric clipping clips at 1.2\n",
                "clipped_sym = min(ratio, 1.2)\n",
                "print(f\"Symmetric (ε=0.2): clipped to {clipped_sym:.2f}x\")\n",
                "\n",
                "# Clip-Higher clips at 1.28\n",
                "clipped_high = min(ratio, 1.28)\n",
                "print(f\"Clip-Higher (ε_high=0.28): clipped to {clipped_high:.2f}x\")\n",
                "\n",
                "print()\n",
                "print(f\"Clip-Higher allows {(clipped_high/clipped_sym - 1)*100:.0f}% more update per step\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Loss Aggregation: Token-Level vs Sequence-Level"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def aggregate_loss_sequence_level(per_token_loss, response_lengths):\n",
                "    \"\"\"\n",
                "    Sequence-level aggregation: each response contributes equally.\n",
                "    \n",
                "    Args:\n",
                "        per_token_loss: Tensor of shape (batch, max_seq_len)\n",
                "        response_lengths: Tensor of shape (batch,) - actual length of each response\n",
                "    \n",
                "    Returns:\n",
                "        loss: Scalar tensor\n",
                "    \"\"\"\n",
                "    batch_size = per_token_loss.shape[0]\n",
                "    \n",
                "    # Create mask for valid tokens\n",
                "    max_len = per_token_loss.shape[1]\n",
                "    mask = torch.arange(max_len, device=per_token_loss.device).unsqueeze(0) < response_lengths.unsqueeze(1)\n",
                "    \n",
                "    # Average loss per response, then average across responses\n",
                "    masked_loss = per_token_loss * mask\n",
                "    per_response_loss = masked_loss.sum(dim=1) / response_lengths.float()  # (batch,)\n",
                "    \n",
                "    return per_response_loss.mean()\n",
                "\n",
                "\n",
                "def aggregate_loss_token_level(per_token_loss, response_lengths):\n",
                "    \"\"\"\n",
                "    Token-level aggregation: each token contributes equally.\n",
                "    \n",
                "    Args:\n",
                "        per_token_loss: Tensor of shape (batch, max_seq_len)\n",
                "        response_lengths: Tensor of shape (batch,) - actual length of each response\n",
                "    \n",
                "    Returns:\n",
                "        loss: Scalar tensor\n",
                "    \"\"\"\n",
                "    max_len = per_token_loss.shape[1]\n",
                "    mask = torch.arange(max_len, device=per_token_loss.device).unsqueeze(0) < response_lengths.unsqueeze(1)\n",
                "    \n",
                "    # Sum all token losses, divide by total token count\n",
                "    total_loss = (per_token_loss * mask).sum()\n",
                "    total_tokens = response_lengths.sum()\n",
                "    \n",
                "    return total_loss / total_tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Two responses: 100 tokens and 1000 tokens, same per-token loss\n",
                        "\n",
                        "Sequence-level loss: 0.1000\n",
                        "Token-level loss: 0.1000\n",
                        "\n",
                        "Effective weight of each response:\n",
                        "  Sequence-level: Response 1 = 50%, Response 2 = 50%\n",
                        "  Token-level: Response 1 = 9.1%, Response 2 = 90.9%\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Show the difference in weighting\n",
                "\n",
                "# Two responses with same per-token loss but different lengths\n",
                "per_token_loss = torch.ones(2, 100) * 0.1  # Same loss per token\n",
                "response_lengths = torch.tensor([100, 1000])  # Short vs long\n",
                "\n",
                "# Pad the second response\n",
                "per_token_loss_padded = torch.zeros(2, 1000)\n",
                "per_token_loss_padded[0, :100] = 0.1\n",
                "per_token_loss_padded[1, :1000] = 0.1\n",
                "\n",
                "seq_loss = aggregate_loss_sequence_level(per_token_loss_padded, response_lengths)\n",
                "tok_loss = aggregate_loss_token_level(per_token_loss_padded, response_lengths)\n",
                "\n",
                "print(\"Two responses: 100 tokens and 1000 tokens, same per-token loss\")\n",
                "print()\n",
                "print(f\"Sequence-level loss: {seq_loss.item():.4f}\")\n",
                "print(f\"Token-level loss: {tok_loss.item():.4f}\")\n",
                "print()\n",
                "\n",
                "# Compute effective weight per response\n",
                "print(\"Effective weight of each response:\")\n",
                "print(f\"  Sequence-level: Response 1 = 50%, Response 2 = 50%\")\n",
                "print(f\"  Token-level: Response 1 = {100/1100*100:.1f}%, Response 2 = {1000/1100*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Overlong Filtering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_overlong_mask(response_lengths, max_length, has_eos):\n",
                "    \"\"\"\n",
                "    Create mask to filter out truncated (overlong) responses.\n",
                "    \n",
                "    Args:\n",
                "        response_lengths: Tensor of shape (batch,) - length of each response\n",
                "        max_length: int - the maximum generation length limit\n",
                "        has_eos: Tensor of shape (batch,) - whether response has EOS token\n",
                "    \n",
                "    Returns:\n",
                "        mask: Tensor of shape (batch,) - True for responses to KEEP\n",
                "    \"\"\"\n",
                "    # Response is truncated if it hit max_length AND has no EOS\n",
                "    is_truncated = (response_lengths >= max_length) & (~has_eos)\n",
                "    \n",
                "    # Keep responses that are NOT truncated\n",
                "    return ~is_truncated\n",
                "\n",
                "\n",
                "def apply_overlong_filter(rewards, advantages, valid_mask):\n",
                "    \"\"\"\n",
                "    Zero out rewards/advantages for truncated responses.\n",
                "    \n",
                "    Args:\n",
                "        rewards: Tensor of shape (batch,)\n",
                "        advantages: Tensor of shape (batch,)\n",
                "        valid_mask: Tensor of shape (batch,) - True for valid responses\n",
                "    \n",
                "    Returns:\n",
                "        filtered_rewards, filtered_advantages\n",
                "    \"\"\"\n",
                "    return rewards * valid_mask.float(), advantages * valid_mask.float()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Response       Length    Has EOS   Reward    Keep?\n",
                        "-------------------------------------------------------\n",
                        "1                 500       True      1.0     True\n",
                        "2                3000       True      1.0     True\n",
                        "3                8192      False      0.0    False ← FILTERED (truncated, might be false neg)\n",
                        "4                8192       True      1.0     True\n",
                        "5                 400       True      0.0     True\n"
                    ]
                }
            ],
            "source": [
                "# TEST: Overlong filtering\n",
                "\n",
                "max_length = 8192\n",
                "\n",
                "response_lengths = torch.tensor([500, 3000, 8192, 8192, 400])\n",
                "has_eos = torch.tensor([True, True, False, True, True])\n",
                "rewards = torch.tensor([1.0, 1.0, 0.0, 1.0, 0.0])  # Response 3 is false negative\n",
                "\n",
                "mask = create_overlong_mask(response_lengths, max_length, has_eos)\n",
                "\n",
                "print(f\"{'Response':<12} {'Length':>8} {'Has EOS':>10} {'Reward':>8} {'Keep?':>8}\")\n",
                "print(\"-\" * 55)\n",
                "for i in range(len(response_lengths)):\n",
                "    note = \" ← FILTERED (truncated, might be false neg)\" if not mask[i] else \"\"\n",
                "    print(f\"{i+1:<12} {response_lengths[i].item():>8} {str(has_eos[i].item()):>10} {rewards[i].item():>8.1f} {str(mask[i].item()):>8}{note}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Complete Configurable Trainer\n",
                "\n",
                "Putting it all together with switchable options."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConfigurableRLTrainer:\n",
                "    \"\"\"\n",
                "    RL trainer with configurable technical pillars.\n",
                "    \n",
                "    Supports:\n",
                "    - Normalization: 'local', 'global', 'hybrid'\n",
                "    - Loss aggregation: 'token', 'sequence'\n",
                "    - Clip-Higher: True/False\n",
                "    - Overlong filtering: True/False\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model, \n",
                "                 norm_type='hybrid',\n",
                "                 loss_agg='token',\n",
                "                 clip_higher=False,\n",
                "                 overlong_filter=False,\n",
                "                 group_size=4,\n",
                "                 max_length=8192,\n",
                "                 epsilon=0.2,\n",
                "                 epsilon_high=0.28,\n",
                "                 lr=1e-5):\n",
                "        \n",
                "        self.model = model\n",
                "        self.norm_type = norm_type\n",
                "        self.loss_agg = loss_agg\n",
                "        self.clip_higher = clip_higher\n",
                "        self.overlong_filter = overlong_filter\n",
                "        self.group_size = group_size\n",
                "        self.max_length = max_length\n",
                "        self.epsilon = epsilon\n",
                "        self.epsilon_high = epsilon_high\n",
                "        \n",
                "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    \n",
                "    def compute_advantages(self, rewards):\n",
                "        \"\"\"Compute advantages using configured normalization.\"\"\"\n",
                "        if self.norm_type == 'local':\n",
                "            return normalize_grpo_local(rewards, self.group_size)\n",
                "        elif self.norm_type == 'global':\n",
                "            return normalize_global(rewards)\n",
                "        elif self.norm_type == 'hybrid':\n",
                "            return normalize_hybrid(rewards, self.group_size)\n",
                "        else:\n",
                "            raise ValueError(f\"Unknown norm_type: {self.norm_type}\")\n",
                "    \n",
                "    def compute_loss(self, log_probs, old_log_probs, advantages, response_lengths):\n",
                "        \"\"\"Compute PPO loss with configured clipping and aggregation.\"\"\"\n",
                "        \n",
                "        # Expand advantages to token level\n",
                "        advantages_expanded = advantages.unsqueeze(1).expand_as(log_probs)\n",
                "        \n",
                "        # Probability ratio\n",
                "        ratio = torch.exp(log_probs - old_log_probs)\n",
                "        \n",
                "        # Clipping\n",
                "        if self.clip_higher:\n",
                "            clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon_high)\n",
                "        else:\n",
                "            clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
                "        \n",
                "        # PPO objective (per token)\n",
                "        obj1 = ratio * advantages_expanded\n",
                "        obj2 = clipped_ratio * advantages_expanded\n",
                "        per_token_obj = torch.min(obj1, obj2)\n",
                "        \n",
                "        # We want to maximize objective, so loss = -objective\n",
                "        per_token_loss = -per_token_obj\n",
                "        \n",
                "        # Aggregation\n",
                "        if self.loss_agg == 'token':\n",
                "            return aggregate_loss_token_level(per_token_loss, response_lengths)\n",
                "        elif self.loss_agg == 'sequence':\n",
                "            return aggregate_loss_sequence_level(per_token_loss, response_lengths)\n",
                "        else:\n",
                "            raise ValueError(f\"Unknown loss_agg: {self.loss_agg}\")\n",
                "    \n",
                "    def train_step(self, prompts, rewards, log_probs, old_log_probs, \n",
                "                   response_lengths, has_eos):\n",
                "        \"\"\"\n",
                "        Single training step with all configured techniques.\n",
                "        \n",
                "        Returns:\n",
                "            loss: The computed loss value\n",
                "        \"\"\"\n",
                "        # Overlong filtering\n",
                "        if self.overlong_filter:\n",
                "            valid_mask = create_overlong_mask(response_lengths, self.max_length, has_eos)\n",
                "            # Zero out rewards for filtered responses\n",
                "            rewards = rewards * valid_mask.float()\n",
                "        \n",
                "        # Compute advantages\n",
                "        advantages = self.compute_advantages(rewards)\n",
                "        \n",
                "        # Compute loss\n",
                "        loss = self.compute_loss(log_probs, old_log_probs, advantages, response_lengths)\n",
                "        \n",
                "        # Optimization step\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "        \n",
                "        return loss.item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Recommended configurations by model type:\n",
                        "\n",
                        "Lite PPO (base model):\n",
                        "  norm_type: hybrid\n",
                        "  loss_agg: token\n",
                        "  clip_higher: False\n",
                        "  overlong_filter: False\n",
                        "\n",
                        "Aligned Model Config:\n",
                        "  norm_type: hybrid\n",
                        "  loss_agg: sequence\n",
                        "  clip_higher: True\n",
                        "  overlong_filter: False\n",
                        "\n",
                        "GRPO (original):\n",
                        "  norm_type: local\n",
                        "  loss_agg: sequence\n",
                        "  clip_higher: False\n",
                        "  overlong_filter: False\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# DEMONSTRATION: Different trainer configurations\n",
                "\n",
                "configs = {\n",
                "    'Lite PPO (base model)': {\n",
                "        'norm_type': 'hybrid',\n",
                "        'loss_agg': 'token',\n",
                "        'clip_higher': False,\n",
                "        'overlong_filter': False,\n",
                "    },\n",
                "    'Aligned Model Config': {\n",
                "        'norm_type': 'hybrid',\n",
                "        'loss_agg': 'sequence',\n",
                "        'clip_higher': True,\n",
                "        'overlong_filter': False,\n",
                "    },\n",
                "    'GRPO (original)': {\n",
                "        'norm_type': 'local',\n",
                "        'loss_agg': 'sequence',\n",
                "        'clip_higher': False,\n",
                "        'overlong_filter': False,\n",
                "    },\n",
                "}\n",
                "\n",
                "print(\"Recommended configurations by model type:\\n\")\n",
                "for name, config in configs.items():\n",
                "    print(f\"{name}:\")\n",
                "    for k, v in config.items():\n",
                "        print(f\"  {k}: {v}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary: Decision Rules\n",
                "\n",
                "```python\n",
                "# Base model configuration\n",
                "trainer = ConfigurableRLTrainer(\n",
                "    model,\n",
                "    norm_type='hybrid',    # Always use hybrid\n",
                "    loss_agg='token',      # Token-level for base models\n",
                "    clip_higher=False,     # No benefit for base models\n",
                "    overlong_filter=False, # Only if max_length <= 8K\n",
                ")\n",
                "\n",
                "# Aligned model configuration  \n",
                "trainer = ConfigurableRLTrainer(\n",
                "    model,\n",
                "    norm_type='hybrid',     # Always use hybrid\n",
                "    loss_agg='sequence',    # Sequence-level for aligned models\n",
                "    clip_higher=True,       # Prevents entropy collapse\n",
                "    overlong_filter=False,  # Only if max_length <= 8K\n",
                ")\n",
                "```\n",
                "\n",
                "**Key insight:** The effectiveness of each technique depends on model state, not just the technique itself.\n",
                "\n",
                "---\n",
                "**Next:** `03_reward_design_fundamentals.ipynb`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
