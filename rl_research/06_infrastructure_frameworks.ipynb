{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 06 — Infrastructure & Frameworks for RL Training\n",
                "\n",
                "> **Purpose:** Understand the production stack for training LLMs with RL. This notebook covers the key components without requiring actual distributed infrastructure.\n",
                "\n",
                "**The Modern RL4LLM Stack:**\n",
                "\n",
                "| Component | Purpose | Tool |\n",
                "|-----------|---------|------|\n",
                "| Distributed Orchestration | Coordinate Actor/Critic/Reward models | **Ray** |\n",
                "| Fast Generation | Accelerate sample generation | **vLLM** |\n",
                "| Memory Optimization | Train 70B+ models | **DeepSpeed ZeRO** |\n",
                "| Training Framework | End-to-end RLHF/GRPO | **OpenRLHF** / **TRL** |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.9.0+cpu\n",
                        "CUDA available: False\n"
                    ]
                }
            ],
            "source": [
                "# Note: This notebook demonstrates concepts and configurations.\n",
                "# Actual distributed training requires a Ray cluster and GPUs.\n",
                "\n",
                "import torch\n",
                "import json\n",
                "from typing import Dict, List, Any\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The RLHF Bottleneck Problem\n",
                "\n",
                "**Key insight:** Generation consumes 80% of training time.\n",
                "\n",
                "```\n",
                "Time Breakdown:\n",
                "┌────────────────────────────────────────────────────────────┐\n",
                "│████████████████████████████████████████████████░░░░░░░░░░░░│\n",
                "│           GENERATION (80%)           │TRAIN(15%)│RWD(5%)│\n",
                "└────────────────────────────────────────────────────────────┘\n",
                "```\n",
                "\n",
                "This is why vLLM (for fast inference) is critical."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def estimate_rlhf_time_breakdown(\n",
                "    num_prompts: int,\n",
                "    group_size: int,\n",
                "    avg_response_tokens: int,\n",
                "    tokens_per_second: float,  # Generation throughput\n",
                "    train_time_per_batch: float,  # Seconds\n",
                ") -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Estimate time breakdown for RLHF training step.\n",
                "    \n",
                "    Args:\n",
                "        num_prompts: Number of prompts per batch\n",
                "        group_size: Responses per prompt (K in GRPO)\n",
                "        avg_response_tokens: Average response length\n",
                "        tokens_per_second: Generation throughput (e.g., 100 for HF, 2000 for vLLM)\n",
                "        train_time_per_batch: Time for gradient update\n",
                "    \n",
                "    Returns:\n",
                "        Time breakdown dict\n",
                "    \"\"\"\n",
                "    total_responses = num_prompts * group_size\n",
                "    total_tokens = total_responses * avg_response_tokens\n",
                "    \n",
                "    gen_time = total_tokens / tokens_per_second\n",
                "    reward_time = total_responses * 0.01  # Assume 10ms per reward\n",
                "    total_time = gen_time + train_time_per_batch + reward_time\n",
                "    \n",
                "    return {\n",
                "        'generation_sec': gen_time,\n",
                "        'training_sec': train_time_per_batch,\n",
                "        'reward_sec': reward_time,\n",
                "        'total_sec': total_time,\n",
                "        'generation_pct': gen_time / total_time * 100,\n",
                "        'training_pct': train_time_per_batch / total_time * 100,\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "HuggingFace Transformers (100 tok/s):\n",
                        "  Generation: 1280.0s (99.4%)\n",
                        "  Training: 5.0s (0.4%)\n",
                        "  Total: 1287.6s\n",
                        "\n",
                        "vLLM (2400 tok/s = 24x faster):\n",
                        "  Generation: 53.3s (87.6%)\n",
                        "  Training: 5.0s (8.2%)\n",
                        "  Total: 60.9s\n",
                        "\n",
                        "Overall speedup: 21.1x\n"
                    ]
                }
            ],
            "source": [
                "# Compare HuggingFace vs vLLM generation speed\n",
                "\n",
                "params = dict(\n",
                "    num_prompts=32,\n",
                "    group_size=8,\n",
                "    avg_response_tokens=500,\n",
                "    train_time_per_batch=5.0,\n",
                ")\n",
                "\n",
                "hf_breakdown = estimate_rlhf_time_breakdown(**params, tokens_per_second=100)\n",
                "vllm_breakdown = estimate_rlhf_time_breakdown(**params, tokens_per_second=2400)\n",
                "\n",
                "print(\"HuggingFace Transformers (100 tok/s):\")\n",
                "print(f\"  Generation: {hf_breakdown['generation_sec']:.1f}s ({hf_breakdown['generation_pct']:.1f}%)\")\n",
                "print(f\"  Training: {hf_breakdown['training_sec']:.1f}s ({hf_breakdown['training_pct']:.1f}%)\")\n",
                "print(f\"  Total: {hf_breakdown['total_sec']:.1f}s\")\n",
                "\n",
                "print(\"\\nvLLM (2400 tok/s = 24x faster):\")\n",
                "print(f\"  Generation: {vllm_breakdown['generation_sec']:.1f}s ({vllm_breakdown['generation_pct']:.1f}%)\")\n",
                "print(f\"  Training: {vllm_breakdown['training_sec']:.1f}s ({vllm_breakdown['training_pct']:.1f}%)\")\n",
                "print(f\"  Total: {vllm_breakdown['total_sec']:.1f}s\")\n",
                "\n",
                "speedup = hf_breakdown['total_sec'] / vllm_breakdown['total_sec']\n",
                "print(f\"\\nOverall speedup: {speedup:.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. vLLM: PagedAttention for Fast Generation\n",
                "\n",
                "**Key innovation:** Paging for KV cache reduces memory waste from 60-80% to <4%.\n",
                "\n",
                "```\n",
                "Traditional KV Cache:              vLLM PagedAttention:\n",
                "┌────────────────────┐            ┌──┬──┬──┬──┬──┬──┐\n",
                "│ Allocated for max  │            │P1│P1│P2│P3│P3│P3│  <- Blocks\n",
                "│ possible length    │            └──┴──┴──┴──┴──┴──┘\n",
                "│   (WASTED!)        │              Allocated on-demand\n",
                "│                    │              Shared across requests\n",
                "└────────────────────┘\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "KV Cache for batch=32, seq=2048:\n",
                        "  Memory: 32.00 GB\n",
                        "  With 60% waste (traditional): 51.20 GB\n",
                        "  With 4% waste (vLLM): 33.28 GB\n"
                    ]
                }
            ],
            "source": [
                "def estimate_kv_cache_memory(\n",
                "    batch_size: int,\n",
                "    seq_len: int,\n",
                "    num_layers: int,\n",
                "    num_heads: int,\n",
                "    head_dim: int,\n",
                "    dtype_bytes: int = 2,  # FP16\n",
                ") -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Estimate KV cache memory requirements.\n",
                "    \n",
                "    Memory = 2 * batch * seq * layers * heads * head_dim * dtype\n",
                "    (2x for K and V)\n",
                "    \"\"\"\n",
                "    kv_memory = 2 * batch_size * seq_len * num_layers * num_heads * head_dim * dtype_bytes\n",
                "    kv_memory_gb = kv_memory / (1024 ** 3)\n",
                "    \n",
                "    return {\n",
                "        'kv_cache_bytes': kv_memory,\n",
                "        'kv_cache_gb': kv_memory_gb,\n",
                "    }\n",
                "\n",
                "\n",
                "# Example: Llama-7B style model\n",
                "kv_estimate = estimate_kv_cache_memory(\n",
                "    batch_size=32,\n",
                "    seq_len=2048,\n",
                "    num_layers=32,\n",
                "    num_heads=32,\n",
                "    head_dim=128,\n",
                ")\n",
                "print(f\"KV Cache for batch=32, seq=2048:\")\n",
                "print(f\"  Memory: {kv_estimate['kv_cache_gb']:.2f} GB\")\n",
                "print(f\"  With 60% waste (traditional): {kv_estimate['kv_cache_gb'] * 1.6:.2f} GB\")\n",
                "print(f\"  With 4% waste (vLLM): {kv_estimate['kv_cache_gb'] * 1.04:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### vLLM Configuration for RLHF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "vLLM Configuration for RLHF:\n",
                        "{\n",
                        "  \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
                        "  \"tensor_parallel_size\": 1,\n",
                        "  \"dtype\": \"bfloat16\",\n",
                        "  \"seed\": 42,\n",
                        "  \"gpu_memory_utilization\": 0.9,\n",
                        "  \"max_model_len\": 4096,\n",
                        "  \"enable_prefix_caching\": true,\n",
                        "  \"max_num_seqs\": 256,\n",
                        "  \"max_num_batched_tokens\": 8192\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "# vLLM configuration for OpenRLHF integration\n",
                "\n",
                "vllm_config = {\n",
                "    # Engine settings\n",
                "    \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
                "    \"tensor_parallel_size\": 1,  # GPUs per model copy\n",
                "    \"dtype\": \"bfloat16\",\n",
                "    \"seed\": 42,\n",
                "    \n",
                "    # Memory management\n",
                "    \"gpu_memory_utilization\": 0.9,  # Use 90% of GPU memory\n",
                "    \"max_model_len\": 4096,  # Maximum sequence length\n",
                "    \"enable_prefix_caching\": True,  # Share prefixes across requests\n",
                "    \n",
                "    # Sampling\n",
                "    \"max_num_seqs\": 256,  # Concurrent sequences\n",
                "    \"max_num_batched_tokens\": 8192,  # Tokens per batch\n",
                "}\n",
                "\n",
                "print(\"vLLM Configuration for RLHF:\")\n",
                "print(json.dumps(vllm_config, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. DeepSpeed ZeRO: Memory-Efficient Training\n",
                "\n",
                "**Three stages of memory optimization:**\n",
                "\n",
                "| Stage | Partitions | Memory Reduction |\n",
                "|-------|-----------|------------------|\n",
                "| ZeRO-1 | Optimizer states | 4x |\n",
                "| ZeRO-2 | + Gradients | 8x |\n",
                "| ZeRO-3 | + Parameters | Linear with GPUs |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def estimate_training_memory(\n",
                "    param_count: int,\n",
                "    batch_size: int,\n",
                "    seq_len: int,\n",
                "    num_gpus: int = 1,\n",
                "    zero_stage: int = 0,\n",
                ") -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Estimate GPU memory for training with different ZeRO stages.\n",
                "    \n",
                "    Memory components:\n",
                "    - Parameters: 2 bytes/param (FP16)\n",
                "    - Gradients: 2 bytes/param\n",
                "    - Optimizer (Adam): 12 bytes/param (FP32 params + momentum + variance)\n",
                "    - Activations: ~batch * seq * hidden * layers (approximated)\n",
                "    \"\"\"\n",
                "    bytes_per_param = 2  # FP16\n",
                "    \n",
                "    # Base memory (no ZeRO)\n",
                "    param_memory = param_count * bytes_per_param\n",
                "    grad_memory = param_count * bytes_per_param\n",
                "    optimizer_memory = param_count * 12  # Adam states\n",
                "    \n",
                "    total_base = param_memory + grad_memory + optimizer_memory\n",
                "    \n",
                "    # Apply ZeRO partitioning\n",
                "    if zero_stage == 0:\n",
                "        memory_per_gpu = total_base\n",
                "    elif zero_stage == 1:\n",
                "        # Partition optimizer states\n",
                "        memory_per_gpu = param_memory + grad_memory + (optimizer_memory / num_gpus)\n",
                "    elif zero_stage == 2:\n",
                "        # Partition optimizer + gradients\n",
                "        memory_per_gpu = param_memory + (grad_memory + optimizer_memory) / num_gpus\n",
                "    elif zero_stage == 3:\n",
                "        # Partition everything\n",
                "        memory_per_gpu = (param_memory + grad_memory + optimizer_memory) / num_gpus\n",
                "    else:\n",
                "        raise ValueError(f\"Invalid ZeRO stage: {zero_stage}\")\n",
                "    \n",
                "    return {\n",
                "        'total_memory_gb': total_base / (1024 ** 3),\n",
                "        'per_gpu_gb': memory_per_gpu / (1024 ** 3),\n",
                "        'reduction_factor': total_base / memory_per_gpu,\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Memory requirements for 7B model on 8 GPUs:\n",
                        "ZeRO Stage   Per GPU (GB)    Reduction \n",
                        "----------------------------------------\n",
                        "Stage 0       104.3           1.0       x\n",
                        "Stage 1       35.9            2.9       x\n",
                        "Stage 2       24.4            4.3       x\n",
                        "Stage 3       13.0            8.0       x\n"
                    ]
                }
            ],
            "source": [
                "# Compare ZeRO stages for 7B model\n",
                "\n",
                "param_count = 7_000_000_000  # 7B parameters\n",
                "num_gpus = 8\n",
                "\n",
                "print(f\"Memory requirements for {param_count/1e9:.0f}B model on {num_gpus} GPUs:\")\n",
                "print(f\"{'ZeRO Stage':<12} {'Per GPU (GB)':<15} {'Reduction':<10}\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for stage in [0, 1, 2, 3]:\n",
                "    mem = estimate_training_memory(param_count, 1, 2048, num_gpus, stage)\n",
                "    print(f\"Stage {stage:<7} {mem['per_gpu_gb']:<15.1f} {mem['reduction_factor']:<10.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DeepSpeed Configuration Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "DeepSpeed ZeRO-3 Config:\n",
                        "{\n",
                        "  \"train_batch_size\": \"auto\",\n",
                        "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
                        "  \"gradient_accumulation_steps\": \"auto\",\n",
                        "  \"bf16\": {\n",
                        "    \"enabled\": true\n",
                        "  },\n",
                        "  \"zero_optimization\": {\n",
                        "    \"stage\": 3,\n",
                        "    \"overlap_comm\": true,\n",
                        "    \"contiguous_gradients\": true,\n",
                        "    \"reduce_bucket_size\": \"auto\",\n",
                        "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
                        "    \"stage3_param_persistence_threshold\": \"auto\",\n",
                        "    \"offload_param\": {\n",
                        "      \"device\": \"none\",\n",
                        "      \"pin_memory\": true\n",
                        "    },\n",
                        "    \"offload_optimizer\": {\n",
                        "      \"device\": \"none\",\n",
                        "      \"pin_memory\": true\n",
                        "    }\n",
                        "  },\n",
                        "  \"gradient_clipping\": 1.0\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "# DeepSpeed ZeRO-3 configuration for RLHF\n",
                "\n",
                "deepspeed_config_zero3 = {\n",
                "    \"train_batch_size\": \"auto\",\n",
                "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
                "    \"gradient_accumulation_steps\": \"auto\",\n",
                "    \n",
                "    \"bf16\": {\n",
                "        \"enabled\": True\n",
                "    },\n",
                "    \n",
                "    \"zero_optimization\": {\n",
                "        \"stage\": 3,\n",
                "        \"overlap_comm\": True,\n",
                "        \"contiguous_gradients\": True,\n",
                "        \"reduce_bucket_size\": \"auto\",\n",
                "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
                "        \"stage3_param_persistence_threshold\": \"auto\",\n",
                "        \n",
                "        # CPU offloading (for memory-constrained setups)\n",
                "        \"offload_param\": {\n",
                "            \"device\": \"none\",  # Set to \"cpu\" to enable\n",
                "            \"pin_memory\": True\n",
                "        },\n",
                "        \"offload_optimizer\": {\n",
                "            \"device\": \"none\",  # Set to \"cpu\" to enable\n",
                "            \"pin_memory\": True\n",
                "        },\n",
                "    },\n",
                "    \n",
                "    \"gradient_clipping\": 1.0,\n",
                "}\n",
                "\n",
                "print(\"DeepSpeed ZeRO-3 Config:\")\n",
                "print(json.dumps(deepspeed_config_zero3, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ray: Distributed Orchestration\n",
                "\n",
                "Ray coordinates the four models in RLHF across GPUs:\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────────┐\n",
                "│                     Ray Placement Groups                     │\n",
                "├─────────────┬─────────────┬─────────────┬───────────────────┤\n",
                "│   Actor     │   Critic    │   Reward    │    Reference      │\n",
                "│  (Train)    │   (Train)   │  (Inference)│   (Inference)     │\n",
                "│   GPU 0-3   │   GPU 4-5   │   GPU 6     │     GPU 7         │\n",
                "└─────────────┴─────────────┴─────────────┴───────────────────┘\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ray concepts demonstration (simulated without actual Ray)\n",
                "\n",
                "class RayActorSimulator:\n",
                "    \"\"\"\n",
                "    Simulates Ray actor behavior for understanding.\n",
                "    In production, use @ray.remote decorator.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, name: str, num_gpus: int):\n",
                "        self.name = name\n",
                "        self.num_gpus = num_gpus\n",
                "        self.state = {}\n",
                "    \n",
                "    def get_resources(self) -> Dict:\n",
                "        return {\"name\": self.name, \"num_gpus\": self.num_gpus}\n",
                "    \n",
                "    def update_weights(self, weights: Dict) -> None:\n",
                "        \"\"\"Receive updated weights from training.\"\"\"\n",
                "        self.state['weights_version'] = weights.get('version', 0)\n",
                "    \n",
                "    def generate(self, prompts: List[str]) -> List[str]:\n",
                "        \"\"\"Generate responses (placeholder).\"\"\"\n",
                "        return [f\"Response to: {p[:20]}...\" for p in prompts]\n",
                "\n",
                "\n",
                "class OpenRLHFSimulator:\n",
                "    \"\"\"\n",
                "    Simulates OpenRLHF's distributed architecture.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config: Dict):\n",
                "        self.config = config\n",
                "        \n",
                "        # Create actors for each model\n",
                "        self.actor = RayActorSimulator(\"actor\", config.get('actor_gpus', 4))\n",
                "        self.critic = RayActorSimulator(\"critic\", config.get('critic_gpus', 2))\n",
                "        self.reward = RayActorSimulator(\"reward\", config.get('reward_gpus', 1))\n",
                "        self.reference = RayActorSimulator(\"reference\", config.get('ref_gpus', 1))\n",
                "    \n",
                "    def get_resource_allocation(self) -> Dict:\n",
                "        \"\"\"Show resource allocation.\"\"\"\n",
                "        return {\n",
                "            'actor': self.actor.get_resources(),\n",
                "            'critic': self.critic.get_resources(),\n",
                "            'reward': self.reward.get_resources(),\n",
                "            'reference': self.reference.get_resources(),\n",
                "            'total_gpus': sum([\n",
                "                self.actor.num_gpus,\n",
                "                self.critic.num_gpus,\n",
                "                self.reward.num_gpus,\n",
                "                self.reference.num_gpus,\n",
                "            ])\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "OpenRLHF Resource Allocation:\n",
                        "  actor: 4 GPUs\n",
                        "  critic: 2 GPUs\n",
                        "  reward: 1 GPUs\n",
                        "  reference: 1 GPUs\n",
                        "  Total: 8 GPUs\n"
                    ]
                }
            ],
            "source": [
                "# Example resource allocation\n",
                "\n",
                "simulator = OpenRLHFSimulator({\n",
                "    'actor_gpus': 4,\n",
                "    'critic_gpus': 2,\n",
                "    'reward_gpus': 1,\n",
                "    'ref_gpus': 1,\n",
                "})\n",
                "\n",
                "allocation = simulator.get_resource_allocation()\n",
                "print(\"OpenRLHF Resource Allocation:\")\n",
                "for model, resources in allocation.items():\n",
                "    if model != 'total_gpus':\n",
                "        print(f\"  {model}: {resources['num_gpus']} GPUs\")\n",
                "print(f\"  Total: {allocation['total_gpus']} GPUs\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. OpenRLHF: Complete Training Framework\n",
                "\n",
                "### Command-Line Interface Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# OpenRLHF CLI command generator\n",
                "\n",
                "def generate_openrlhf_command(\n",
                "    algorithm: str = 'grpo',\n",
                "    pretrain: str = 'meta-llama/Llama-2-7b-hf',\n",
                "    reward_model: str = 'OpenRLHF/reward-model',\n",
                "    dataset: str = 'OpenRLHF/prompt-collection',\n",
                "    num_gpus: int = 8,\n",
                "    use_vllm: bool = True,\n",
                "    colocate: bool = False,\n",
                ") -> str:\n",
                "    \"\"\"\n",
                "    Generate OpenRLHF training command.\n",
                "    \"\"\"\n",
                "    cmd = [\n",
                "        \"python\", \"-m\", \"openrlhf.cli.train_ppo_ray\",\n",
                "        f\"--pretrain {pretrain}\",\n",
                "        f\"--reward_pretrain {reward_model}\",\n",
                "        f\"--prompt_data {dataset}\",\n",
                "        \n",
                "        # Algorithm selection\n",
                "        f\"--advantage_estimator {'group_norm' if algorithm == 'grpo' else 'gae'}\",\n",
                "        \"--use_kl_loss\" if algorithm == 'grpo' else \"\",\n",
                "        \n",
                "        # Training params\n",
                "        \"--actor_learning_rate 1e-5\",\n",
                "        \"--critic_learning_rate 5e-6\",\n",
                "        \"--init_kl_coef 0.1\",\n",
                "        \"--eps_clip 0.2\",\n",
                "        \"--max_epochs 4\",\n",
                "        \"--train_batch_size 128\",\n",
                "        \"--rollout_batch_size 32\",\n",
                "        \n",
                "        # vLLM settings\n",
                "        f\"--vllm_num_engines {2 if use_vllm else 0}\",\n",
                "        \"--vllm_gpu_memory_utilization 0.9\" if use_vllm else \"\",\n",
                "        \n",
                "        # Colocation\n",
                "        \"--colocate_all_models\" if colocate else \"\",\n",
                "        \n",
                "        # Resource allocation\n",
                "        f\"--actor_num_gpus_per_node {num_gpus // 2}\",\n",
                "        \"--critic_num_gpus_per_node 2\",\n",
                "        \"--reward_num_gpus_per_node 1\",\n",
                "        \"--ref_num_gpus_per_node 1\",\n",
                "    ]\n",
                "    \n",
                "    # Filter empty strings and join\n",
                "    return \" \\\\\\n  \".join([c for c in cmd if c])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "OpenRLHF GRPO Training Command:\n",
                        "============================================================\n",
                        "python \\\n",
                        "  -m \\\n",
                        "  openrlhf.cli.train_ppo_ray \\\n",
                        "  --pretrain meta-llama/Llama-2-7b-hf \\\n",
                        "  --reward_pretrain OpenRLHF/reward-model \\\n",
                        "  --prompt_data OpenRLHF/prompt-collection \\\n",
                        "  --advantage_estimator group_norm \\\n",
                        "  --use_kl_loss \\\n",
                        "  --actor_learning_rate 1e-5 \\\n",
                        "  --critic_learning_rate 5e-6 \\\n",
                        "  --init_kl_coef 0.1 \\\n",
                        "  --eps_clip 0.2 \\\n",
                        "  --max_epochs 4 \\\n",
                        "  --train_batch_size 128 \\\n",
                        "  --rollout_batch_size 32 \\\n",
                        "  --vllm_num_engines 2 \\\n",
                        "  --vllm_gpu_memory_utilization 0.9 \\\n",
                        "  --actor_num_gpus_per_node 4 \\\n",
                        "  --critic_num_gpus_per_node 2 \\\n",
                        "  --reward_num_gpus_per_node 1 \\\n",
                        "  --ref_num_gpus_per_node 1\n"
                    ]
                }
            ],
            "source": [
                "# Generate GRPO training command\n",
                "\n",
                "grpo_cmd = generate_openrlhf_command(\n",
                "    algorithm='grpo',\n",
                "    num_gpus=8,\n",
                "    use_vllm=True,\n",
                ")\n",
                "\n",
                "print(\"OpenRLHF GRPO Training Command:\")\n",
                "print(\"=\" * 60)\n",
                "print(grpo_cmd)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Memory-Efficient Configuration (4 GPUs, colocated):\n",
                        "============================================================\n",
                        "python \\\n",
                        "  -m \\\n",
                        "  openrlhf.cli.train_ppo_ray \\\n",
                        "  --pretrain meta-llama/Llama-2-7b-hf \\\n",
                        "  --reward_pretrain OpenRLHF/reward-model \\\n",
                        "  --prompt_data OpenRLHF/prompt-collection \\\n",
                        "  --advantage_estimator group_norm \\\n",
                        "  --use_kl_loss \\\n",
                        "  --actor_learning_rate 1e-5 \\\n",
                        "  --critic_learning_rate 5e-6 \\\n",
                        "  --init_kl_coef 0.1 \\\n",
                        "  --eps_clip 0.2 \\\n",
                        "  --max_epochs 4 \\\n",
                        "  --train_batch_size 128 \\\n",
                        "  --rollout_batch_size 32 \\\n",
                        "  --vllm_num_engines 2 \\\n",
                        "  --vllm_gpu_memory_utilization 0.9 \\\n",
                        "  --colocate_all_models \\\n",
                        "  --actor_num_gpus_per_node 2 \\\n",
                        "  --critic_num_gpus_per_node 2 \\\n",
                        "  --reward_num_gpus_per_node 1 \\\n",
                        "  --ref_num_gpus_per_node 1\n"
                    ]
                }
            ],
            "source": [
                "# Generate memory-efficient command (colocate + smaller GPU count)\n",
                "\n",
                "efficient_cmd = generate_openrlhf_command(\n",
                "    algorithm='grpo',\n",
                "    num_gpus=4,\n",
                "    use_vllm=True,\n",
                "    colocate=True,\n",
                ")\n",
                "\n",
                "print(\"Memory-Efficient Configuration (4 GPUs, colocated):\")\n",
                "print(\"=\" * 60)\n",
                "print(efficient_cmd)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. TRL: HuggingFace Trainer Interface\n",
                "\n",
                "For simpler setups, TRL provides higher-level trainers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TRL GRPOTrainer Configuration:\n",
                        "{\n",
                        "  \"model_name\": \"meta-llama/Llama-2-7b-hf\",\n",
                        "  \"num_generation\": 8,\n",
                        "  \"beta\": 0.1,\n",
                        "  \"learning_rate\": 1e-05,\n",
                        "  \"per_device_train_batch_size\": 8,\n",
                        "  \"gradient_accumulation_steps\": 4,\n",
                        "  \"num_train_epochs\": 3,\n",
                        "  \"max_new_tokens\": 512,\n",
                        "  \"temperature\": 1.0,\n",
                        "  \"use_vllm\": true,\n",
                        "  \"vllm_mode\": \"colocate\",\n",
                        "  \"vllm_gpu_memory_utilization\": 0.9,\n",
                        "  \"bf16\": true,\n",
                        "  \"gradient_checkpointing\": true\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "# TRL GRPOTrainer configuration template\n",
                "\n",
                "trl_grpo_config = {\n",
                "    # Model\n",
                "    \"model_name\": \"meta-llama/Llama-2-7b-hf\",\n",
                "    \n",
                "    # GRPO specific\n",
                "    \"num_generation\": 8,  # Group size (K)\n",
                "    \"beta\": 0.1,  # KL coefficient\n",
                "    \n",
                "    # Training\n",
                "    \"learning_rate\": 1e-5,\n",
                "    \"per_device_train_batch_size\": 8,  # Must be multiple of num_generation\n",
                "    \"gradient_accumulation_steps\": 4,\n",
                "    \"num_train_epochs\": 3,\n",
                "    \n",
                "    # Generation\n",
                "    \"max_new_tokens\": 512,\n",
                "    \"temperature\": 1.0,\n",
                "    \n",
                "    # vLLM acceleration\n",
                "    \"use_vllm\": True,\n",
                "    \"vllm_mode\": \"colocate\",\n",
                "    \"vllm_gpu_memory_utilization\": 0.9,\n",
                "    \n",
                "    # Optimization\n",
                "    \"bf16\": True,\n",
                "    \"gradient_checkpointing\": True,\n",
                "}\n",
                "\n",
                "print(\"TRL GRPOTrainer Configuration:\")\n",
                "print(json.dumps(trl_grpo_config, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TRL GRPOTrainer Usage:\n",
                        "\n",
                        "from trl import GRPOConfig, GRPOTrainer\n",
                        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                        "\n",
                        "# Load model and tokenizer\n",
                        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
                        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
                        "\n",
                        "# Define reward function\n",
                        "def reward_fn(completions: list[str]) -> list[float]:\n",
                        "    \"\"\"Your reward function here.\"\"\"\n",
                        "    return [len(c) / 100 for c in completions]  # Example: reward length\n",
                        "\n",
                        "# Configure trainer\n",
                        "config = GRPOConfig(\n",
                        "    num_generation=8,\n",
                        "    beta=0.1,\n",
                        "    learning_rate=1e-5,\n",
                        "    per_device_train_batch_size=8,\n",
                        "    use_vllm=True,\n",
                        ")\n",
                        "\n",
                        "# Create trainer\n",
                        "trainer = GRPOTrainer(\n",
                        "    model=model,\n",
                        "    config=config,\n",
                        "    tokenizer=tokenizer,\n",
                        "    reward_funcs=[reward_fn],\n",
                        "    train_dataset=dataset,\n",
                        ")\n",
                        "\n",
                        "# Train\n",
                        "trainer.train()\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# TRL usage example (pseudocode - requires actual model)\n",
                "\n",
                "trl_example_code = '''\n",
                "from trl import GRPOConfig, GRPOTrainer\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "# Load model and tokenizer\n",
                "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
                "\n",
                "# Define reward function\n",
                "def reward_fn(completions: list[str]) -> list[float]:\n",
                "    \"\"\"Your reward function here.\"\"\"\n",
                "    return [len(c) / 100 for c in completions]  # Example: reward length\n",
                "\n",
                "# Configure trainer\n",
                "config = GRPOConfig(\n",
                "    num_generation=8,\n",
                "    beta=0.1,\n",
                "    learning_rate=1e-5,\n",
                "    per_device_train_batch_size=8,\n",
                "    use_vllm=True,\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = GRPOTrainer(\n",
                "    model=model,\n",
                "    config=config,\n",
                "    tokenizer=tokenizer,\n",
                "    reward_funcs=[reward_fn],\n",
                "    train_dataset=dataset,\n",
                ")\n",
                "\n",
                "# Train\n",
                "trainer.train()\n",
                "'''\n",
                "\n",
                "print(\"TRL GRPOTrainer Usage:\")\n",
                "print(trl_example_code)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Async Training: Decoupling Generation and Training\n",
                "\n",
                "**Problem:** Synchronous training wastes GPU time.\n",
                "\n",
                "```\n",
                "Synchronous:\n",
                "GPU 0: [GENERATE.....][IDLE][TRAIN][IDLE][GENERATE.....]\n",
                "GPU 1: [IDLE][GENERATE.....][IDLE][TRAIN][IDLE]\n",
                "\n",
                "Asynchronous:\n",
                "GPU 0 (Gen):   [GENERATE][GENERATE][GENERATE][GENERATE]\n",
                "GPU 1 (Train): [TRAIN][TRAIN][TRAIN][TRAIN][TRAIN]\n",
                "```\n",
                "\n",
                "**Key insight:** DPO is robust to off-policy data, enabling 25-40% speedups."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AsyncTrainingSimulator:\n",
                "    \"\"\"\n",
                "    Demonstrates async training concept.\n",
                "    \n",
                "    In production, OpenRLHF uses --async_train flag.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, staleness_tolerance: int = 2):\n",
                "        self.staleness_tolerance = staleness_tolerance\n",
                "        self.generation_version = 0\n",
                "        self.training_version = 0\n",
                "        self.buffer = []\n",
                "    \n",
                "    def generate_batch(self) -> Dict:\n",
                "        \"\"\"Simulate generation with current policy version.\"\"\"\n",
                "        self.generation_version += 1\n",
                "        batch = {\n",
                "            'data': f'batch_{self.generation_version}',\n",
                "            'policy_version': self.generation_version,\n",
                "        }\n",
                "        self.buffer.append(batch)\n",
                "        return batch\n",
                "    \n",
                "    def train_step(self) -> Dict:\n",
                "        \"\"\"Train on buffered data (may be off-policy).\"\"\"\n",
                "        if not self.buffer:\n",
                "            return {'status': 'waiting'}\n",
                "        \n",
                "        batch = self.buffer.pop(0)\n",
                "        self.training_version += 1\n",
                "        \n",
                "        staleness = self.training_version - batch['policy_version']\n",
                "        \n",
                "        return {\n",
                "            'batch': batch['data'],\n",
                "            'training_version': self.training_version,\n",
                "            'batch_version': batch['policy_version'],\n",
                "            'staleness': staleness,\n",
                "            'is_on_policy': staleness <= self.staleness_tolerance,\n",
                "        }\n",
                "    \n",
                "    def simulate(self, steps: int) -> List[Dict]:\n",
                "        \"\"\"Simulate async training.\"\"\"\n",
                "        results = []\n",
                "        \n",
                "        for _ in range(steps):\n",
                "            # Generation runs continuously\n",
                "            self.generate_batch()\n",
                "            self.generate_batch()  # 2x faster than training\n",
                "            \n",
                "            # Training runs on available data\n",
                "            result = self.train_step()\n",
                "            results.append(result)\n",
                "        \n",
                "        return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Async Training Simulation:\n",
                        "Step   Train Ver  Batch Ver  Stale    On-Policy \n",
                        "--------------------------------------------------\n",
                        "0      1          1          0        ✓         \n",
                        "1      2          2          0        ✓         \n",
                        "2      3          3          0        ✓         \n",
                        "3      4          4          0        ✓         \n",
                        "4      5          5          0        ✓         \n"
                    ]
                }
            ],
            "source": [
                "# Simulate async training\n",
                "\n",
                "simulator = AsyncTrainingSimulator(staleness_tolerance=2)\n",
                "results = simulator.simulate(5)\n",
                "\n",
                "print(\"Async Training Simulation:\")\n",
                "print(f\"{'Step':<6} {'Train Ver':<10} {'Batch Ver':<10} {'Stale':<8} {'On-Policy':<10}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for i, r in enumerate(results):\n",
                "    if 'staleness' in r:\n",
                "        print(f\"{i:<6} {r['training_version']:<10} {r['batch_version']:<10} \"\n",
                "              f\"{r['staleness']:<8} {'✓' if r['is_on_policy'] else '✗':<10}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Summary: Production Stack Checklist\n",
                "\n",
                "| Component | Small Scale (1-4 GPUs) | Large Scale (8+ GPUs) |\n",
                "|-----------|------------------------|----------------------|\n",
                "| Framework | TRL `GRPOTrainer` | OpenRLHF |\n",
                "| Generation | vLLM colocated | vLLM separate engines |\n",
                "| Memory | DeepSpeed ZeRO-2 | DeepSpeed ZeRO-3 |\n",
                "| Distributed | Accelerate | Ray |\n",
                "| Training | Synchronous | Async (--async_train) |\n",
                "\n",
                "### Quick Start Commands\n",
                "\n",
                "```bash\n",
                "# Install\n",
                "pip install openrlhf vllm deepspeed\n",
                "\n",
                "# Single-node GRPO (4 GPUs)\n",
                "python -m openrlhf.cli.train_ppo_ray \\\n",
                "  --pretrain meta-llama/Llama-2-7b-hf \\\n",
                "  --advantage_estimator group_norm \\\n",
                "  --use_kl_loss \\\n",
                "  --vllm_num_engines 1 \\\n",
                "  --colocate_all_models\n",
                "\n",
                "# Multi-node (Ray cluster)\n",
                "ray start --head  # On head node\n",
                "ray start --address=<head-ip>  # On worker nodes\n",
                "# Then run training command\n",
                "```\n",
                "\n",
                "---\n",
                "**Next:** `07_dpo_alternatives.ipynb` (DPO, ORPO, SimPO)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
